
PARAMETER ESTIMATION FOR STATISTICAL
PARSING MODELS: THEORY AND PRACTICE
OF DISTRIBUTION-FREE METHODS

A fundamental problem in statistical parsing is the choice of criteria and al-
gorithms used to estimate the parameters in a model. The predominant ap-
proach in computational linguistics has been to use a parametric model with
some variant of maximum-likelihood estimation. The assumptions under which
maximum-likelihood estimation is justiﬁed are arguably quite strong. This paper
discusses the statistical theory underlying various parameter-estimation meth-
ods, and gives algorithms which depend on alternatives to (smoothed) maximum-
likelihood estimation. We ﬁrst give an overview of results from statistical learn-
ing theory. We then show how important concepts from the classiﬁcation liter-
ature – speciﬁcally, generalization results based on margins on training data –
can be derived for parsing models. Finally, we describe parameter estimation
algorithms which are motivated by these generalization bounds.

A fundamental problem in statistical parsing is the choice of criteria and
algorithms used to estimate the parameters in a model. The predominant ap-
proach in computational linguistics has been to use a parametric model with
maximum-likelihood estimation, usually with some method for “smoothing”
parameter estimates to deal with sparse data problems. Methods falling into
this category include Probabilistic Context-Free Grammars and Hidden Markov
Models, Maximum Entropy models for tagging and parsing, and recent work
on Markov Random Fields.

This paper discusses the statistical theory underlying various parameter-
estimation methods, and gives algorithms which depend on alternatives to

(smoothed) maximum-likelihood estimation. The assumptions under which
maximum-likelihood estimation is justiﬁed are arguably quite strong – in par-
ticular, an assumption is made that the structure of the statistical process gen-
erating the data is known (for example, maximum–likelihood estimation for
PCFGs is justiﬁed providing that the data was actually generated by a PCFG).
In contrast, work in computational learning theory has concentrated on models
with the weaker assumption that training and test examples are generated from
the same distribution, but that the form of the distribution is unknown: in this
sense the results hold across all distributions and are called “distribution-free”.
The result of this work – which goes back to results in statistical learning theory
by Vapnik (1998) and colleagues, and to work within Valiant’s PAC model of
learning (Valiant, 1984) – has been the development of algorithms and theory
which provide radical alternatives to parametric maximum-likelihood meth-
ods. These algorithms are appealing in both theoretical terms, and in their
impressive results in many experimental studies.

In the ﬁrst part of this paper (sections 2 and 3) we describe linear models
for parsing, and give an example of how the usual maximum-likelihood esti-
mates for PCFGs can be sub-optimal. Sections 4, 5 and 6 describe the basic
framework under which we will analyse parameter estimation methods. This is
essentially the framework advocated by several books on learning theory (see
Devroye et al., 1996; Vapnik, 1998; Cristianini and Shawe-Taylor, 2000). As
a warm-up section 5 describes statistical theory for the simple case of ﬁnite
hypothesis classes. Section 6 then goes on to the important case of hyperplane
classiﬁers. Section 7 describes how concepts from the classiﬁcation literature
– speciﬁcally, generalization results based on margins on training data – can be
derived for linear models for parsing. Section 8 describes parameter estimation
algorithms motivated by these results. Section 9 gives pointers to results in the
literature using the algorithms, and also discusses relationships to Markov Ran-
dom Fields or maximum-entropy models (Ratnaparkhi et al., 1994; Johnson et
al., 1999; Lafferty et al., 2001).

In this section we introduce the framework for the learning problem that is
studied in this paper. The task is to learn a function F : X → Y where X is
some set of possible inputs (for example a set of possible sentences), and Y is
a domain of possible outputs (for example a set of parse trees). 

this can be achieved by ﬁxing some arbitrary ordering on the set Y.)

Several natural language problems can be seen to be special cases of this
framework, through different deﬁnitions of GEN and Φ. In the next section
we show how weighted context-free grammars are one special case. Tagging
problems can also be framed in this way (e.g., Collins, 2002b): in this case
GEN(x) is all possible tag sequences for an input sentence x. In (Johnson
et al., 1999), GEN(x) is the set of parses for a sentence x under an LFG
grammar, and the representation Φ can track arbitrary features of these parses.
In (Ratnaparkhi et al., 1994; Collins, 2000; Collins and Duffy, 2002) GEN(x)
is the top N parses from a ﬁrst pass statistical model, and the representation
Φ tracks the log-probability assigned by the ﬁrst pass model together with
arbitrary additional features of the parse trees. Walker et al. (2001) show how
the approach can be applied to NLP generation: in this case x is a semantic
representation, y is a surface string, and GEN is a deterministic system that
maps x to a number of candidate surface realizations. The framework can
also be considered to be a generalization of multi-class classiﬁcation problems,
where for all inputs x, GEN(x) is a ﬁxed set of k labels {1, 2, . . . , k} (e.g.,
see Crammer and Singer, 2001; Elisseeff et al., 1999).

Say we have a context-free grammar (see (Hopcroft and Ullman, 1979) for a
formal deﬁnition) G = (N, Σ, R, S) where N is a set of non-terminal symbols,
Σ is an alphabet, R is a set of rules of the form X → Y1Y2 · · · Yn for n ≥
0, X ∈ N, Yi ∈ (N ∪ Σ), and S is a distinguished start symbol in N. The
grammar deﬁnes a set of possible strings, and possible string/tree pairs, in a
language. We use GEN(x) for all x ∈ Σ∗ to denote the set of possible trees

For convenience we will take the rules in R to be placed in some arbitrary
ordering r1, . . . , rn. A weighted grammar G = (N, Σ, R, S, Θ) also includes
a parameter vector Θ ∈ ℜn which assigns a weight to each rule in R: the
i-th component of Θ is the weight of rule ri. Given a sentence x and a tree y
spanning the sentence, we assume a function Φ(x, y) which tracks the counts
of the rules in (x, y). Speciﬁcally, the i-th component of Φ(x, y) is the number
of times rule ri is seen in (x, y). Under these deﬁnitions, the weighted context-
free grammar deﬁnes a function hΘ from sentences to trees:

Finding hΘ(x), the parse with the largest weight, can be achieved in polyno-
mial time using the CKY parsing algorithm (in spite of a possibly exponential
number of members of GEN(x)), assuming that the weighted CFG can be
converted to an equivalent weighted CFG in Chomsky Normal Form.

In this paper we consider the structure of the grammar to be ﬁxed, the learn-
ing problem being reduced to setting the values of the parameters Θ. A ba-
sic question is as follows: given a “training sample” of sentence/tree pairs
{(x1, y1), . . . , (xm, ym)}, what criterion should be used to set the weights in
the grammar? A very common method – that of Probabilistic Context-Free
Grammars (PCFGs) – uses the parameters to deﬁne a distribution P (x, y|Θ)
over possible sentence/tree pairs in the grammar. Maximum likelihood esti-
mation is used to set the weights. We will consider the assumptions under
which this method is justiﬁed, and argue that these assumptions are likely to
be too strong. We will also give an example to show how PCFGs can be badly
mislead when the assumptions are violated. As an alternative we will propose
distribution-free methods for estimating the weights, which are justiﬁed un-
der much weaker assumptions, and can give quite different estimates of the
parameter values in some situations.

We would like to generalize weighted context-free grammars by allowing
the representation Φ(x, y) to be essentially any feature-vector representation
of the tree. There is still a grammar G, deﬁning a set of candidates GEN(x)
for each sentence. The parameters of the parser are a vector Θ. The parser’s
output is deﬁned in the same way as equation (1.1). The important thing in this
generalization is that the representation Φ is now not necessarily directly tied
to the productions in the grammar. This is essentially the approach advocated
by (Ratnaparkhi et al., 1994; Abney, 1997; Johnson et al., 1999), although the
criteria that we will propose for setting the parameters Θ are quite different.

While superﬁcially this might appear to be a minor change, it introduces
two major challenges. The ﬁrst problem is how to set the parameter values
under these general representations. The PCFG method described in the next

section, which results in simple relative frequency estimators of rule weights,
is not applicable to more general representations. A generalization of PCFGs,
Markov Random Fields (MRFs), has been proposed by several authors (Rat-
naparkhi et al., 1994; Abney, 1997; Johnson et al., 1999; Della Pietra et al.,
1997). In this paper we give several alternatives to MRFs, and we describe the
theory and assumptions which underly various models.

The second challenge is that now that the parameters are not tied to rules
in the grammar the CKY algorithm is not applicable – in the worst case we
may have to enumerate all members of GEN(x) explicitly to ﬁnd the highest-
scoring tree. One practical solution is to deﬁne the “grammar” G as a ﬁrst pass
statistical parser which allows dynamic programming to enumerate its top N
candidates. A second pass uses the more complex representation Φ to choose
the best of these parses. This is the approach used in several papers (e.g.,
Ratnaparkhi et al., 1994; Collins, 2000; Collins and Duffy, 2002).


This section reviews the basic theory underlying Probabilistic Context-Free
Grammars (PCFGs). Say we have a context-free grammar G = (N, Σ, R, S)
as deﬁned in section 2.1. We will use T to denote the set of all trees generated
by G. Now say we assign a weight p(r) in the range 0 to 1 to each rule r in
R. Assuming some arbitrary ordering r1, . . . , rn of the n rules in R, we use Θ
to denote a vector of parameters, Θ = hlog p(r1), log p(r2), . . . , log p(rn)i. If
c(T, r) is the number of times rule r is seen in a tree T , then the “probability”
of a tree T can be written as.

We can now study how to train the grammar from a training sample of
trees. Say there is a training set of trees {T1, T2, . . . , Tm}. The log-likelihood

of the training set given parameters Θ is L(Θ) = Pj log P (Tj|Θ). The

maximum-likelihood estimates are to take ˆΘ = arg maxΘ∈Ω L(Θ), where Ω
is the set of allowable parameter settings (i.e., the parameter settings which
obey the constraints in Booth and Thompson, 1973).

So under what circumstances is maximum-likelihood estimation justiﬁed?
Say there is a true set of weights Θ∗, which deﬁne an underlying distribution
P (T |Θ∗), and that the training set is a sample of size m from this distribution.
Then it can be shown that as m increases to inﬁnity, then with probability 1
the parameter estimates ˆΘ converge to values which give the same distribution
over trees as the “true” parameter values Θ∗.

To illustrate the deﬁciencies of PCFGs, we give a simple example. Say
we have a random process which generates just 3 trees, with probabilities
{p1, p2, p3}, as shown in ﬁgure 1.1a. The training sample will consist of a set
of trees drawn from this distribution. A test sample will be generated from the
same distribution, but in this case the trees will be hidden, and only the surface
strings will be seen (i.e., haaaai, haaai and hai with probabilities p1, p2, p3
respectively). We would like to learn a weighted CFG with as small error as
possible on a randomly drawn test sample.

As the size of the training sample goes to inﬁnity, the relative frequencies
of trees {T1, T2, T3} in the training sample will converge to {p1, p2, p3}. This
makes it easy to calculate the rule weights that maximum-likelihood estimation
converges to – see ﬁgure 1.1b. We will call the PCFG with these asymptotic
weights the asymptotic PCFG. Notice that the grammar generates trees never
seen in training data, shown in ﬁgure 1.1c. The grammar is ambiguous for
strings haaaai (both T1 and T4 are possible) and haaai (T2 and T5 are possi-
ble). In fact, under certain conditions T4 and T5 will get higher probabilities
under the asymptotic PCFG than T1 and T2, and both strings haaaai and haaai
will be mis-parsed. Figure 1.1d shows the distribution of the asymptotic PCFG
over the 8 trees when p1 = 0.2, p2 = 0.1 and p3 = 0.7. In this case both
ambiguous strings are mis-parsed by the asymptotic PCFG, resulting in an ex-
pected error rate of (p1 + p2) = 30% on newly drawn test examples.

Figure 1.1d. The probabilities assigned to the trees as the training size goes to inﬁnity, for
p1 = 0.2, p2 = 0.1, p3 = 0.7. Notice that P (T4) > P (T1), and P (T5) > P (T2), so the
induced PCFG will incorrectly map haaaai to T4 and haaai to T2.

This is a striking failure of the PCFG when we consider that it is easy to de-
rive weights on the grammar rules which parse both training and test examples
with no errors.2 On this example there exist weighted grammars which make
no errors, but the maximum likelihood estimation method will fail to ﬁnd these
weights, even with unlimited amounts of training data.

The next 4 sections of this chapter describe theoretical results underlying
the parameter estimation algorithms in section 8. In sections 4.1 to 4.3 we de-
scribe the basic framework under which we will analyse the various learning
approaches. In section 5 we describe analysis for a simple case, ﬁnite hypoth-
esis classes, which will be useful for illustrating ideas and intuition underlying
the methods. In section 6 we describe analysis of hyperplane classiﬁers. In sec-
tion 7 we describe how the results for hyperplane classiﬁers can be generalized
to apply to the linear models introduced in section 2.

This section introduces a general framework for supervised learning prob-
lems. There are several books (Devroye et al., 1996; Vapnik, 1998; Cristianini
and Shawe-Taylor, 2000) which cover the material in detail. We will use this
framework to analyze both parametric methods (PCFGs, for example), and the
distribution–free methods proposed in this paper. We assume the following:

An input domain X and an output domain Y. The task will be to learn a
function mapping each element of X to an element of Y. In parsing, X
is a set of possible sentences and Y is a set of possible trees.

There is some underlying probability distribution D(x, y) over X × Y.
The distribution is used to generate both training and test examples. It
is an unknown distribution, but it is constant across training and test
examples – both training and test examples are drawn independently,
identically distributed from D(x, y).

There is a loss function L(y, ˆy) which measures the cost of proposing
an output ˆy when the “true” output is y. A commonly used cost is the
0-1 loss L(y, ˆy) = 0 if y = ˆy, and L(y, ˆy) = 1 otherwise. We will
concentrate on this loss function in this paper.

Under 0-1 loss this is the expected proportion of errors that the hypoth-
esis makes on examples drawn from the distribution D. We would like
to learn a function whose expected loss is as low as possible: Er(h)
is a measure of how successful a function h is. Unfortunately, because
we do not have direct access to the distribution D, we cannot explicitly
calculate the expected loss of a hypothesis.

The training set is a sample of m pairs {(x1, y1), . . . , (xm, ym)} drawn
from the distribution D. This is the only information we have about D.

Finally, a useful concept is the Bayes Optimal hypothesis, which we will
denote as hB.
It is deﬁned as hB(x) = arg maxy∈Y D(x, y). The Bayes
optimal hypothesis simply outputs the most likely y under the distribution D
for each input x. It is easy to prove that this function minimizes the expected
loss Er(h) over the space of all possible functions – the Bayes optimal hy-
pothesis cannot be improved upon. Unfortunately, in general we do not know
D(x, y), so the Bayes optimal hypothesis, while useful as a theoretical con-
struct, cannot be obtained directly in practice. Given that the only access to the
distribution D(x, y) is indirect, through a training sample of ﬁnite size m, the
learning problem is to ﬁnd a hypothesis whose expected risk is low, using only
the training sample as evidence.

Parametric models attempt to solve the supervised learning problem by ex-
plicitly modeling either the joint distribution D(x, y) or the conditional distri-
butions D(y|x) for all x.

In the joint distribution case, there is a parameterized probability distribution
P (x, y|Θ). As the parameter values Θ are varied the distribution will also
vary. The parameter space Ω is a set of possible parameter values for which

P (x, y|Θ) is a well-deﬁned distribution (i.e., for which Px,y P (x, y|Θ) = 1).

A crucial assumption in parametric approaches is that there is some Θ∗ ∈ Ω
such that D(x, y) = P (x, y|Θ∗).


Because of this, if we consider the function ˆh(x) =
arg maxy∈Y P (x, y| ˆΘ), then in the limit ˆh(x) will converge to the Bayes op-
timal function hB(x). So under the assumption that D(x, y) = P (x, y|Θ∗)
for some Θ∗ ∈ Ω, and with inﬁnite amounts of training data, the maximum-
likelihood method is provably optimal.

Methods which model the conditional distribution D(y|x) are similar. The
parameters now deﬁne a conditional distribution P (y|x, Θ). The assumption
is that there is some Θ∗ such that ∀x, D(y|x) = P (y|x, Θ∗). Maximum-
likelihood estimates can be deﬁned in a similar way, and in this case the func-
tion ˆh(x) = arg maxy∈Y P (y|x, ˆΘ) will converge to the Bayes optimal func-
tion hB(x) as the sample size goes to inﬁnity.


From the arguments in the previous section, parametric methods are optimal

provided that two assumptions hold:

1 The distribution generating the data is in the class of distributions being

considered.

2 The training set is large enough for the distribution deﬁned by the maximum-

likelihood estimates to converge to the “true” distribution D(x, y) (in
general the guarantees of ML estimation are asymptotic, holding only in
the limit as the training data size goes to inﬁnity).

This paper proposes alternatives to maximum-likelihood methods which
give theoretical guarantees without making either of these assumptions. There
is no assumption that the distribution generating the data comes from some
predeﬁned class – the only assumption is that the same, unknown distribution
generates both training and test examples. The methods also provide bounds
suggesting how many training samples are required for learning, dealing with
the case where there is only a ﬁnite amount of training data.

A crucial idea in distribution-free learning is that of a hypothesis space.
This is a set of functions under consideration, each member of the set being
a function h : X → Y. For example, in weighted context-free grammars the
hypothesis space is.

So each possible parameter setting deﬁnes a different function from sentences
to trees, and H is the inﬁnite set of all such functions as Θ ranges over the
parameter space ℜn.

Learning is then usually framed as the task of choosing a “good” function
in H on the basis of a training sample as evidence. 

This strategy is called “Empirical Risk Minimization” (ERM) by Vapnik (1998).
Two questions which arise are:

In the limit, as the training size goes to inﬁnity, does the error of the ERM
method Er(ˆh) approach the error of the best function in the set, Er(h∗),
regardless of the underlying distribution D(x, y)? In other words, is this
method of choosing a hypothesis always consistent?

The answer to this depends on the nature of the hypothesis space H. For
ﬁnite hypothesis spaces the ERM method is always consistent. For many
inﬁnite hypothesis spaces, such as the hyperplane classiﬁers described
in section 6 of this paper, the method is also consistent. However, some
inﬁnite hypothesis spaces can lead to the method being inconsistent –
speciﬁcally, if a measure called the Vapnik-Chervonenkis (VC) dimen-
sion (Vapnik and Chervonenkis, 1971) of H is inﬁnite, the ERM method
may be inconsistent. Intuitively, the VC dimension can be thought of as
a measure of the complexity of an inﬁnite set of hypotheses.

If the method is consistent, how quickly does Er(ˆh) converge to Er(h∗)?
In other words, how much training data is needed to have a good chance
of getting close to the best function in H? We will see in the next section
that the convergence rate depends on various measures of the “size” of
the hypothesis space. For ﬁnite sets, the rate of convergence depends
directly upon the size of H. For inﬁnite sets, several measures have
been proposed – we will concentrate on rates of convergence based on a
concept called the margin of a hypothesis on training examples.

This section gives results and analysis for situations where the hypothesis
space H is a ﬁnite set. This is in some ways an unrealistically simple situation
– many hypothesis spaces used in practice are inﬁnite sets – but we give the
results and proofs because they can be useful in developing intuition for the
nature of convergence bounds. In the following sections we consider inﬁnite
hypothesis spaces such as weighted context-free grammars.

A couple of basic results from probability theory will be very useful. The
ﬁrst results are the Chernoff bounds. Consider a binary random variable X
(such as the result of a coin toss) which has probability p of being 1, and
(1 − p) of being 0. Now consider a sample of size m, {x1, x2, . . . , xm} drawn
from this process. Deﬁne the relative frequency of xi = 1 (the coin coming

up heads) in this sample to be ˆp = Pi xi/m. The relative frequency ˆp is a

very natural estimate of the underlying probability p, and by the law of large
numbers ˆp will converge to p as the sample size m goes to inﬁnity. Chernoff
bounds give results concerning how quickly ˆp converges to p. Thus Chernoff
bounds go a step further than the law of large numbers, which is an asymptotic
result (a result concerning what happens as the sample size goes to inﬁnity).
The bounds are:

Theorem 1 (Chernoff Bounds). For all p ∈ [0, 1], ǫ > 0, with the probability
P being taken over the distribution of training samples of size m generated
with underlying parameter p.

The ﬁrst bound states that for all values of p, and for all values of ǫ, if we
repeatedly draw training samples of size m of a binary variable with underlying
probability p, the relative proportion of training samples for which the value
(p− ˆp) exceeds ǫ is at most3 e−2mǫ2. 

It is always possible for ˆp to diverge substantially from
p – it is possible to draw an extremely unrepresentative training sample, such
as a sample of all heads when p = 0.7, for example – but as the sample size is
increased the chances of us being this unlucky become increasingly unlikely.

A second useful result is the Union Bound:

Here we use the notation P [A∪B] to mean the probability of A or B occurring.
The Union Bound follows directly from the axioms of probability theory. For
example, if n = 2, then P [A1 ∪ A2] = P [A1] + P [A2] − P [A1A2] ≤ P [A1] +
P [A2], where P [A1A2] means the probability of both A1 and A2 occurring.
The more general result for all n follows by induction on n.

We are now in a position to apply these results to learning problems. First,
consider just a single member of H, a function h. Say we draw a training sam-
ple {(x1, y1), . . . , (xm, ym)} from some unknown distribution D(x, y). We
can calculate the relative frequency of errors of h on this sample,

So for any single member of H, the Chernoff bound describes how its observed
error on the training set is related to its true probability of error. Now consider
the entire set of hypotheses H. Say we assign an arbitrary ordering to the
n = |H| hypotheses, so that H = {h1, h2, . . . , hn}. Consider the probability
of any one of the hypotheses hi having its estimated loss ˆEr(hi) diverge by
more than ǫ from its expected loss Er(hi).


Thus for all hypotheses h in the set H, ˆEr(h) converges to Er(h) as the sample
size m goes to inﬁnity. This result is known as a Uniform Convergence Result,
in that it describes how a whole set of empirical error rates converge to their
respective expected errors. Note that this result holds for the hypothesis with
minimum error on the training sample. It can be shown that this implies that the
ERM method for ﬁnite hypothesis spaces – choosing the hypothesis ˆh which
has minimum error on the training sample – is consistent, in that in the limit as
m → ∞, the error of ˆh converges to the error of the minimum error hypothesis.
Another important result is how the rate of convergence depends on the
size of the hypothesis space. Qualitatively, the bound implies that to avoid
overtraining the number of training samples should scale with log |H|.

Ideally, we would like a learning method to have expected error that is close
Now consider the ERM

to the loss of the bayes-optimal hypothesis hB.
method. 

Breaking the error down in this way suggests that there are two components to
the difference from the optimal loss Er(hB). The ﬁrst term captures the errors
due to a ﬁnite sample size – if the hypothesis space is too large, then theorem 3
states that there is a good chance that the ERM method will pick a hypothesis
that is far from the best in the hypothesis space, and the ﬁrst term will be large.
Thus the ﬁrst term indicates a pressure to keep H small, so that there is a good


chance of ﬁnding the best hypothesis in the set. In contrast, the second term
reﬂects a pressure to make H large, so that there is a good chance that at least
one of the hypotheses is close to the Bayes optimal hypothesis. The two terms
can be thought of as being analogues to the familiar “bias–variance” trade-off,
the ﬁrst term being a variance term, the second being the bias.

In this section we describe a method which explicitly attempts to model the
trade-off between these two types of errors. Rather than picking a single hy-
pothesis class, Structural Risk Minimization (Vapnik, 1998) advocates picking
a set of hypothesis classes H1, H2, . . . , Hs of increasing size (i.e., such that
|H1| < |H2| < · · · < |Hs|). The following theorem then applies (it is an
extension of theorem 3, and is derived in a similar way through application of
the Chernoff and Union bounds):

Theorem 4 Assume a set of ﬁnite hypothesis classes {H1, H2, . . . , Hs}, and
some distribution D(x, y). For all i = 1, . . . , s, for all hypotheses h ∈ Hi,
with probability at least 1 − δ over the choice of training set of size m drawn
from D.

This theorem is very similar to theorem 3, except that the second term in the
bound now varies depending on which Hi a function h is drawn from. Note
also that we pay an extra price of log(s) for our hedging over which of the
hypothesis spaces the function is drawn from. The SRM principle is then as
follows:

1 Pick a set of hypothesis classes, Hi for i = 1, . . . , s, of increasing size.
This must be done independently of the training data for the above bound
to apply.

2 Choose the hypothesis h which minimizes the bound in theorem 4.

Thus rather than simply choosing the hypothesis with the lowest error on
the training sample, there is now a trade-off between training error and the size
of the hypothesis space of which h is a member. The SRM method advocates
picking a compromise between keeping the number of training errors small
versus keeping the size of the hypothesis class small.

Note that this approach has a somewhat similar ﬂavour to Bayesian ap-
proaches. The Maximum A-Posteriori (MAP) estimates in a Bayesian ap-
proach involve choosing the parameters which maximize a combination of the
data likelihood and a prior over the parameter values.

The ﬁrst term is a measure of how well the parameters Θ ﬁt the data. The
second term is a prior which can be interpreted as a term which penalizes
more complex parameter settings. The SRM approach in our example implies
choosing the hypothesis that minimizes the bound in theorem 4.

The function indi-
cating the “goodness” of a hypothesis h again has two terms, one measuring
how well the hypothesis ﬁts the data, the second penalizing hypotheses which
are too “complex”. Here complexity has a very speciﬁc meaning: it is a di-
rect measure of how quickly the training data error ˆEr(h) converges to its true
value Er(h).

This section describes analysis applied for binary classiﬁers, where the set
Y = {−1, +1}. We consider hyperplane classiﬁers, where a linear separator
in some feature space is used to separate examples into the two classes. This
section describes uniform convergence bounds for hyperplane classiﬁers. Al-
gorithms which explicitly minimize these bounds – namely the Support Vector
Machine and Boosting algorithms – are described in section 8.

There has been a large amount of research devoted to the analysis of hyper-
plane classiﬁers. They go back to one of the earliest learning algorithms, the
Perceptron algorithm (Rosenblatt, 1958). They are similar to the linear models
for parsing we proposed in section 2 (in fact the framework of section 2 can be
viewed as a generalization of hyperplane classiﬁers). We will initially review
some results applying to linear classiﬁers, and then discuss how various results
may be applied to linear models for parsing.

We will discuss a hypothesis space of n-dimensional hyperplane classiﬁers,

deﬁned as follows:

Each instance x is represented as a vector Φ(x) in ℜn.

 There is a clear geometric
interpretation of this classiﬁer. The points Φ(x) are in n-dimensional
Euclidean space. The parameters Θ, b deﬁne a hyperplane through the


space, the hyperplane being the set of points z such that (z · Θ + b) = 0.
This is a hyperplane with normal Θ, at distance b/||Θ|| from the origin,
j . This hyperplane is used
to classify points: all points falling on one side of the hyperplane are
classiﬁed as +1, points on the other side are classiﬁed as −1.

It can be shown that the ERM method is consistent for hyperplanes, through
a method called VC analysis (Vapnik and Chervonenkis, 1971). We will not
go into details here, but roughly speaking, the VC-dimension of a hypothesis
space is a measure of its size or complexity. A set of hyperplanes in ℜn has
VC dimension of (n + 1). For any hypothesis space with ﬁnite VC dimension
the ERM method is consistent.

An alternative to VC-analysis is to analyse hyperplanes through properties
of “margins” on training examples. For any hyperplane deﬁned by parameters
(Θ, b), for a training sample {(x1, y1), . . . , (xm, ym)}, the margin on the i-th
training example is deﬁned as

The margin γΘ,b has a simple geometric interpretation: it is the minimum dis-
tance of any training point to the hyperplane deﬁned by Θ, b. The following
theorem then holds:

Theorem 5 (Shawe-Taylor et al. 1998). Assume the hypothesis class H is
a set of hyperplanes, and that there is some distribution D(x, y) generating
examples. Deﬁne R to be a constant such that ∀x, ||Φ(x)|| ≤ R. For all

The bound is minimized for the hyperplane with maximum margin (i.e., max-
imum value for γΘ,b) on the training sample. This bound suggests that if the
training data is separable, the hyperplane with maximum margin should be
chosen as the hypothesis with the best bound on its expected error.
It can
be shown that the maximum margin hyperplane is unique, and can be found
efﬁciently using algorithms described in section 8.1. Search for the maximum-
margin hyperplane is the basis of “Support Vector Machines” (hard-margin
version; Vapnik, 1998).

The previous theorem does not apply when the training data cannot be clas-
siﬁed with 0 errors by a hyperplane. There is, however, a similar theorem that
can be applied in the non-separable case. First, deﬁne ˆL(Θ, b, γ) to be the pro-
portion of examples on training data with margin less than γ for the hyperplane
hΘ,b:

The following theorem can now be stated:

Theorem 6 Cristianini and Shawe-Taylor, 2000, Theorem 4.19. Assume the
hypothesis class H is a set of hyperplanes, and that there is some distribution
D(x, y) generating examples. Let R be a constant such that ∀x, ||Φ(x)|| ≤ R.
For all hΘ,b ∈ H, for all γ > 0, with probability at least 1 − δ over the choice
of training set of size m drawn from D.

(The ﬁrst result of the form of Theorem 6 was given in (Bartlett 1998). This
was a general result for large margin classiﬁers; the immediate corollary that
implies the above theorem was given in (Anthony and Bartlett 1999). Note that
Zhang (2002) proves a related theorem where the log2 m factor is replaced by
log m. Note also that the square-root in the second term of theorem 6 means
that this bound is in general a looser bound than the bound in theorem 5. This is
one cost of moving to the case where some training samples are misclassiﬁed,
or where some training samples are classiﬁed with a small margin.)

This result is important in cases where a large proportion of training samples
can be classiﬁed with relatively large margin, but a relatively small number of
outliers make the problem inseparable, or force a small margin. The result
suggests that in some cases a few examples are worth “giving up on”, resulting
in the ﬁrst term in the bound being larger than 0, but the second term being
much smaller due to a larger value for γ. The soft margin version of Support
Vector Machines (Cortes and Vapnik, 1995), described in section 8.1, attempts
to explicitly manage the trade-off between the two terms in the bound.

Theorem 7 (Schapire et al., 1998). Assume the hypothesis class H is a set
of hyperplanes in ℜn, and that there is some distribution D(x, y) generating
examples.

This bound suggests a strategy that keeps the 1-norm of the parameters low,
while trying to classify as many of the training examples as possible with large
margin. It can be shown that the AdaBoost algorithm (Freund and Schapire,
1997) is an effective way of achieving this goal; its application to parsing is
described in section 8.2.

We now consider how the theory for hyperplane classiﬁers might apply to
the linear models for parsing described in section 2.

The method for converting parsing to a margin-based problem is similar to
the method for ranking problems described in (Freund et al., 1998), and to
the approach to multi-class classiﬁcation problems in (Schapire et al., 1998;
Crammer and Singer, 2001; Elisseeff et al., 1999). As a ﬁrst step, we give a
deﬁnition of the margins on training examples. Assume we have a training
sample {(x1, y1), . . . , (xm, ym)}.

Theorem 8 Assume the hypothesis class H is a set of linear models as deﬁned
in equation (1.11) and equation (1.12), and that there is some distribution
D(x, y) generating examples. For all hΘ ∈ H, for all γ > 0, with probability
at least 1 − δ over the choice of training set of size m drawn from D,

where R is a constant such that ∀x ∈ X , ∀y ∈ GEN(x), ∀z ∈ GEN(x),
||Φ(x, y) − Φ(x, z)|| ≤ R. The variable N is the smallest positive integer
such that ∀x ∈ X , |GEN(x)| − 1 ≤ N.

Proof: The proof follows from results in (Zhang, 2002). See the appendix of
this chapter for the proof.

Note that this is similar to the bound in theorem 6. A difference, however, is
the dependence on N, a bound on the number of candidates for any example.
Even though this term is logarithmic, the dependence is problematic because
the number of candidate parses for a sentence will usually have an exponen-
tial dependence on the length of the sentence, leading to log N having linear

dependence on the maximum sentence length. (For example, the number of la-
beled binary-branching trees for a sentence of length n, with G non-terminals,
is Gn(2n)!
(n+1)!n! , the log of this number is O(n log G + n log n).) It is an open prob-
lem whether tighter bounds – in particular, bounds which do not depend on N
– can be proved. Curiously, we show in section 8.3 that the perceptron algo-
rithm leads to a margin-based learning bound that is independent of the value
for N. This suggests that it may be possible to prove tighter bounds than those
in theorem 8.

The bounds in theorems 8 and 9 suggested a trade-off between keeping the
values for ˆL(Θ, γ) and ˆL1(Θ, γ) low and keeping the value of γ high. The
algorithms described in section 8 attempt to ﬁnd a hypothesis Θ which can
achieve low values for these quantities with a high value for γ. The algorithms
are direct modiﬁcations of algorithms for learning hyperplane classiﬁers for bi-
nary classiﬁcation: these classiﬁcation algorithms are motivated by the bounds
in theorems 6 and 7.

In this section we describe parameter estimation algorithms which are moti-
vated by the generalization bounds for linear models in section 7 of this paper.
The ﬁrst set of algorithms, support vector machines, use constrained optimiza-
tion problems that are related to the bounds in theorems 8 and 9. The second
algorithm we describe is a modiﬁcation of AdaBoost (Freund and Schapire,
1997), which is motivated by the bound in theorem 9. Finally, we describe
a variant of the perceptron algorithm applied to parsing. The perceptron al-
gorithm does not explicitly attempt to optimize the generalization bounds in
section 7, but its convergence and generalization properties can be shown to
be dependent on the existence of parameter values which separate the training
data with large margin under the 2-norm. In this sense they are a close relative
to support vector machines.

We now describe an algorithm which is motivated by the bound in theo-
rem 8. First, recall the deﬁnition of the margin for the parameter values Θ on
the i-th training example.



Vapnik (1998) shows that the hyperplane Θ∗ is unique4, and gives a method
for ﬁnding Θ∗. The method involves solving the following constrained opti-
mization problem:

Any hyperplane Θ satisfying these constraints separates the data with margin
γΘ = 1/||Θ||. By minimizing ||Θ||2 (or equivalently ||Θ||) subject to the
constraints, the method ﬁnds the parameters Θ with maximal value for γΘ.

Simply ﬁnding the maximum-margin hyperplane may not be optimal or
even possible: the data may not be separable, or the data may be noisy. The
bound in theorem 8 suggests giving up on some training examples which may
be difﬁcult or impossible to separate.
(Cortes and Vapnik, 1995) suggest a
reﬁned optimization task for the classiﬁcation case which addresses this prob-
lem; we suggest the following modiﬁed optimization problem as a natural ana-
logue of this approach (our approach is similar to the method for multi-class
classiﬁcation problems in Crammer and Singer, 2001):

Here we have introduced a “slack variable” ǫi for each training example. At the
solution of the optimization problem, the margin on the i-th training example is
at least (1−ǫi)/||Θ||. On many examples the slack variable ǫi will be zero, and
the margin γi
Θ will be at least 1/||Θ||. On some examples the slack variable
ǫi will be positive, implying that the algorithm has “given up” on separating
the example with margin 1/||Θ||. The constant C controls the cost for having
non-zero values of ǫi. As C → ∞, the problem becomes the same as the hard-
margin SVM problem, and the method attempts to ﬁnd a hyperplane which
correctly separates all examples with margin at least 1/||Θ|| (i.e., all slack
variables are 0). For smaller C, the training algorithm may “give up” on some
examples (i.e., set ǫi > 0) in order to keep ||Θ||2 low. Thus by varying C, the
method effectively modiﬁes the trade-off between the two terms in the bound
in theorem 8. In practice, a common approach is to train the model for several
values of C, and then to pick the classiﬁer which has best performance on some
held-out set of development data.

Both kinds of SVM optimization problem outlined above have been studied
extensively (e.g., see Joachims, 1998; Platt, 1998) and can be solved relatively
efﬁciently. (A package for SVMs, written by Thorsten Joachims, is available
from http://ais.gmd.de/˜thorsten/svm light/.)

This can be framed as a linear programming problem. See (Demiriz et al.,
2001) for details, and the relationships between linear programming approaches
and the boosting algorithms described in the next section.

The AdaBoost algorithm (Freund and Schapire, 1997) is one method for
optimizing the bound for hyperplane classiﬁers in theorem 7 (Schapire et al.,
1998).
This section describes a modiﬁed version of AdaBoost, applied to
the parsing problem. Figure 1.2 shows the modiﬁed algorithm. The algorithm
converts the training set into a set of triples:

Each member (x, y1, y2) of T is a triple such that x is a sentence, y1 is the
correct tree for that sentence, and y2 is an incorrect tree also proposed by
GEN(x). AdaBoost maintains a distribution Dt over the training examples
such that Dt(x, y1, y2) is proportional to exp{−Θ · (Φ(x, y1) − Φ(x, y2))}.
Members of T which are well discriminated by the current parameter values
Θ are given low weight by the distribution, whereas examples which are poorly
discriminated are weighted more highly.

The magnitude of
rs can be taken as a measure of how correlated (Φs(x, y1) − Φs(x, y2)) is
with the distribution Dt. If it is highly correlated, |rs| will be large, and the
s-th parameter will be useful in driving down the margins on the more highly
weighted members of T .

In the classiﬁcation case, Schapire et al. (1998) show that the AdaBoost
algorithm has direct properties in terms of optimizing the value of ˆL1(Θ, b, γ)
deﬁned in equation (1.10). Unfortunately it is not possible to show that the
algorithm in ﬁgure 1.2 has a similar effect on the parsing quantity ˆL1(Θ, γ) in
equation (1.15). Instead, we show its effect on a similar quantity5 ˆRL1:


There is a strong relation between the values of |rs|, and the effect on the
values of ˆRL1(Θ, γ). If we deﬁne ǫt = (1 − |rst|)/2 then the following theo-
rem holds:

It can be shown that f (δ, γ) is less than one providing that γ < δ: the implica-
tion is that for all γ < δ, ˆRL1(Θ, γ) decreases exponentially in the number of
iterations, T . So if the AdaBoost algorithm can successfully maintain high val-
ues of |rst| for several iterations, it will be successful at minimizing ˆRL1(Θ, γ)
for a relatively large range of γ. Given that ˆRL1 is related to ˆL1, we can view
this as an approximate method for optimizing the bound in theorem 9. In prac-
tice, a set of held-out data is usually used to optimize T , the number of rounds
of boosting.

The algorithm states a restriction on the representation Φ. For all members
(x, y1, y2) of T , for s = 1, . . . , n, (Φs(x, y1) − Φs(x, y2)) must be in the range
−1 to +1. This is not as restrictive as it might seem. If Φ is always strictly
positive, it can be rescaled so that its components are always between 0 and +1.
If some components may be negative, it sufﬁces to rescale the components so
that they are always between −0.5 and +0.5. A common use of the algorithm,
as applied in (Collins, 2000), is to have the n components of Φ to be the values
of n indicator functions, in which case all values of Φ are either 0 or 1, and the
condition is satisﬁed.

The ﬁnal parameter estimation algorithm which we will describe is a variant
of the perceptron algorithm, as introduced by (Rosenblatt, 1958). Figure 1.3
shows the algorithm. Note that the main computational expense is in calculat-
ing y = hΘ(xi) for each example in turn. For weighted context-free grammars

this step can be achieved in polynomial time using the CKY parsing algorithm.
Other representations may have to rely on explicitly calculating Φ(xi, z) · Θ
for all z ∈ GEN(xi), and hence depend computationally on the number of
candidates |GEN(xi)| for i = 1, . . . , m.

Proof: See (Collins, 2002b) for a proof. The proof is a simple modiﬁcation
of the proof for hyperplane classiﬁers (Block, 1962; Novikoff, 1962, see also
Freund and Schapire, 1999).

This theorem implies that if the training sample in ﬁgure 1.3 is separable,
and we iterate the algorithm repeatedly over the training sample, then the algo-
rithm converges to a parameter setting that classiﬁes the training set with zero
errors. (In particular, we need at most (R/γ)2 passes over the training sample
before convergence.) Thus we now have an algorithm for training weighted
context-free grammars which will ﬁnd a zero error hypothesis if it exists. For
example, the algorithm would ﬁnd a weighted grammar with zero expected
error on the example problem in section 3.

Of course convergence to a zero-error hypothesis on training data says little
about how well the method generalizes to new test examples. Fortunately a
second theorem gives a bound on the generalization error of the perceptron
method:

Theorem 12 (Direct consequence of the sample compression bound in (Lit-
tlestone and Warmuth, 1986); see also theorem 4.25, page 70, Cristianini and
Shawe-Taylor, 2000). Say the perceptron algorithm makes d mistakes when
run to convergence over a training set of size m. 

Given that d ≤ (R/γ)2, this bound states that if the problem is separable with
large margin – i.e., the ratio R/γ is relatively small – then the perceptron will
converge to a hypothesis with good expected error with a reasonable number
of training examples.

The perceptron algorithm is remarkable in a few respects. First, the algo-
rithm in ﬁgure 1.3 can be efﬁcient even in cases where GEN(x) is of expo-
nential size in terms of the input x, providing that the highest scoring structure
can be found efﬁciently for each training example. For example, ﬁnding the
arg max can be achieved in polynomial time for context-free grammars, so
they can be trained efﬁciently using the algorithm. This is in contrast to the
support vector machine and boosting algorithms, where we are not aware of
algorithms whose computational complexity does not depend on the size of
GEN(xi) for i = 1, . . . , n. Second, the convergence properties (number of
updates) of the algorithm are also independent of the size of GEN(xi) for
i = 1, . . . , n, depending on the maximum achievable margin γ on the training
set. Third, the generalization theorem (theorem 12) shows that the generaliza-
tion properties are again independent of the size of each GEN(xi), depending
only on γ. This is in contrast to the bounds in theorems 8 and 9, which de-
pended on N, a bound on the number of candidates for any input.

The theorems quoted here do not treat the case where the data is not sepa-
rable, but results for the perceptron algorithm can also be derived in this case.
See (Freund and Schapire, 1999) for analysis of the classiﬁcation case, and see
(Collins, 2002b) for how these results can be carried over to problems such as
parsing. Collins (2002b) shows how the perceptron algorithm can be applied
to tagging problems, with improvements in accuracy over a maximum-entropy
tagger on part-of-speech tagging and NP chunking; see this paper for more
analysis of the perceptron algorithm, and some modiﬁcations to the basic al-
gorithm.

In this section we give further discussion of the algorithms in this chap-
ter. Section 9.1 describes experimental results using some of the algorithms.
Section 9.2 describes relationships to Markov Random Field approaches.

There are several papers describing experiments on NLP tasks using the
algorithms described in this paper. Collins (2000) describes a boosting method

which is related to the algorithm in ﬁgure 1.2. In this case GEN(x) is the top
N most likely parses from the parser of (Collins, 1999). The representation
Φ(x, y) combines the log probability under the initial model, together with a
large number of additional indicator functions which are various features of
trees. The paper describes a boosting algorithm which is particularly efﬁcient
when the features are indicator (binary-valued) functions, and the features are
relatively sparse. The method gives a 13% relative reduction in error over
the original parser of (Collins, 1999). (See (Ratnaparkhi et al., 1994) for an
approach which also uses a N-best output from a baseline model combined
with “global” features, but a different algorithm for training the parameters of
the model.)

Collins (2002a) describes a similar approach applied to named entity extrac-
tion. GEN(x) is the top 20 most likely hypotheses from a maximum-entropy
tagger. The representation again includes the log probability under the origi-
nal model, together with a large number of indicator functions. The boosting
and perceptron algorithms give relative error reductions of 15.6% and 17.7%
respectively.

Collins and Duffy (2002) and Collins and Duffy (2001) describe the per-
ceptron algorithm applied to parsing and tagging problems. GEN(x) is again
the top N most likely parses from a baseline model. The particular twist in
these papers is that the representation Φ(x, y) for both the tagging and pars-
ing problems is an extremely high-dimensional representation, which tracks all
subtrees in the parsing case (in the same way as the DOP approach to parsing,
see Bod, 1998), or all sub-fragments of a tagged sequence. The key to making
the method computationally efﬁcient (in spite of the high dimensionality of Φ)
is that for any pair of structures (x1, y1) and (x2, y2) it can be shown that the
inner product Φ(x1, y1) · Φ(x2, y2) can be calculated efﬁciently using dynamic
programming. The perceptron algorithm has an efﬁcient “dual” implementa-
tion which makes use of inner products between examples – see (Cristianini
and Shawe-Taylor, 2000; Collins and Duffy, 2002). Collins and Duffy (2002)
show a 5% relative error improvement for parsing, and a more signiﬁcant 15%
relative error improvement on the tagging task.

Collins (2002b) describes perceptron algorithms applied to the tagging task.
GEN(x) for a sentence x of length n is the set of all possible tag sequences
of length n (there are T n such sequences if T is the number of tags). The repre-
sentation used is similar to the feature-vector representations used in maximum-
entropy taggers, as in (Ratnaparkhi, 1996). The highest scoring tagged se-
quence under this representation can be found efﬁciently using the percep-
tron algorithm, so the weights can be trained using the algorithm in ﬁgure 1.3
without having to exhaustively enumerate all tagged sequences. The method
gives improvements over the maximum-entropy approach: a 12% relative error


reduction for part-of-speech tagging, a 5% relative error reduction for noun-
phrase chunking.

Another method for training the parameters Θ can be derived from log-
linear models, or Markov Random Fields (otherwise known as maximum-
entropy models).
Several approaches (Ratnaparkhi et al., 1994; Johnson
et al., 1999; Lafferty et al., 2001) use the parameters Θ to deﬁne a conditional
probability distribution over the candidates y ∈ GEN(x):

Once the model is trained, the output on a new sentence x is the highest prob-
ability parse, arg maxy∈GEN(x) P (y | x, Θ) = arg maxy∈GEN(x) Φ(x, y) · Θ.
So the output under parameters Θ is identical to the method used throughout
this paper.

The differences between this method and the approaches advocated in this
paper are twofold. First, the statistical justiﬁcation differs: the log-linear ap-
proach is a parametric approach (see section 4.2), explicitly attempting to
model the conditional distribution D(y | x), and potentially suffering from
the problems described in section 4.3.

The second difference concerns the algorithms for training the parameters.
In training log-linear models, a ﬁrst crucial concept is the log-likelihood of the
training data,


Parameter estimation methods in the MRF framework generally involve maxi-
mizing the log-likelihood while controlling for overﬁtting the training data. A
ﬁrst method for controlling the degree of overﬁtting, as used in (Ratnaparkhi
et al., 1994), is to use feature selection. In this case a greedy method is used to
minimize the log likelihood using only a small number of features. It can be
shown that the boosting algorithms can be considered to be a feature selection
method for minimizing the exponential loss.

A second method for controlling overﬁtting, used in (Johnson et al., 1999;
Lafferty et al., 2001), is to use a gaussian prior over the parameters. The
method then selects the MAP parameters – the parameters which maximize
the objective function

for some constant C which is determined by the variance term in the gaussian
prior. This method has at least a superﬁcial similarity to the SVM algorithm
in section 8.1 (2-norm case), which also attempts to balance the norm of the
parameters versus a function measuring how well the parameters ﬁt the data
(i.e., the sum of the slack variable values).

We should stress again, however, that in spite of some similarities between
the algorithms for MRFs and the boosting and SVM methods, the statistical
justiﬁcation for the methods differs considerably.


This paper has described a number of methods for learning statistical gram-
mars. All of these methods have several components in common: the choice
of a grammar which deﬁnes the set of candidates for a given sentence, and
the choice of representation of parse trees. A score indicating the plausibility
of competing parse trees is taken to be a linear model, the result of the inner
product between a tree’s feature vector and the vector of model parameters.
The only respect in which the methods differ is in how the parameter values
(the “weights” on different features) are calculated using a training sample as
evidence.

Section 4 introduced a framework under which various parameter estimation
methods could be studied. This framework included two main components.
First, we assume some ﬁxed but unknown distribution over sentence/parse-
tree pairs. Both training and test examples are drawn from this distribution.
Second, we assume some loss function, which dictates the penalty on test ex-
amples for proposing a parse which is incorrect. We focused on a simple loss
function, where the loss is 0 if the proposed parse is identical to the correct
parse, 1 otherwise. Under these assumptions, the “quality” of a parser is its
expected loss (expected error rate) on newly drawn test examples. The goal of


learning is to use the training data as evidence for choosing a function which
has small expected loss.

A central idea in the analysis of learning algorithms is that of the margins
on examples in training data. We described theoretical bounds which motivate
approaches which attempt classify a large proportion of examples in training
with a large margin. Finally, we described several algorithms which can be
used to achieve this goal on the parsing problem.

There are several open problems highlighted in this paper:

The margin bounds for parsing (theorems 8 and 9) both depend on N, a
bound on the number of candidates for any input sentence. It is an open
question whether bounds which are independent of N can be proved.
The perceptron algorithm in section 8.3 has generalization bounds which
are independent of N, suggesting that this might also be possible for the
margin bounds.

The Boosting and Support Vector Machine methods both require enu-
merating all members of GEN(xi) for each training example xi. The
perceptron algorithm avoided this in the case where the highest scoring
hypothesis could be calculated efﬁciently, for example using the CKY
algorithm. It would be very useful to derive SVM and boosting algo-
rithms whose computational complexity can be shown to depend on the
separation γ rather than the size of GEN(xi) for each training example
xi.

The boosting algorithm in section 8.2 optimized the quantity ˆRL1, rather
than the desired quantity ˆL1. It would be useful to derive a boosting
algorithm which provably optimized ˆL1.


I would like to thank Sanjoy Dasgupta, Yoav Freund, John Langford, David
McAllester, Rob Schapire and Yoram Singer for answering many of the ques-
tions I have had about the learning theory and algorithms in this paper. Fer-
nando Pereira pointed out several issues concerning analysis of the perceptron
algorithm. Thanks also to Nigel Duffy, for many useful discussions while we
were collaborating on the use of kernels for parsing problems. I would like to
thank Tong Zhang for several useful insights concerning margin-based gener-
alization bounds for multi-class problems. Thanks to Brian Roark for helpful
comments on an initial draft of this paper, and to Patrick Haffner for many use-
ful suggestions. Thanks also to Peter Bartlett, for feedback on the paper, and
some useful pointers to references.

The proofs in this section closely follow the framework and results of (Zhang, 2002). The
basic idea is to show that the covering number results of (Zhang, 2002) apply to the parsing
problem, with the modiﬁcation that any dependence on m (the sample size) is replaced by a
dependence on mN (where N is the smallest integer such that .

In the problems in this paper we again assume that sample points are (x, y) pairs, where
x ∈ X is an input, and y ∈ Y is the correct structure for that input. There is some function
GEN(x) which maps any x ∈ X to a set of candidates. There is also a function Φ : X ×
Y → ℜn that maps each (x, y) pair to a feature vector. We will transform any sample point
(x, y) to a matrix Z ∈ ℜN ×n in the following way. Take N to be a positive integer such that
|GEN(x)| = (N + 1).
Zhang (2002) shows how bounds on the covering numbers of L lead to the theorems 6 and 8 of
(Zhang, 2002), which are similar but tighter bounds than the bounds given in theorems 6 and 7
in section 6 of the current paper. Theorem A1 below states a relationship between the covering
numbers for L and M. Under this result, theorems 8 and 9 in the current paper follow from
the covering bounds on M in exactly the same way that theorems 6 and 8 of (Zhang, 2002) are

derived from the covering numbers of L, and theorem 2 of (Zhang, 2002). So theorem A1 leads
almost directly to theorems 8 and 9 in the current paper.