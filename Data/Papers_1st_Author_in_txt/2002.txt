Abstract. We give a uniﬁed account of boosting and logistic regression in which each learning problem is
cast in terms of optimization of Bregman distances. The striking similarity of the two problems in this frame-
work allows us to design and analyze algorithms for both simultaneously, and to easily adapt algorithms de-
signed for one problem to the other. For both problems, we give new algorithms and explain their potential
advantages over existing methods. These algorithms are iterative and can be divided into two types based
on whether the parameters are updated sequentially (one at a time) or in parallel (all at once). We also de-
scribe a parameterized family of algorithms that includes both a sequential- and a parallel-update algorithm as
special cases, thus showing how the sequential and parallel approaches can themselves be uniﬁed. For all of
the algorithms, we give convergence proofs using a general formalization of the auxiliary-function proof tech-
nique. As one of our sequential-update algorithms is equivalent to AdaBoost, this provides the ﬁrst general
proof of convergence for AdaBoost. We show that all of our algorithms generalize easily to the multiclass case,
and we contrast the new algorithms with the iterative scaling algorithm. We conclude with a few experimen-
tal results with synthetic data that highlight the behavior of the old and newly proposed algorithms in different
settings.

We give a uniﬁed account of boosting and logistic regression in which we show that both
learning problems can be cast in terms of optimization of Bregman distances. In our frame-
work, the two problems become very similar, the only real difference being in the choice of
Bregman distance: unnormalized relative entropy for boosting, and binary relative entropy
for logistic regression.

The similarity of the two problems in our framework allows us to design and analyze
algorithms for both simultaneously. We are now able to borrow methods from the maximum-
entropy literature for logistic regression and apply them to the exponential loss used by
AdaBoost, especially convergence-proof techniques. Conversely, we can now easily adapt
boosting methods to the problem of minimizing the logistic loss used in logistic regression.

The result is a family of new algorithms for both problems together with convergence proofs
for the new algorithms as well as AdaBoost.

For both AdaBoost and logistic regression, we attempt to choose the parameters or weights
associated with a given family of functions called features or, in the boosting literature, weak
hypotheses. AdaBoost works by sequentially updating these parameters one by one. That
is, on each of a series of iterations, a single feature (weak hypothesis) is chosen and the
parameter associated with that single feature is adjusted. In contrast, methods for logistic
regression, most notably iterative scaling (Darroch & Ratcliff, 1972; Della Pietra, Della
Pietra, & Lafferty, 1997), update all parameters in parallel on each iteration.

Our ﬁrst new algorithm is a method for optimizing the exponential loss using parallel
updates. It seems plausible that a parallel-update method will often converge faster than a
sequential-update method, provided that the number of features is not so large as to make
parallel updates infeasible. A few experiments described at the end of this paper suggest
that this is the case.

Our second algorithm is a parallel-update method for the logistic loss. Although parallel-
update algorithms are well known for this function, the updates that we derive are new.
Because of the uniﬁed treatment we give to the exponential and logistic loss functions,
we are able to present and prove the convergence of the algorithms for these two losses
simultaneously. The same is true for the other algorithms presented in this paper as well.

We next describe and analyze sequential-update algorithms for the two loss functions.
For exponential loss, this algorithm is equivalent to the AdaBoost algorithm of Freund and
Schapire (1997). By viewing the algorithm in our framework, we are able to prove that
AdaBoost correctly converges to the minimum of the exponential loss function. This is a
new result: Although Kivinen and Warmuth (1999) and Mason et al. (1999) have given
convergence proofs for AdaBoost, their proofs depend on assumptions about the given
minimization problem which may not hold in all cases. Our proof holds in general without
such assumptions.

Our uniﬁed view leads directly to a sequential-update algorithm for logistic regression
that is only a minor modiﬁcation of AdaBoost and which is very similar to the algorithm
proposed by Duffy and Helmbold (1999). Like AdaBoost, this algorithm can be used in
conjunction with any classiﬁcation algorithm, usually called the weak learning algorithm,
that can accept a distribution over examples and return a weak hypothesis with low error
rate with respect to the distribution. However, this new algorithm provably minimizes the
logistic loss rather than the arguably less natural exponential loss used by AdaBoost.

A potentially important advantage of the new algorithm for logistic regression is that the
weights that it places on examples are bounded in [0, 1]. This suggests that it may be possible
to use the new algorithm in a setting in which the boosting algorithm selects examples to
present to the weak learning algorithm by ﬁltering a stream of examples (such as a very
large dataset). As pointed out by Watanabe (1999) and Domingo and Watanabe (2000), this
is not possible with AdaBoost since its weights may become extremely large. They provide
a modiﬁcation of AdaBoost for this purpose in which the weights are truncated at 1. We
speculate that our new algorithm may lead to a viable and mathematically cleaner alternative.
We next describe a parameterized family of iterative algorithms that includes both
parallel- and sequential-update algorithms as well as a whole range of algorithms between

these two extremes. The convergence proof that we give holds for this entire family of
algorithms.

Although most of this paper considers only the binary case in which there are just two
possible labels associated with each example, it turns out that the multiclass case requires
no additional work. That is, all of the algorithms and convergence proofs that we give for
the binary case turn out to be directly applicable to the multiclass case without modiﬁcation.
For comparison, we also describe the generalized iterative scaling algorithm of Darroch
and Ratcliff (1972). In rederiving this procedure in our setting, we are able to relax one of
the main assumptions usually required by this algorithm.

The paper is organized as follows: Section 2 describes the boosting and logistic regression
models as they are usually formulated. Section 3 gives background on optimization using
Bregman distances, and Section 4 then describes how boosting and logistic regression can
be cast within this framework. Section 5 gives our parallel-update algorithms and proofs of
their convergence, while Section 6 gives the sequential-update algorithms and convergence
proofs. The parameterized family of iterative algorithms is described in Section 7. The
extension to multiclass problems is given in Section 8. In Section 9, we contrast our methods
with the iterative scaling algorithm. In Section 10, we discuss various notions of convergence
of AdaBoost and relate our results to previous work on boosting. In Section 11, we give
some initial experiments that demonstrate the qualitative behavior of the various algorithms
in different settings.

Variants of our sequential-update algorithms ﬁt into the general family of “arcing” algo-
rithms presented by Breiman (1999, 1997a), as well as Mason et al.’s “AnyBoost” family of
algorithms (Mason et al., 1999). The information-geometric view that we take also shows
that some of the algorithms we study, including AdaBoost, ﬁt into a family of algorithms
described in 1967 by Bregman (1967), and elaborated upon by Censor and Lent (1981), for
satisfying a set of constraints.1

Our work is based directly on the general setting of Lafferty, Della Pietra, and Della
Pietra (1997) in which one attempts to solve optimization problems based on general
Bregman distances. They gave a method for deriving and analyzing parallel-update al-
gorithms in this setting through the use of auxiliary functions. All of our algorithms and
convergence proofs are based on this method.

Our work builds on several previous papers which have compared boosting approaches
to logistic regression. Friedman, Hastie, and Tibshirani (2000) ﬁrst noted the similarity be-
tween the boosting and logistic regression loss functions, and derived the sequential-update
algorithm LogitBoost for the logistic loss. However, unlike our algorithm, theirs requires
that the weak learner solve least-squares problems rather than classiﬁcation problems.

Duffy and Helmbold (1999) gave conditions under which a loss function gives a boosting
algorithm. They showed that minimizing logistic loss does lead to a boosting algorithm in
the PAC sense. This suggests that the logistic loss algorithm of Section 6 of this paper,
which is close to theirs, may turn out also to have the PAC boosting property. We leave this
as an open problem.

Lafferty (1999) went further in studying the relationship between logistic regression and
the exponential loss through the use of a family of Bregman distances. However, the setting
described in his paper apparently cannot be extended to precisely include the exponential
loss. The use of Bregman distances that we describe has important differences leading to a
natural treatment of the exponential loss and a new view of logistic regression.

Our work builds heavily on that of Kivinen and Warmuth (1999) who, along with Lafferty,
were the ﬁrst to make a connection between AdaBoost and information geometry. They
showed that the update used by AdaBoost is a form of “entropy projection.” However,
the Bregman distance that they used differed slightly from the one that we have chosen
(normalized relative entropy rather than unnormalized relative entropy) so that AdaBoost’s
ﬁt in this model was not quite complete; in particular, their convergence proof depended on
an assumption that does not hold in general.2 Kivinen and Warmuth also described updates
for general Bregman distances including, as one of their examples, the Bregman distance
that we use to capture logistic regression.

Cesa-Bianchi, Krogh, and Warmuth (1994) describe an algorithm for a closely related
problem to ours: minimization of a relative entropy subject to linear constraints. In related
work, Littlestone, Long, and Warmuth (1995) describe algorithms where convergence prop-
erties are analyzed through a method that is similar to the auxiliary function techniques.
A variety of work in the online learning literature, such as the work by Littlestone, Long,
and Warmuth (1995) and the work by Kivinen and Warmuth (1997, 2001) on exponentiated
gradient methods, also use Bregman divergences, and techniques that are related to the
auxiliary function method.

Let S = (cid:4)(x1, y1), . . . , (xm , ym )(cid:5) be a set of training examples where each instance xi
belongs to a domain or instance space X , and each label yi ∈ {−1,+1}.
We assume that we are also given a set of real-valued functions on X , h1, . . . , hn. Fol-
lowing convention in the Maximum-Entropy literature, we call these functions features; in
the boosting literature, these would be called weak or base hypotheses.
fλ(xi ) = (cid:2)
We study the problem of approximating the yi ’s using a linear combination of features.
That is, we are interested in the problem of ﬁnding a vector of parameters λ ∈ Rn such that
λ j h j (xi ) is a “good approximation” of yi . 

Although minimization of the number of
classiﬁcation errors may be a worthwhile goal, in its most general form, the problem is
intractable (see, for instance, H¨offgen & Simon, 1992). It is therefore often advantageous to
instead minimize some other nonnegative loss function. 

AdaBoost is usually described as a procedure that works together with an oracle or
subroutine called the weak learner. Brieﬂy, on each of a series of rounds, the weak learner
picks one feature (weak hypothesis) h j . Note that the features h1, . . . , hn correspond to
the entire space of weak hypotheses rather than merely the weak hypotheses that were
previously found up to that point by the weak learner. Of course, this will often be an
enormous space, but one, nevertheless, that can be discussed mathematically. In practice,
it may often be necessary to rely on a weak learner that can only approximately search the
entire space. For instance, greedy algorithms such as C4.5 are often used for this purpose
to ﬁnd a “good” decision tree from the space of all possible decision trees.
To simplify the discussion, let us suppose for the moment that all of the weak hypotheses
are Boolean, i.e., with range {−1,+1}. In this case, the weak learner attempts to choose
the weak hypothesis with smallest error rate, that is, with the smallest weighted number of
mistakes (in which h j (xi ) (cid:12)= yi ) relative to a distribution over training examples selected by
AdaBoost. Given the choice of weak hypothesis h j , AdaBoost then updates the associated
parameter λ j by adding some value α to it where α is a simple formula of this weighted
error rate (note that a parameter may be updated more than once in this framework).

As mentioned above, in practice, the weak learner may not always succeed in ﬁnding
the “best” h j (in the sense of minimizing weighted error rate), for instance, if the size of
the space of weak hypotheses precludes an exhaustive search. However, in this paper, we
make the idealized assumption that the weak learner always chooses the best h j . Given this
assumption, it has been noted by Breiman (1997a, 1999) and various later authors (Friedman,
Hastie, & Tibshirani, 2000; Mason et al., 1999; R¨atsch, Onoda, & M¨uller, 2001; Schapire
& Singer, 1999) that the choice of both h j and α are done in such a way as to cause the
greatest decrease in the exponential loss induced by λ, given that only a single component
of λ is to be updated. In this paper, we show for the ﬁrst time that AdaBoost is in fact a
provably effective method for ﬁnding parameters λ which minimize the exponential loss
(assuming, as noted above, that the weak learner always chooses the “best” h j ).

In practice, early stopping (limiting the number of rounds of boosting, rather than running
the algorithm to convergence) is often used to mitigate problems with overtraining. In
this case the sequential algorithms in this paper can be considered to be feature selection
methods, in that only a subset of the parameters will obtain non-zero values. Thus, the
sequential methods can be used both for feature selection, or for search for the minimum
of the loss function.

We also give an entirely new algorithm for minimizing exponential loss in which, on
each round, all of the parameters λ j are updated in parallel rather than one at a time.
Our hope is that in some situations this parallel-update algorithm will be faster than the
sequential-update algorithm. See Section 11 for preliminary experiments in this regard.

Instead of using fλ as a classiﬁcation rule, we might instead postulate that the yi ’s were
generated stochastically as a function of the xi ’s and attempt to use fλ(x) to estimate the
probability of the associated label y.

Generalized and improved iterative scaling (Darroch & Ratcliff, 1972; Della Pietra, Della
Pietra, & Lafferty, 1997) are popular parallel-update methods for minimizing this loss. In
this paper, we give an alternative parallel-update algorithm which we compare to iterative
scaling techniques in preliminary experiments in Section 11.

In this section, we give background on optimization using Bregman distances. This will
form the unifying basis for our study of boosting and logistic regression. The particular
set-up that we follow is taken primarily from Lafferty, Della Pietra, and Della Pietra (1997).
Let F :  → R be a strictly convex function deﬁned on a closed, convex set ⊆ Rm.
Assume F is differentiable at all points of int, the interior of , which we assume is
nonempty. 

Generally, although not always a
metric or even symmetric, it can be shown that every Bregman distance is nonnegative
and is equal to zero if and only if its two arguments are equal. We assume that BF can be
extended to a continuous extended real-valued function over all of  × .
There is a natural optimization problem that can be associated with a Bregman distance,
namely, to ﬁnd the vector p ∈  that is closest to a given vector q0 ∈  subject to a set of
linear constraints. In other words, the problem is to project q0 onto a linear subspace. The
constraints deﬁning the linear subspace are speciﬁed by some m × n matrix M and some
vector ˜p ∈ . 

At this point, we introduce a function LF and a set Q ⊆  which are intimately related
to the optimization problem in Eqs. (6) and (7). After giving formal deﬁnitions, we give in-
formal arguments—through the use of Lagrange multipliers—for the relationships between
P, Q and LF . Finally, we state Theorem 1, which gives a complete connection between
these concepts, and whose results will be used throughout this paper.
Let us deﬁne the function LF : int × Rm → int to be

In order for this to be mathematically sound, we assume that ∇ F is a bijective (one-to-one
and onto) mapping from int to Rm so that its inverse (∇ F )−1 is deﬁned. It is straightforward
to verify that LF has the following “additive” property:

We now return to the optimization problem in Eqs. (6) and (7), and describe informally
how it can be solved in some cases using the method of Lagrange multipliers. To use this
method, we start by forming the Lagrangian:

By the usual theory of Lagrange multi-
pliers, the solution to the original optimization problem is determined by the saddle point
of this Lagrangian, where the minimum should be taken with respect to the parameters p,
and the maximum should be taken with respect to the Lagrange multipliers λ.

Differentiating K (p, λ) with respect to λ and setting the result equal to zero simply
implies that p must satisfy the constraints in Eq. (5), and hence that p ∈ P. So we have
shown that ﬁnding a saddle point of the Lagrangian—and thereby solving the constrained
optimization problem in Eqs. (6) and (7)—is equivalent to ﬁnding a point in P ∩ Q.

Finally, if we plug Eq. (13) into the Lagrangian in Eq. (11), we are left with the problem

of maximizing.

In other words, because BF (˜p(cid:17) q0) is constant (relative to λ), the original optimization
problem has been reduced to the “dual” problem of minimizing BF (˜p(cid:17) q) over q ∈ Q.
To summarize, we have argued informally that if there is a point q(cid:11) in P ∩ Q then this
point minimizes BF (˜p(cid:17) q0) over p ∈ P and also minimizes BF (˜p(cid:17) q0) over q ∈ Q. It
turns out, however, that P ∩ Q can sometimes be empty, in which case this method does
not yield a solution. Nevertheless, if we instead use the closure of Q, which, intuitively,
has the effect of allowing some or all of the Lagrange multipliers to be inﬁnite, then there
will always exist a unique solution. That is, as stated in the next theorem, for a large family
of Bregman distances, P ∩ ¯Q always contains exactly one point, and that one point is the

unique solution of both optimization problems (where we also extend the constraint set of
the dual problem from Q to ¯Q).
We take Theorem 1 from Lafferty, Della Pietra, and Della Pietra (1997). We do not give
the full details of the conditions that F must satisfy for this theorem to hold since these go
beyond the scope of the present paper. Instead, we refer the reader to Della Pietra, Della
Pietra, and Lafferty (2001) for a precise formulation of these conditions and a complete
proof. A proof for the case of (normalized) relative entropy is given by Della Pietra, Della
Pietra, and Lafferty (1997). Moreover, their proof requires very minor modiﬁcations for all
of the cases considered in the present paper. Closely related results are given by Censor and
Lent (1981) and Csisz´ar (1991, 1995). See also Censor and Zenios’s book (1997).
Theorem 1. Let ˜p, q0, M, , F, BF ,P and Q be as above. Assume BF (˜p(cid:17) q0) < ∞.
Then for a large family of functions F, including all functions considered in this paper,
there exists a unique q(cid:11) ∈  satisfying:
Moreover, any one of these four properties determines q(cid:11) uniquely.

Proof sketch: As noted above, a complete and general proof is given by Della Pietra,
Della Pietra, and Lafferty (2001). However, the proof given by Della Pietra, Della Pietra,
and Lafferty (1997) for normalized relative entropy can be modiﬁed very easily for all of
the cases of interest in the present paper. The only step that needs slight modiﬁcation is in
showing that the minimum in part 3 exists. For this, we note in each case that the set

is bounded. Therefore, we can restrict the minimum in part 3 to the intersection of ¯Q with
the closure of this set. Since this smaller set is compact and since BF (˜p(cid:17)·) is continuous,
the minimum must be attained at some point q.

The rest of the proof is essentially identical (modulo superﬁcial changes in notation).

This theorem will be extremely useful in proving the convergence of the algorithms
described below. We will show in the next section how boosting and logistic regression can
be viewed as optimization problems of the type given in part 3 of the theorem. Then, to
prove optimality, we only need to show that our algorithms converge to a point in P ∩ ¯Q.
Part 2 of Theorem 1 is a kind of Pythagorean theorem that is often very useful (for

instance, in the proof of the theorem), though not used directly in this paper.

We return now to the boosting and logistic regression problems outlined in Section 2, and
show how these can be cast in the form of the optimization problems outlined above.

To view this problem in the form given in Section 3, we let ˜p = 0, q0 = 1 (the all 0’s and all
λ j yi h j (xi ).
We let the space  = Rm+. Finally, we take F to be as in Eq. (4) so that BF is the unnormalized
relative entropy.
Logistic regression can be reduced to an optimization problem of this form in nearly the

same way. 

For shorthand, we call this the LogLoss problem. We deﬁne ˜p and M exactly as for expo-
nential loss. The vector q0 is still constant, but now is deﬁned to be (1/2)1, and the space
 is now restricted to be [0, 1]m. These are minor differences, however. 
. Thus, DB (0(cid:17)LF (q0, Mλ)) is equal to Eq. (17) so minimizing
DB (0(cid:17) q) over q ∈ ¯Q is equivalent to minimizing Eq. (17). As before, this is the same as
ﬁnding q ∈ ¯Q satisfying the constraints in Eq. (16).
Thus, the exponential loss and logistic loss problems ﬁt into our general framework using
nearly identical settings of the parameters. The main difference is in the choice of Bregman
distance—unnormalized relative entropy for exponential loss and binary relative entropy
for logistic loss. The former measures distance between nonnegative vectors representing
weights over the instances, while the latter measures distance between distributions on
possible labels, summed over all of the instances.

In this section, we describe a new algorithm for the ExpLoss and LogLoss problems using
an iterative method in which all weights λ j are updated on each iteration. The algorithm
is shown in ﬁgure 1. The algorithm can be used with any function F satisfying certain
conditions described below. In particular, we will see that it can be used with the choices
of F given in Section 4. Thus, this is really a single algorithm that can be used for both
loss-minimization problems by setting the parameters appropriately. Note that, without loss
of generality, we assume in this section that for all instances i.
The algorithm is very simple. On each iteration, the vector δt is computed as shown and
added to the parameter vector λt . We assume for all our algorithms that the inputs are such
that inﬁnite-valued updates never occur.

This algorithm is new for both minimization problems. Optimization methods for Exploss,
notably AdaBoost, have generally involved updates of one feature at a time. Parallel-update

methods for LogLoss are well known (see, for example, Darroch & Ratcliff, 1972; Della
Pietra, Della Pietra, & Lafferty, 1997)). However, our updates take a different form from
the usual updates derived for logistic models. We discuss the differences in Section 9.

We will prove next that the algorithm given in Fig. 1 converges to optimality for either
loss. We prove this abstractly for any matrix M and vector q0, and for any function F
satisfying Theorem 1 and the following conditions:

We will show later that the choices of F given in Section 4 satisfy these conditions which

will allow us to prove convergence for ExpLoss and LogLoss.

To prove convergence, we use the auxiliary-function technique of Della Pietra, Della
Pietra, and Lafferty (1997). Very roughly, the idea of the proof is to derive a nonnegative
lower bound called an auxiliary function on how much the loss decreases on each iteration.
Since the loss never increases and is lower bounded by zero, the auxiliary function must
converge to zero. The ﬁnal step is to show that when the auxiliary function is zero, the
constraints deﬁning the set P must be satisﬁed, and therefore, by Theorem 1, we must have
converged to optimality.

Before proving convergence of speciﬁc algorithms, we prove the following lemma which
shows, roughly, that if a sequence has an auxiliary function, then the sequence converges to
the optimum point q(cid:11). Thus, proving convergence of a speciﬁc algorithm reduces to simply
ﬁnding an auxiliary function.

Note that the qt ’s will lie in a compact subspace of Q if Condition 2 holds and
BF (0(cid:17) q1) <∞. In the algorithm in ﬁgure 1, and in general in the algorithms in this
paper, λ1 = 0, so that q1 = q0 and the condition BF (0(cid:17) q0) <∞ implies BF (0(cid:17) q1) <∞.
BF (0(cid:17) q0) <∞ is an input condition for all of the algorithms in this paper.
Proof: By condition (22), BF (0(cid:17) qt ) is a nonincreasing sequence. As is the case for all
Bregman distances, BF (0(cid:17) qt ) is also bounded below by zero. 

Using the condition of Eq. (22), this means that A(qt ) must also
converge to zero. Because we assume that the qt ’s lie in a compact space, the sequence of
qt ’s must have a subsequence converging to some point ˆq ∈ . By continuity of A, we have
A(ˆq) = 0. Therefore, ˆq ∈ P from the condition given by Eq. (23), where P is as in Eq. (7).
On the other hand, ˆq is the limit of a sequence of points in Q so ˆq ∈ ¯Q. Thus, ˆq ∈ P ∩ ¯Q
so ˆq = q(cid:11) by Theorem 1.

This argument and the uniqueness of q(cid:11) show that the qt ’s have only a single limit point
q(cid:11). Suppose that the entire sequence did not converge to q(cid:11). Then we could ﬁnd an open set
B containing q(cid:11) such that {q1, q2, . . .} − B contains inﬁnitely many points and therefore
has a limit point which must be in the closed set  − B and so must be different from q(cid:11).
This, we have already argued, is impossible. Therefore, the entire sequence converges to q(cid:11).

We can now apply this lemma to prove the convergence of the algorithm of ﬁgure 1.
Theorem 3. Let F satisfy Theorem 1 and Conditions 1 and 2, and assume that
BF (0(cid:17) q0) <∞. Let the sequences λ1, λ2, . . . and q1, q2, . . . be generated by the algorithm
of ﬁgure 1.

To apply this theorem to the ExpLoss and LogLoss problems, we only need to verify that

Conditions 1 and 2 are satisﬁed. 

The ﬁrst and second equalities use Eqs. (18) and (19), respectively. The ﬁnal inequality uses
1 + x ≤ ex for all x.
Condition 2 holds trivially for LogLoss since  = [0, 1]m is bounded. 

Note that while Condition 1 holds for the loss functions we are considering, it may not hold
for all Bregman distances. Lafferty, Della Pietra, and Della Pietra (1997) describe parallel
update algorithms for Bregman distances, using the auxiliary function technique. Their
method does not require Condition 1, and therefore applies to arbitrary Bregman distances;
however, each iteration of the algorithm requires solution of a system of equations that
requires a numerical search technique such as Newton’s method.

In this section, we describe another algorithm for the minimization problems described in
Section 4. However, unlike the algorithm of Section 5, the one that we present now only
updates the weight of one feature at a time. While the parallel-update algorithm may give
faster convergence when there are not too many features, the sequential-update algorithm
can be used when there are a very large number of features using an oracle for selecting
which feature to update next. For instance, AdaBoost, which is essentially equivalent to
the sequential-update algorithm for ExpLoss, uses an assumed weak learning algorithm to
select a weak hypothesis, i.e., one of the features. The sequential algorithm that we present
for LogLoss can be used in exactly the same way.

The algorithm is shown in ﬁgure 2. On each round, a single feature jt is ﬁrst chosen to
maximize the inner product of the corresponding column of the matrix M with the vector
qt . The quantity αt is then computed and added to the jt ’th component of λ.

It may seem surprising or even paradoxical that the algorithm does not explicitly guarantee
that all of the components of λ are eventually updated, and yet we are able to prove
convergence to optimality. Apparently, all components which “need” to be nonzero will
eventually be selected by the algorithm for updating. Moreover, on each iteration, although
only one component is actually updated, in fact, all of the components are considered for
updating which means that all of them are implicitly used in the computation of the eventual
update to λ.
Theorem 4. Given the assumptions of Theorem 3, the algorithm of ﬁgure 2 converges to
optimality in the sense of Theorem 3.

As mentioned above, this algorithm is essentially equivalent to AdaBoost, speciﬁcally, the
version of AdaBoost ﬁrst presented by Freund and Schapire (1997). In AdaBoost, on each
iteration, a distribution Dt over the training examples is computed and the weak learner
seeks a weak hypothesis with low error with respect to this distribution. The algorithm
presented in this section assumes that the space of weak hypotheses consists of the features
h1, . . . , hn, and that the weak learner always succeeds in selecting the feature with lowest
error (or, more accurately, with error farthest from 1/2). 

Theorem 4 then is the ﬁrst proof that AdaBoost always converges to the minimum of the
exponential loss (assuming an idealized weak learner of the form above). Note that when
q(cid:11) (cid:12)= 0, this theorem also tells us the exact form of lim Dt . However, we do not know what
the limiting behavior of Dt is when q(cid:11) = 0, nor do we know about the limiting behavior of
the parameters λt (whether or not q(cid:11) = 0).
We have also presented in this section a new algorithm for logistic regression. In fact,
this algorithm is the same as one given by Duffy and Helmbold (1999) except for the choice
of αt . In practical terms, very little work would be required to alter an existing learning
system based on AdaBoost so that it uses logistic loss rather than exponential loss—the
only difference is in the manner in which qt is computed from λt . Thus, we could easily
convert any system such as SLIPPER (Cohen & Singer, 1999), BoosTexter (Schapire &
Singer, 2000) or alternating trees (Freund & Mason, 1999) to use logistic loss. We can
even do this for systems based on “conﬁdence-rated” boosting (Schapire & Singer, 1999)
in which αt and jt are chosen together on each round to minimize Eq. (30) rather than an
approximation of this expression as used in the algorithm of ﬁgure 2. (Note that the proof
of Theorem 4 can easily be modiﬁed to prove the convergence of such an algorithm using
the same auxiliary function.)

In previous sections, we described separate parallel-update and sequential-update algo-
rithms. In this section, we describe a parameterized family of algorithms that includes the
parallel-update algorithm of Section 5 as well as a sequential-update algorithm that is differ-
ent from the one in Section 6. Thus, in this section, we show how the parallel and sequential
viewpoints can themselves be uniﬁed in a manner that admits a uniﬁed presentation and
uniﬁed convergence proofs. Moreover, the family of algorithms that we present includes a
number of new algorithms including, as just mentioned, a sequential-update algorithm that,
in our experiments, consistently performed better than the one in Section 6. This family of

algorithms also includes other algorithms that may in certain situations be more appropriate
than any of the algorithms presented up to this point. For instance, one of these algorithms
is tailored for the case when the Euclidean norm of each row of the matrix M is bounded
by a constant, in other words, for when the feature-vectors associated with the examples
are known to lie in a Euclidean ball (centered at the origin) of bounded radius.

On each round, the quantities W
t, j are computed as before, and the
vector dt is computed as δt was computed in ﬁgure 1. Now, however, this vector dt is not
added directly to λt . Instead, another vector at is selected which provides a “scaling” of the
features. This vector is chosen to maximize a measure of progress while restricted to belong
to the set AM. The allowed form of these scaling vectors is given by the set A, a parameter
of the algorithm. 

The parallel-update algorithm of ﬁgure 1 is obtained by choosing A = {1} and assuming
|Mi j| ≤ 1 for all i. (Equivalently, we can make no such assumption, and choose
that
A = {c1| c > 0}.) An alternative is to not restrict the scaling vectors at all, i.e., we set A
to be Rn+. In this case, ﬁnding at is a linear programming problem with n variables and
m constraints, and the features are dynamically scaled to make optimal progress on each
iteration. There may be computational reasons for doing this, in that the rate of convergence
may depend on the relative scaling of the features.
We can obtain a sequential-update algorithm by choosing A to be the set of unit vectors
(i.e., with one component equal to 1 and all others equal to 0), and assuming that Mi j ∈
[−1,+1] for all i, j. 

In this section, we show how all of our results can be extended to the multiclass case.
Because of the generality of the preceding results, we will see that no new algorithms need
be devised and no new convergence proofs need be proved for this case. Rather, all of the
preceding algorithms and proofs can be directly applied to the multiclass case.
In the multiclass case, the label set Y has cardinality k. 

This distance measures the relative entropy between the distributions over labels for instance
i deﬁned by p and q, summed over all instances i. 

Now let M(i,(cid:17)), j = h j (xi , yi )− h j (xi , (cid:17)), and let q0 = (1/k)1. Plugging in these deﬁnitions
gives that BF (0(cid:17)LF (q0, Mλ)) is equal to Eq. (35). Thus, the algorithms of Sections 5–7
can all be used to solve this minimization problem, and the corresponding convergence
proofs are also directly applicable.

Choosing M as we did for multiclass logistic regression and q0 = 1, we have that
BF (0(cid:17)LF (q0, Mλ)) is equal to the loss in Eq. (37). We can thus use the preceding algo-
rithms to solve this multiclass problem as well. In particular, the sequential-update algorithm
gives AdaBoost.M2.

In this section, we describe the generalized iterative scaling (GIS) procedure of Darroch and
Ratcliff (1972) for comparison to our algorithms. We largely follow the description of GIS
given by Berger, Della Pietra, and Della Pietra (1996) for the multiclass case. To make the
comparison as stark as possible, we present GIS in our notation and prove its convergence
using the methods developed in previous sections. In doing so, we are also able to relax one
of the key assumptions traditionally used in studying GIS.

We adopt the notation and set-up used for multiclass logistic regression in Section 8. (To
our knowledge, there is no analog of GIS for the exponential loss so we only consider the
= 1 − ¯qi so that qi,(cid:17) is
case of logistic loss.) We also extend this notation by deﬁning qi,yi
now deﬁned for all (cid:17) ∈ Y. Moreover, it can be veriﬁed that qi,(cid:17) = ˆPr[(cid:17)| xi ] as deﬁned in
Eq. (34) if q = LF (q0, Mλ).
In GIS, the following assumptions regarding the features are usually made:

Since, in the multiclass case, a constant can be added to all features h j without changing
the model or loss function, and since the features can be scaled by any constant, the two
assumptions we consider clearly can be made to hold without loss of generality. The im-
proved iterative scaling algorithm of Della Pietra, Della Pietra, and Lafferty (1997) also
requires only these milder assumptions but is more complicated to implement, requiring a
numerical search (such as Newton-Raphson) for each feature on each iteration.

GIS works much like the parallel-update algorithm of Section 5 with F, M and q0
as deﬁned for multiclass logistic regression in Section 8. The only difference is in the
computation of the vector of updates δt , for which GIS requires direct access to the features
h j . 

Equation (44) follows from the log bound ln x ≤ x − 1. Equation (46) uses Eq. (29) and
our assumption on the form of the h j ’s. Equation (47) follows from our deﬁnition of the
update δ.

Finally, combining Eqs. (40), (42), (43) and (48) gives Eq. (41) completing the proof.

It is clear that the differences between GIS and the updates given in this paper stem from
Eq. (42), which is derived from ln x = −C + ln(eC x), with C = i (yi ) on the i’th term in
the sum. This choice of C effectively means that the log bound is taken at a different point
(ln x = −C + ln(eC x) ≤ −C + eC x − 1). In this more general case, the bound is exact at
x = e
−C; hence, varying C varies where the bound is taken, and thereby varies the updates.

In this section we discuss various notions of convergence of AdaBoost, relating the work
in this paper to previous work on boosting, and in particular to previous work on the
convergence properties of AdaBoost.

The algorithms in this paper deﬁne a sequence of parameter settings λ1, λ2, . . . . There
are various functions of the parameter settings, for which sequences are therefore also
deﬁned and for which convergence properties may be of interest. For instance, one can
investigate convergence in value, i.e., convergence of the exponential loss function, as de-
ﬁned in Eq. (14); convergence of either the unnormalized distributions qt or the normalized
i qt
distributions qt /(
), over the training examples; and convergence in parameters, that
i
is, convergence of λt .

In this paper, we have shown that AdaBoost, and the other algorithms proposed, converge
to the inﬁmum of the exponential loss function. We have also shown that the unnormalized
distribution converges to the distribution q(cid:11) as deﬁned in Theorem 1. The normalized
distribution converges, provided that q(cid:11) (cid:12)= 0. In the case q(cid:11) = 0 the limit of qt /(
) is
clearly not well deﬁned.
that q(cid:11) (cid:12)= 0. 

Here Pm is the simplex over the m training examples (i.e., the space of possible normalized
distributions); DR (q(cid:17) q0) is the relative entropy between distributions q and q0; and q0 is
the uniform distribution over the training examples, q0 = (1/m)1. This paper has discussed
the properties of the unnormalized distribution: it is interesting that Kivinen and Warmuth’s
results imply analogous relations for the normalized distribution.

We should note that we have implicitly assumed in the algorithms that the weak learner
can make use of an unnormalized distribution, rather than the normalized distribution over
training examples that is usually used by boosting algorithms. We think this is a minor point
though: indeed, there is nothing to prevent the normalized distribution being given to the
weak learner instead (the algorithms would not change, and the normalized distribution is
qi = 0, in which case the algorithm has already converged). In our
view, the use of the unnormalized rather than the normalized distribution is a minor change,
although the use of the normalized distribution is perhaps more intuitive (for instance, the
“edge” of a weak learner is deﬁned with respect to the normalized distribution).
Finally, the convergence of the parameter values λt is problematic. In the case that q(cid:11) = 0,
some of the parameter values must diverge to +∞ or −∞. In fact, the parameter values
can diverge even if q(cid:11) (cid:12)= 0: all that is needed is that one or more of the components of q(cid:11)
be equal to zero. Even if q(cid:11) is on the interior of , there is no guarantee of convergence
of the parameter values, for if the constraints are not linearly independent, there may be
several parameter values which give the optimal point. Thus, the parameters may diverge
under our assumptions, or even under the assumption that q(cid:11) (cid:12)= 0. This is problematic, as
the values for λ are used to deﬁne the ﬁnal hypothesis that is applied to test data examples.

In this section, we brieﬂy describe some experiments using synthetic data. We stress that
these experiments are preliminary and are only intended to suggest the possibility of these
algorithms’ having practical value. More systematic experiments are clearly needed using
both real-world and synthetic data, and comparing the new algorithms to other commonly
used procedures.

In our experiments, we generated random data and classiﬁed it using a very noisy hy-
perplane. More speciﬁcally, in the 2-class case, we ﬁrst generated a random hyperplane in
100-dimensional space represented by a vector w ∈ R100 (chosen uniformly at random from
the unit sphere). We then chose 1000 points x ∈ R100. In the case of real-valued features,
each point was normally distributed x ∼ N (0, I). In the case of Boolean features, each
point x was chosen uniformly at random from the Boolean hypercube {−1,+1}100. We
next assigned a label y to each point depending on whether it fell above or below the chosen
hyperplane, i.e., y = sign(w · x). After each label was chosen, we perturbed each point x.
In the case of real-valued features, we did this by adding a random amount ε to x where
ε ∼ N (0, 0.8 I). For Boolean features, we ﬂipped each coordinate of x independently with
probability 0.05. Note that both of these forms of perturbation have the effect of causing
the labels of points near the separating hyperplane to be more noisy than points that are
farther from it. The features were identiﬁed with coordinates of x.

For real-valued features, we also conducted a similar experiment involving ten classes
rather than two. In this case, we generated ten random hyperplanes w1, . . . , w10, each chosen
uniformly at random from the unit sphere, and classiﬁed each point x by arg maxy wy · x
(prior to perturbing x).

Finally, in some of the experiments, we limited each weight vector to depend on just 4

of the 100 possible features.

In the ﬁrst set of experiments, we tested the algorithms to see how effective they are at
minimizing the logistic loss on the training data. (We did not run corresponding experiments
for exponential loss since typically we are not interested in minimizing exponential loss per
se, but rather in using it as a proxy for some other quantity that we do want to minimize, such
as the classiﬁcation error rate.) We ran the parallel-update algorithm of Section 5 (denoted
“par” in the ﬁgures), as well as the sequential-update algorithm that is a special case of the
parameterized family described in Section 7 (denoted “seq”). Finally, we ran the iterative
scaling algorithm described in Section 9 (“i.s.”). (We did not run the sequential-update
algorithm of Section 6 since, in preliminary experiments, it seemed to consistently perform
worse than the sequential-update algorithm of Section 7).

As noted in Section 9, GIS requires that all features be nonnegative. Given features that
do not satisfy this constraint, one can subtract a constant c j from each feature h j without
changing the model.

Like the former approach, this causes h j to be nonnegative without affecting the model of
(both denominator and numerator of Eq.

In a preliminary version of this paper,3 we did experiments using only the former approach
and found that GIS performed uniformly and considerably worse than any of the other
algorithms tested. After the publication of that version, we tried the latter method of making
the features nonnegative and obtained much better performance. All of the experiments in
the current paper, therefore, use this latter approach.

The results of the ﬁrst set of experiments are shown in ﬁgure 4. Each plot of this ﬁgure
shows the logistic loss on the training set for each of the three methods as a function of
the number of iterations. (The loss has been normalized to be 1 when λ = 0.) Each plot
corresponds to a different variation on generating the data, as described above. When there
are only a small number of relevant features, the sequential-update algorithms seems to
have a clear advantage, but when there are many relevant features, none of the methods
seems to be best across-the-board. Of course, all methods eventually converge to the same
level of loss.

In the second experiment, we tested how effective the new competitors of AdaBoost
are at minimizing the test misclassiﬁcation error. 

Figure 5. The test misclassiﬁcation error on data generated by noisy hyperplanes.

Many thanks to Manfred Warmuth for ﬁrst teaching us about Bregman distances and for
many comments on an earlier draft. John Lafferty was also extraordinarily helpful, both
in the feedback that he gave us on our results, and in helping us with Theorem 1. Thanks
also to Michael Cameron-Jones, Sanjoy Dasgupta, Nigel Duffy, David Helmbold, Raj Iyer
and the anonymous reviewers of this paper for helpful discussions and suggestions. Some
of this research was done while Yoram Singer was at AT&T Labs.

1. More speciﬁcally, Bregman (1967) and later Censor and Lent (1981) describe optimization methods based
on Bregman distances where one constraint is satisﬁed at each iteration, for example, a method where the
constraint which makes the most impact on the objective function is greedily chosen at each iteration. The
simplest version of AdaBoost, which assumes weak hypotheses with values in {−1,+1}, is an algorithm of
this type if we assume that the weak learner is always able to choose the weak hypothesis with minimum
weighted error.

2. Speciﬁcally, their assumption is equivalent to the inﬁmum of the exponential loss being strictly positive (when

the data is separable it can be shown that the inﬁmum is zero).

3. Appeared in Proceedings of the Thirteenth Annual Conference on Computational Learning Theory, 2000.

