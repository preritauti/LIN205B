Prepositional Phrase Attachment through a Backed-Oﬀ Model

Recent work has considered corpus-based or statistical approaches to the problem of prepositional
phrase attachment ambiguity. Typically, ambiguous verb phrases of the form v np1 p np2 are
resolved through a model which considers values of the four head words (v, n1, p and n2). This
paper shows that the problem is analogous to n-gram language models in speech recognition,
and that one of the most common methods for language modeling, the backed-oﬀ estimate, is
applicable. Results on Wall Street Journal data of 84.5% accuracy are obtained using this method.
A surprising result is the importance of low-count events - ignoring events which occur less than 5
times in training data reduces performance to 81.6%.

Prepositional phrase attachment is a common cause of structural ambiguity in natural language.
For example take the following sentence:

Pierre Vinken, 61 years old, joined the board as a nonexecutive director.

The PP ‘as a nonexecutive director’ can either attach to the NP ‘the board’ or to the VP ‘joined’,
giving two alternative structures. (In this case the VP attachment is correct):

Work by Ratnaparkhi, Reynar and Roukos [RRR94] and Brill and Resnik [BR94] has considered
corpus-based approaches to this problem, using a set of examples to train a model which is then
used to make attachment decisions on test data. Both papers describe methods which look at the
four head words involved in the attachment - the VP head, the ﬁrst NP head, the preposition and
the second NP head (in this case joined, board, as and director respectively).

This paper proposes a new statistical method for PP-attachment disambiguation based on the four
head words.

The training and test data were supplied by IBM, being identical to that used in [RRR94]. Examples
of verb phrases containing a (v np pp) sequence had been taken from the Wall Street Journal
Treebank [MSM93]. For each such VP the head verb, ﬁrst head noun, preposition and second head
noun were extracted, along with the attachment decision (1 for noun attachment, 0 for verb). For
example the verb phrase:

The elements of this quintuple will from here on be referred to as the random variables A, V , N 1,
P , and N 2. In the above verb phrase A = 0, V = joined, N 1 = board, P = as, and N 2 = director.

The data consisted of training and test ﬁles of 20801 and 3097 quintuples respectively. In addition,
a development set of 4039 quintuples was also supplied. This set was used during development of
the attachment algorithm, ensuring that there was no implicit training of the method on the test
set itself.

A PP-attachment algorithm must take each quadruple (V = v, N 1 = n1, P = p, N 2 = n2) in test
data and decide whether the attachment variable A = 0 or 1. The accuracy of the algorithm is
then the percentage of attachments it gets ‘correct’ on test data, using the A values taken from the
treebank as the reference set.

The probability of the attachment variable A being 1 or 0 (signifying noun or verb attachment
respectively) is a probability, p, which is conditional on the values of the words in the quadruple.
In general a probabilistic algorithm will make an estimate, ˆp, of this probability:

For brevity this estimate will be referred to from here on as:

If this is true the attachment is made to the noun, if not then it is made to the verb.

When evaluating an algorithm it is useful to have an idea of the lower and upper bounds on its
performance. Some key results are summarised in the table below. All results in this section are
on the IBM training and test data, with the exception of the two ‘average human’ results.

‘Always noun attachment’ means attach to the noun regardless of (v,n1,p,n2). ‘Most likely for each
preposition’ means use the attachment seen most often in training data for the preposition seen in
the test quadruple. The human performance results are taken from [RRR94], and are the average
performance of 3 treebanking experts on a set of 300 randomly selected test events from the WSJ
corpus, ﬁrst looking at the four head words alone, then using the whole sentence.

A reasonable lower bound seems to be 72.2% as scored by the ‘Most likely for each preposition’
method. An approximate upper bound is 88.2% - it seems unreasonable to expect an algorithm to
perform much better than a human.

We will use the symbol f to denote the number of times a particular tuple is seen in train-
ing data. For example f (1, is, revenue, f rom, research) is the number of times the quadruple
(is, revenue, f rom, research) is seen with a noun attachment. Counts of lower order tuples can also
be made - for example f (1, P = f rom) is the number of times (P = f rom) is seen with noun attach-
ment in training data, f (V = is, N 2 = research) is the number of times (V = is, N 2 = research)
is seen with either attachment and any value of N1 and P.

A maximum likelihood method would use the training data to give the following estimation for the
conditional probability:

Unfortunately sparse data problems make this estimate useless. A quadruple may appear in test
data which has never been seen in training data.
ie. f (v, n1, p, n2) = 0. The above estimate is
undeﬁned in this situation, which happens extremely frequently in a large vocabulary domain such
as WSJ. (In this experiment about 95% of those quadruples appearing in test data had not been
seen in training data).

Even if f (v, n1, p, n2) > 0, it may still be very low, and this may make the above MLE estimate in-
accurate. Unsmoothed MLE estimates based on low counts are notoriously bad in similar problems
such as n-gram language modeling [GC90]. However later in this paper it is shown that estimates
based on low counts are surprisingly useful in the PP-attachment problem.

Hindle and Rooth [HR93] describe one of the ﬁrst statistical approaches to the prepositional phrase
attachment problem. Over 200,000 (v, n1, p) triples were extracted from 13 million words of AP
news stories. The attachment decisions for these triples were unknown, so an unsupervised training
method was used (section 5.2 describes the algorithm in more detail). Two human judges annotated
the attachment decision for 880 test examples, and the method performed at 80% accuracy on these
cases. Note that it is diﬃcult to compare this result to results on Wall Street Journal, as the two
corpora may be quite diﬀerent.

The Wall Street Journal Treebank [MSM93] enabled both [RRR94] and [BR94] to extract a large
amount of supervised training material for the problem. Both of these methods consider the second
noun, n2, as well as v, n1 and p, with the hope that this additional information will improve results.

[BR94] use 12,000 training and 500 test examples. A greedy search is used to learn a sequence
of ‘transformations’ which minimise the error rate on training data. A transformation is a rule
which makes an attachment decision depending on up to 3 elements of the (v, n1, p, n2) quadruple.
(Typical examples would be ‘If P=of then choose noun attachment’ or ‘If V=buy and P=for choose
verb attachment’). A further experiment incorporated word-class information from WordNet into
the model, by allowing the transformations to look at classes as well as the words. (An example
would be ‘If N2 is in the time semantic class, choose verb attachment’). The method gave 80.8%
accuracy with words only, 81.8% with words and semantic classes, and they also report an accuracy
of 75.8% for the metric of [HR93] on this data. Transformations (using words only) score 81.9%1
on the IBM data used in this paper.


[RRR94] use the data described in section 2.1 of this paper - 20801 training and 3097 test examples
from Wall Street Journal. They use a maximum entropy model which also considers subsets of the
quadruple. Each sub-tuple predicts noun or verb attachment with a weight indicating its strength
of prediction - the weights are trained to maximise the likelihood of training data. For example
(P = of ) might have a strong weight for noun attachment, while (V = buy, P = f or) would have
a strong weight for verb attachment.
[RRR94] also allow the model to look at class information,
this time the classes were learned automatically from a corpus. Results of 77.7% (words only) and
81.6% (words and classes) are reported. Crucially they ignore low-count events in training data by
imposing a frequency cut-oﬀ somewhere between 3 and 5.

[KATZ87] describes backed-oﬀ n-gram word models for speech recognition. There the task is to
estimate the probability of the next word in a text given the (n-1) preceding words. The MLE
estimate of this probability would be:

But again the denominator f (w1, w2....wn−1) will frequently be zero, especially for large n. The
backed-oﬀ estimate is a method of combating the sparse data problem. It is deﬁned recursively as
follows:

Else backing-oﬀ continues in the same way.

The idea here is to use MLE estimates based on lower order n-grams if counts are not high enough
to make an accurate estimate at the current level. The cut oﬀ frequencies (c1, c2....) are thresholds

determining whether to back-oﬀ or not at each level - counts lower than ci at stage i are deemed
to be too low to give an accurate estimate, so in this case backing-oﬀ continues. (α1, α2,....) are
normalisation constants which ensure that conditional probabilities sum to one.

Note that the estimation of ˆp(wn|w1, w2....wn−1) is analogous to the estimation of ˆp(1|v, n1, p, n2),
and the above method can therefore also be applied to the PP-attachment problem. For example a
simple method for estimation of ˆp(1|v, n1, p, n2) would go from MLE estimates of ˆp(1|v, n1, p, n2) to
ˆp(1|v, n1, p) to ˆp(1|v, n1) to ˆp(1|v) to ˆp(1).

A key observation in choosing between these tuples is that the preposition is particularly important
to the attachment decision. For this reason only tuples which contained the preposition were used
in backed oﬀ estimates - this reduces the problem to a choice between 3 triples and 3 pairs at
each respective stage. Section 6.2 describes experiments which show that tuples containing the
preposition are much better indicators of attachment.

The following method of combining the counts was found to work best in practice:

Note that this method eﬀectively gives more weight to tuples with high overall counts. Another
obvious method of combination, a simple average2, gives equal weight to the three tuples regardless
of their total counts and does not perform as well.

The cut-oﬀ frequencies must then be chosen. A surprising diﬀerence from language modeling is
that a cut-oﬀ frequency of 0 is found to be optimum at all stages. This eﬀectively means however
low a count is, still use it rather than backing oﬀ a level.

The ﬁgure below shows the results for the method on the 3097 test sentences, also giving the total
count and accuracy at each of the backed-oﬀ stages.

In an eﬀort to reduce sparse data problems the following processing was run over both test and
training data:

• All 4-digit numbers were replaced with the string ‘YEAR’.

• All other strings of numbers (including those which had commas or decimal points) were

replaced with the token ‘NUM’.

• The verb and preposition ﬁelds were converted entirely to lower case.

• In the n1 and n2 ﬁelds all words starting with a capital letter followed by one or more lower

case letters were replaced with ‘NAME’.

• All strings ‘NAME-NAME’ were then replaced by ‘NAME’.

• All verbs were reduced to their morphological stem using the morphological analyser described

in [KSZE94].

These modiﬁcations are similar to those performed on the corpus used by [BR94].

The result using this modiﬁed corpus was 84.5%, an improvement of 0.4% on the previous result.

Results from [RRR94], [BR94] and the backed-oﬀ method are shown in the table below4. All results
are for the IBM data. These ﬁgures should be taken in the context of the lower and upper bounds
of 72.2%-88.2% proposed in section 2.3.

4Results for [BR94] with words and classes were not available on the IBM data

On the surface the method described in [HR93] looks very similar to the backed-oﬀ estimate. For
this reason the two methods deserve closer comparison. Hindle and Rooth used a partial parser
to extract head nouns from a corpus, together with a preceding verb and a following preposition,
giving a table of (v, n1, p) triples. An iterative, unsupervised method was then used to decide
between noun and verb attachment for each triple. The decision was made as follows5:

Here f (w, p) is the number of times preposition p is seen attached to word w in the table, and
f (w) = Pp f (w, p).

If we ignore n2 then the IBM data is equivalent to Hindle and Rooth’s (v, n1, p) triples, with the
advantage of the attachment decision being known, allowing a supervised algorithm. The test used
in [HR93] can then be stated as follows in our notation:

This is eﬀectively a comparison of the maximum likelihood estimates of ˆp(p|1, n1) and ˆp(p|0, v), a
diﬀerent measure from the backed-oﬀ estimate which gives ˆp(1|v, p, n1).

The backed-oﬀ method based on just the f (v, p) and f (n1, p) counts would be:

5This ignores reﬁnements to the test such as smoothing of the estimate, and a measure of the conﬁdence of the

decision. However the measure given is at the core of the algorithm.

An experiment was implemented to investigate the diﬀerence in performance between these two
methods. The test set was restricted to those cases where f (1, n1) > 0, f (0, v) > 0, and Hindle
and Rooth’s method gave a deﬁnite decision.
the above inequality is strictly less-than or
greater-than). This gave 1924 test cases. Hindle and Rooth’s method scored 82.1% accuracy (1580
correct) on this set, whereas the backed-oﬀ measure scored 86.5% (1665 correct).

A possible criticism of the backed-oﬀ estimate is that it uses low count events without any smooth-
ing, which has been shown to be a mistake in similar problems such as n-gram language models.
In particular, quadruples and triples seen in test data will frequently be seen only once or twice in
training data.

An experiment was made with all counts less than 5 being put to zero,6 eﬀectively making the
algorithm ignore low count events. In [RRR94] a cut-oﬀ ‘between 3 and 5’ is used for all events.
The training and test data were both the unprocessed, original data sets.

The decrease in accuracy from 84.1% to 81.6% is clear evidence for the importance of low counts.

We have excluded tuples which do not contain a preposition from the model. This section gives
results which justify this.

The table below gives accuracies for the sub-tuples at each stage of backing-oﬀ. The accuracy ﬁgure
for a particular tuple is obtained by modifying the algorithm in section 4.1 to use only information
from that tuple at the appropriate stage. For example for (v, n1, n2), stage 2 would be modiﬁed to
read

All other stages in the algorithm would be unchanged. The accuracy ﬁgure is then the percentage
accuracy on the test cases where the (v, n1, n2) counts were used. The development set with no
morphological processing was used for these tests.

At each stage there is a sharp diﬀerence in accuracy between tuples with and without a preposition.
Moreover, if the 14 tuples in the above table were ranked by accuracy, the top 7 tuples would be
the 7 tuples which contain a preposition.

The backed-oﬀ estimate scores appreciably better than other methods which have been tested on
the Wall Street Journal corpus. The accuracy of 84.5% is close to the human performance ﬁgure of
88% using the 4 head words alone. A particularly surprising result is the signiﬁcance of low count
events in training data. The algorithm has the additional advantages of being conceptually simple,
and computationally inexpensive to implement.

There are a few possible improvements which may raise performance further. Firstly, while we
have shown the importance of low-count events, some kind of smoothing may improve performance
further - this needs to be investigated. Word-classes of semantically similar words may be used to
help the sparse data problem - both [RRR94] and [BR94] report signiﬁcant improvements through
the use of word-classes. Finally, more training data is almost certain to improve results.

