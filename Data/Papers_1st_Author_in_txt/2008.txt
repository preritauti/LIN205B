Log-linear and maximum-margin models are two commonly-used methods in supervised machine
learning, and are frequently used in structured prediction problems. Efﬁcient learning of parameters
in these models is therefore an important problem, and becomes a key factor when learning from
very large data sets. This paper describes exponentiated gradient (EG) algorithms for training
such models, where EG updates are applied to the convex dual of either the log-linear or max-
margin objective function; the dual in both the log-linear and max-margin cases corresponds to
minimizing a convex function with simplex constraints. We study both batch and online variants of
the algorithm, and provide rates of convergence for both cases. In the max-margin case, O( 1
e ) EG
updates are required to reach a given accuracy e
in the dual; in contrast, for log-linear models only
O(log( 1
e )) updates are required. For both the max-margin and log-linear cases, our bounds suggest
that the online EG algorithm requires a factor of n less computation to reach a desired accuracy than
the batch EG algorithm, where n is the number of training examples. Our experiments conﬁrm that
the online algorithms are much faster than the batch algorithms in practice. We describe how the EG
updates factor in a convenient way for structured prediction problems, allowing the algorithms to be
efﬁciently applied to problems such as sequence learning or natural language parsing. We perform
extensive evaluation of the algorithms, comparing them to L-BFGS and stochastic gradient descent
for log-linear models, and to SVM-Struct for max-margin models. The algorithms are applied to a
multi-class problem as well as to a more complex large-scale parsing task. In all these settings, the
EG algorithms presented here outperform the other methods.

Structured prediction problems involve learning to map inputs x to labels y, where the labels have
rich internal structure, and where the set of possible labels for a given input is typically exponential
in size. Examples of structured prediction problems include sequence labeling and natural language
parsing. Several models that implement learning in this scenario have been proposed over the last
few years, including log-linear models such as conditional random ﬁelds (CRFs, Lafferty et al.,
2001), and maximum-margin models such as maximum-margin Markov networks (Taskar et al.,
2004a).

For both log-linear and max-margin models, learning is framed as minimization of a regularized
loss function which is convex. In spite of the convexity of the objective function, ﬁnding the optimal
parameters for these models can be computationally intensive, especially for very large data sets.
This problem is exacerbated in structured prediction problems, where the large size of the set of
possible labels adds an additional layer of complexity. The development of efﬁcient optimization
algorithms for learning in structured prediction problems is therefore an important problem.

In this paper we describe learning algorithms that exploit the structure of the dual optimization
problems for log-linear and max-margin models. For both log-linear and max-margin models the
dual problem corresponds to the minimization of a convex function Q subject to simplex constraints.

Thus each ui is constrained to form a distribution over the set of possible labels. The max-margin
and log-linear problems differ only in their deﬁnition of Q.

The algorithms in this paper make use of exponentiated gradient (EG) updates (Kivinen and
Warmuth, 1997) in solving the problem in Eq. 1, in particular for the cases of log-linear or max-
margin models. We focus on two classes of algorithms, which we call batch and online. In the
batch case, the entire set of ui variables is updated simultaneously at each iteration of the algorithm;
in the online case, a single ui variable is updated at each step. The “online” case essentially cor-
responds to coordinate-descent on the dual function Q, and is similar to the SMO algorithm (Platt,
1998) for training SVMs. The online algorithm has the advantage of updating the parameters after
every sample point, rather than after making a full pass over the training examples; intuitively, this
should lead to considerably faster rates of convergence when compared to the batch algorithm, and
indeed our experimental and theoretical results support this intuition. A different class of online
algorithms consists of stochastic gradient descent (SGD) and its variants (e.g., see LeCun et al.,
1998; Vishwanathan et al., 2006). 

We describe theoretical results concerning the convergence of the EG algorithms, as well as

experiments. Our key results are as follows:

(cid:15) For the max-margin case, we show that O( 1

e ) time is required for both the online and batch
algorithms to converge to within e of the optimal value of Q(u). This is qualitatively sim-
ilar to recent results in the literature for max-margin approaches (e.g., see Shalev-Shwartz
et al., 2007). For log-linear models, we show convergence rates of O(log( 1
e )), a signiﬁcant
improvement over the max-margin case.

For both the max-margin and log-linear cases, our bounds suggest that the online algorithm
requires a factor of n less computation to reach a desired accuracy, where n is the number of
training examples. Our experiments conﬁrm that the online algorithms are much faster than
the batch algorithms in practice.

We describe how the EG algorithms can be efﬁciently applied to an important class of struc-
tured prediction problems where the set of labels Y is exponential in size. In this case the
number of dual variables is also exponential in size, making algorithms which deal directly
with the ui variables intractable. Following Bartlett et al. (2005), we focus on a formulation
where each label y is represented as a set of “parts”, for example corresponding to labeled
cliques in a max-margin network, or context-free rules in a parse tree. Under an assumption
that part-based marginals can be calculated efﬁciently—for example using junction tree algo-
rithms for CRFs, or the inside-outside algorithm for context-free parsing—the EG algorithms
can be implemented efﬁciently for both max-margin and log-linear models.

In our experiments we compare the online EG algorithm to various state-of-the-art algo-
rithms. For log-linear models, we compare to the L-BFGS algorithm (Byrd et al., 1995)
and to stochastic gradient descent. For max-margin models we compare to the SVM-Struct
algorithm of Tsochantaridis et al. (2004). The methods are applied to a standard multi-class
learning problem, as well as to a more complex natural language parsing problem. In both
settings we show that the EG algorithm converges to the optimum much faster than the other
algorithms.

In addition to proving convergence results for the deﬁnition of Q(u) used in max-margin and
log-linear models, we give theorems which may be useful when optimizing other deﬁnitions
of Q(u) using EG updates. In particular, we give conditions for convergence which depend
on bounds relating the Bregman divergence derived from Q(u) to the Kullback-Leibler diver-
gence. 

The rest of this paper is organized as follows. In Section 2, we introduce the log-linear and
max-margin learning problems, and describe their dual optimization problems. Section 3 describes
the batch and online EG algorithms; in Section 4, we describe how the algorithms can be efﬁciently
applied to structured prediction problems. Section 5 then gives convergence proofs for the batch and

online cases. Section 6 discusses related work. Sections 7 and 8 give experiments, and Section 9
discusses our results.

This work builds on previous work described by Bartlett et al. (2005) and Globerson et al.
(2007). Bartlett et al. (2005) described the application of the EG algorithm to max-margin param-
eter estimation, and showed how the method can be applied efﬁciently to part-based formulations.
Globerson et al. (2007) extended the approach to log-linear parameter estimation, and gave new
convergence proofs for both max-margin and log-linear estimation. The work in the current paper
gives several new results. We prove rates of convergence for a randomized version of the EG online
algorithm; previous work on EG algorithms had not given convergence rates for the online case.
We also report new experiments, including experiments with the randomized strategy. Finally, the
O(log( 1
e )) convergence rates for the log-linear case are new. The results in Globerson et al. (2007)
gave O( 1
e ) rates for the batch algorithm for log-linear models, and did not give any theoretical rates
of convergence for the online case.

In this section we present the log-linear and max-margin optimization problems for supervised
learning. For each problem, we describe the equivalent dual optimization problem, which will form
the core of our optimization approach.

Consider a supervised learning setting with objects x 2 X and labels y 2 Y .2 In the structured
learning setting, the labels may be sequences, trees, or other high-dimensional data with internal
structure. Assume we are given a function f(x; y) : X (cid:2) Y ! Rd that maps (x; y) pairs to feature
vectors. 

In this paper we will consider two deﬁnitions for ‘(w; xi; yi). The ﬁrst deﬁnition, originally

introduced by Taskar et al. (2004a), is a variant of the hinge loss, and is deﬁned as follows:

In general the set of labels for a given example x may be a set Y (x) that depends on x; in fact, in our experiments
on dependency parsing Y does depend on x. For simplicity, in this paper we use the ﬁxed notation Y for all x; it is
straightforward to extend our notation to the more general case.

Here e(xi; yi; y) is some non-negative measure of the error incurred in predicting y instead of yi
as the label of xi. We assume that e(xi; yi; yi) = 0 for all i, so that no loss is incurred for correct
prediction, and therefore ‘MM(w; xi; yi) is always non-negative. 

The second loss function that we will consider is based on log-linear models, and is commonly
used in conditional random ﬁelds (CRFs, Lafferty et al., 2001). 

The function L is convex in w for both deﬁnitions ‘MM and ‘LL. Furthermore, in both cases
minimization of L can be re-cast as optimization of a dual convex problem. The dual problems in
the two cases have a similar structure, as we describe in the next two sections.

This is a convex optimization problem, and has an equivalent convex dual which was derived by
Lebanon and Lafferty (2002). Denote the dual variables by ui;y where i = 1; : : : ; n and y 2 Y . We
also use u to denote the set of all variables, and ui the set of all variables corresponding to a given i.

The minimum of D-MM is equal to (cid:0)1 times the minimum of P-MM. (Note that for D-MM the
minimizer u(cid:3) may not be unique; in this case we take u(cid:3) to be any member of the set of minimizers
of QMM(u)). The optimal primal parameters are again related to the optimal dual parameters, through
Cw(cid:3) = w(u(cid:3)). Here again the constraints are that ui is a distribution over Y for all i.

It can be seen that the D-LL and D-MM problems have a similar structure, in that they both
involve minimization of a convex function Q(u) over the set D n. This will allow us to describe
algorithms for both problems using a common framework.

In this section we describe batch and online algorithms for minimizing a convex function Q(u)
subject to the constraints u 2 D n. The algorithms can be applied to both the D-LL and D-MM
optimization problems that were introduced in the previous section. The algorithms we describe
are based on exponentiated gradient (EG) updates, originally introduced by Kivinen and Warmuth
(1997) in the context of online learning algorithms.3


3. Kivinen and Warmuth (1997) study the online setting, as opposed to a ﬁxed data set which we study here. They are
thus not interested in minimizing a ﬁxed objective, but rather study regret type bounds. This leads to algorithms and
theoretical analyses that are different from the ones considered in the current work.

Pseudo-code for the two schemes is given in Figures 1 and 2. From here on we will refer to the
batch and online EG algorithms applied to the log-linear dual as LLEG-Batch, and LLEG-Online
respectively. Similarly, when applied to the max-margin dual, they will be referred to as MMEG-
Batch and MMEG-Online.

Note that another plausible online algorithm would be a “deterministic” algorithm that repeat-
edly cycles over the training examples in a ﬁxed order. The motivation for the alternative, random-
ized, algorithm is two-fold. First, we are able to prove bounds on the rate of convergence of the
randomized algorithm; we have not been able to prove similar bounds for the deterministic variant.
Second, our experiments show that the randomized variant converges signiﬁcantly faster than the
deterministic algorithm.

The EG online algorithm is essentially performing coordinate descent on the dual objective,
and is similar to SVM algorithms such as SMO (Platt, 1998). For binary classiﬁcation, the exact
minimum of the dual objective with respect to a given coordinate can be found in closed form,4
and more complicated algorithms such as the exponentiated-gradient method may be unnecessary.
However for multi-class or structured problems, the exact minimum with respect to a coordinate ui
(i.e., a set of jY j dual variables) cannot be found in closed form: this is a key motivation for the use
of EG algorithms in this paper.

In Section 5 we give convergence proofs for the batch and online algorithms. The techniques
used in the convergence proofs are quite general, and could potentially be useful in deriving EG
algorithms for convex functions Q other than QLL and QMM. Before giving convergence results for
the algorithms, we describe in the next section how the EG algorithms can be applied to structured
problems.

We now describe how the EG updates can be applied to structured prediction problems, for example
parameter estimation in CRFs or natural language parsing. In structured problems the label set Y
is typically very large, but labels can have useful internal structure. As one example, in CRFs each

4. This is true for the max-margin case. For log-linear models, minimization with respect to a single coordinate is a

little more involved.

We follow the framework for structured problems described by Bartlett et al. (2005). Each label
y is deﬁned to be a set of parts. We use R to refer to the set of all possible parts.5 We make the
assumption that the feature vector for an entire label y decomposes into a sum over feature vectors
for individual parts as follows.

Note that we have overloaded f to apply to either labels y or parts r.

As one example, consider a CRF which has an underlying graph with m nodes, and a maximum
clique size of 2. Assume that each node can be labeled with one of two labels, 0 or 1. In this case
the labeling of an entire graph is a vector y 2 f0;1gm. Each possible input x is usually a vector in
X m for some set X , although this does not have to be the case. Each part corresponds to a tuple
(u; v; yu; yv) where (u; v) is an edge in the graph, and yu; yv are the labels for the two vertices u and
v. The feature vector f(x; r) can then track any properties of the input x together with the labeled
clique r = (u; v; yu; yv). In CRFs with clique size greater than 2, each part corresponds to a labeled
clique in the graph. In natural language parsing, each part can correspond to a context-free rule at a
particular position in the sentence x (see Bartlett et al., 2005; Taskar et al., 2004b, for more details).
The label set Y can be extremely large in structured prediction problems. For example, in a
CRF with an underlying graph with m nodes and k possible labels at each node, there are km possible
labelings of the entire graph. The algorithms we have presented so far require direct manipulation
of dual variables ui;y corresponding to each possible labeling of each training example; they will
therefore be intractable in cases where there are an exponential number of possible labels. However,
in this section we describe an approach that does allow an efﬁcient implementation of the algorithms
in several cases. The approach is based on the method originally described in Bartlett et al. (2005).
The key idea is as follows. Instead of manipulating the dual variables ui for each i directly, we
will make use of alternative data structures si for all i. Each si is a vector of real values si;r for all
r 2 R. In general we will assume that there are a tractable (polynomial) number of possible parts,
and therefore that the number of si;r variables is also polynomial. For example, for a linear chain
CRF with m nodes and k labels at every node, each part takes the form r = (u; v; yu; yv), and there
are (m (cid:0) 1)k2 possible parts.

For example, when Y is a sequence of variables, the cost could be the Hamming distance between
the correct sequence yi and the predicted sequence y; it is straightforward to decompose the Ham-
ming distance as a sum over parts as shown above. For brevity, in what follows we use ei;r instead
of e(xi; yi; r).

Notice that, for both objective functions, the gradients can be expressed as a sum over parts. For
the QLL objective function, this follows from the fact that ui = p(si) and from the assumption that
the feature vector decomposes into parts. For the QMM objective, it follows from the latter, and the
assumption that the loss decomposes into parts. The following lemma describes how EG updates
on the u variables can be restated in terms of updates to the s variables, provided that the gradient
decomposes into parts in this way.

6. Note that in the max-margin case, the optimal u values may have zero probabilities which correspond to inﬁnite s
values. This does not pose a problem, since the algorithm will indeed converge to inﬁnite s values at the limit, but
st will not be inﬁnite for any ﬁnite t. For the log-linear case, the optimal u will never have zero values, as shown in
Globerson et al. (2007).

The main computational challenge in the new algorithms comes in computing the parameter
vector w(p(st)). The value for w(p(st)) can be expressed as a function of the marginal probabilities
of the part variables, as follows.


The mapping from parameters st
i) can be computed efﬁciently in several impor-
tant cases of structured models. For example, in CRFs belief propagation can be used to efﬁciently
calculate the marginal values, assuming that the tree-width of the underlying graph is small. In
weighted context-free grammars the inside-outside algorithm can be used to calculate marginals,
assuming that the set of parts R corresponds to context-free rule productions. Once marginals are
computed, it is straightforward to compute w(p(st)) and thereby implement the part-based EG al-
gorithms.


In this section, we provide convergence results for the EG batch and online algorithms presented in
Section 3. Section 5.1 provides the key results, and the following sections give the proofs and the
technical details.

Our convergence results give bounds on how quickly the error jQ(u)(cid:0)Q(u(cid:3))j decreases with respect
to the number of iterations, T , of the algorithms. In all cases we have jQ(u) (cid:0) Q(u(cid:3))j ! 0 as T ! ¥
.
In what follows we use D[pkq] to denote the KL divergence between p;q 2 D n (see Section 5.2).
We also use jAj¥
to denote the maximum magnitude element of A (i.e., jAj¥ = max(i;y);( j;z) jA(i;y);( j;z)j).
The ﬁrst theorem provides results for the EG-batch algorithms, and the second for the randomized
online algorithms.

The randomized online algorithm will produce different results at every run, since different
points will be processed on different runs. Our main result for this algorithm characterizes the mean
value of the objective Q(uT +1) when averaged over all possible random orderings of points. The
result implies that this mean will converge to the optimal value Q(u(cid:3)).

The above result characterizes the average behavior of the randomized algorithm, but does not
provide guarantees for any speciﬁc run of the algorithm. However, by applying the standard ap-
proach of repeated sampling (see, for example, Mitzenmacher and Upfal, 2005; Shalev-Shwartz
et al., 2007), one can obtain a solution that, with high probability, does not deviate by much from
the average behavior. In what follows, we brieﬂy outline this derivation.

Thus, for any desired conﬁdence 1 (cid:0) d , we can obtain a solution that is within a factor of 2 of the
bound for T iterations in Theorem 2 by using T log2( 1
d ) iterations. In our experiments, we found
that repeated trials of the randomized algorithm did not yield signiﬁcantly different results.

The ﬁrst consequence of the two theorems above is that the batch and randomized online algo-
rithms converge to a u with the optimal value Q(u(cid:3)). This follows since Equations 6 and 7 imply
that as T ! ¥

the value of Q(uT +1) approaches Q(u(cid:3)).

Crucially, note that these rates suggest that the online algorithms are signiﬁcantly more efﬁcient
than the batch algorithms; speciﬁcally, the bounds suggest that the online algorithms require a factor
of n less computation in both the QLL and QMM cases. Thus these results suggest that the randomized
online algorithm should converge much faster than the batch algorithm. Roughly speaking, this is
a direct consequence of the learning rate h being a factor of n larger in the online case (see also
Section 9). This prediction is conﬁrmed in our empirical evaluations, which show that the online
algorithm is far more efﬁcient than the batch algorithm.

To gain further intuition into the order of magnitude of iterations required, note that the factor
D[u(cid:3)ku1] which appears in the above expressions is at most nlog jY j, which can be achieved by
i to be the uniform distribution over Y for all i.
In the remainder of this section we give proofs of the results in Theorems 1 and 2. In doing so,

we also give theorems that apply to the optimization of general convex functions Q : D n ! R.

8. Note that if we run the batch algorithm for T iterations (as in the ﬁgure), nT training examples are processed.
In contrast, running the online algorithm for T iterations (again, as shown in the ﬁgure) only requires T training
examples to be processed. It is important to take this into account when comparing the rates in Theorems 1 and 2;
this is the motivation for measuring computation in terms of the number of examples that are processed.

In this section we provide a useful lemma that determines when the EG updates in the batch al-
gorithm will result in monotone improvement of Q(u). The lemma requires a condition on the
relation between the Bregman and KL divergences which we deﬁne as follows (the second part of
the deﬁnition will be used in the next section).

The previous section showed that for appropriate choices of the learning rate h
, the batch EG updates
are guaranteed to improve the QLL and QMM loss functions at each iteration. In this section we build
directly on these results, and address the following question: how many iterations does the batch
for a given e > 0? We show that as long as
EG algorithm require so that the jQ(ut) (cid:0) Q(u)j (cid:20) e
Q(u) is t -upper-bounded, the number of iterations required is O( 1
e ). This bound thus holds for both
the log-linear and max-margin batch algorithms. Next, we show that if Q(u) is (µ;t )-bounded, the
rate can be signiﬁcantly improved to requiring O(log( 1
e )) iterations. We conclude by showing that
QLL(u) is (µ;t )-bounded, implying that the O(log( 1

e )) rate holds for LLEG-Batch.

The idea of solving regularized loss-minimization problems via their convex duals has been ad-
dressed in several previous papers. Here we review those, speciﬁcally focusing on the log-linear
and max-margin problems.

Zhang (2002) presented a study of convex duals of general regularized loss functions, and pro-
vided a methodology for deriving such duals. He also considered a general procedure for solving
such duals by optimizing one coordinate at a time. However, it is not clear how to implement this
procedure in the structured learning case (i.e., when jY j is large), and convergence rates are not
given.

In the speciﬁc context of log-linear models, several papers have addressed dual optimization.
Earlier work (Jaakkola and Haussler, 1999; Keerthi et al., 2005; Zhu and Hastie, 2001) treated the
logistic regression model, a simpler version of a CRF. In the binary logistic regression case, there is
essentially one parameter ui per example with the constraint 0 (cid:20) ui (cid:20) 1, and therefore simple line-
search methods can be used for optimization. Minka (2003) presents empirical results which show
that this approach performs similarly to conjugate gradient. The problem becomes much harder
when ui is constrained to be a distribution over many labels, as in the case discussed here. Recently,
Memisevic (2006) addressed this setting, and suggests optimizing ui by transferring probability
mass between two labels y1; y2 while keeping the distribution normalized. This requires a strategy
for choosing these two labels, and the author suggests one which seems to perform well.

While some previous work on log-linear models proved convergence of dual methods (e.g.,
Keerthi et al., 2005), we are not aware of rates of convergence that have been reported in this
context. Convergence rates for related algorithms, in particular a generalization of EG, known
as the Mirror-Descent algorithm, have been studied in a more general context in the optimization
literature. For instance, Beck and Teboulle (2003) describe convergence results which apply to
quite general deﬁnitions of Q(u), but which have only O( 1
e 2 ) convergence rates, as compared to
our results of O( 1
e )) for the max-margin and log-linear cases respectively. Also, their
work considers optimization over a single simplex, and does not consider online-like algorithms
such as the one we have presented.

For max-margin models, numerous dual methods have been suggested, an earlier example being
the SMO algorithm of Platt (1998). Such methods optimize subsets of the u parameters in the dual
SVM formulation (see also Crammer and Singer, 2002). Analysis of a similar algorithm (Hush
et al., 2006) results in an O( 1
e ) rate, similar to the one we have here. Another algorithm for solving
SVMs via the dual is the multiplicative update method of Sha et al. (2007). These updates are
shown to converge to the optimum of the SVM dual, but convergence rate has not been analyzed,
and extension to the structured case seems non-trivial. An application of EG to binary SVMs was
previously studied by Cristianini et al. (1998). They show convergence rates of O( 1
e 2 ), that are
slower than our O( 1

e ), and no extension to structured learning (or multi-class) is discussed.

Recently, several new algorithms have been presented, along with a rate of convergence analysis
(Joachims, 2006; Shalev-Shwartz et al., 2007; Teo et al., 2007; Tsochantaridis et al., 2004; Taskar
et al., 2006). All of these algorithms are similar to ours in having a relatively low dependence on n
in terms of memory and computation. Among these, Shalev-Shwartz et al. (2007), Teo et al. (2007)
and Taskar et al. (2006) present an O( 1
e ) rate, but where accuracy is measured in the primal or via
the duality gap, and not in the dual as in our analysis. Thus, it seems that a rate of O( 1
e ) is currently
the best known result for algorithms that have a relatively low dependence on n (general QP solvers,
which may have O(log( 1
e )) behavior, generally have a larger dependence on n, both in time and
space). Note that, as in our analysis, all these convergence rates depend on jAj¥

.

Finally, we emphasize again that the EG algorithm is substantially different from stochastic
gradient and stochastic subgradient approaches (LeCun et al., 1998; Nedic and Bertsekas, 2001;
Shalev-Shwartz et al., 2007; Vishwanathan et al., 2006). EG and stochastic gradient methods are
similar in that they both process a single training example at a time. However, EG corresponds to
block-coordinate descent in the dual, and uses the exact gradient with respect to the block being
updated. In contrast, stochastic gradient methods directly optimize the primal problem, and at each
update use a single example to approximate the gradient (or subgradient) of the primal objective
function.

In this section we analyze the performance of the EG algorithms for optimization of regularized
log-likelihood. We describe experiments on two tasks: ﬁrst, the MNIST digit classiﬁcation task,
which is a multiclass classiﬁcation task; second, a log-linear model for a structured natural-language
dependency-parsing task. In each case we ﬁrst give results for the EG method, and then compare

We do not report results on LLEG-Batch, since we found it to converge much more slowly than
the online algorithm. This is expected from our theoretical results, which anticipate a factor of n
speed-up for the online algorithm. We also report experiments comparing the randomized online
algorithm to a deterministic online EG algorithm, where samples are drawn in a ﬁxed order (e.g.,
the algorithm ﬁrst visits the ﬁrst example, then the second, etc.).

Although EG is guaranteed to converge for an appropriately chosen h

, it turns out to be bene-
ﬁcial to use an adaptive learning rate. Here we use the following simple strategy: we ﬁrst consider
only 10% of the data-set, and ﬁnd a value of h
that results in monotone improvement for at least
95% of the samples. Denote this value by h
ini (for the experiments in Section 7.1 we simply use
ini = 0:5). For learning over the entire data-set, we keep a learning rate h
i for each sample i (where
i = 1; : : : ; n), and initialize this rate to h
i until
an improvement in the objective is obtained. Finally, after the update, we multiply h
i by 1:05, so
that it does not decrease monotonically.

It is important that when updating a single example using the online algorithms, the improve-
ment (or decrease) in the dual can be easily evaluated, allowing the halving strategy described in
the previous paragraph to be implemented efﬁciently. 


The primal parameters w(u) are maintained throughout the algorithm (see Figure 3), so that this
change in the dual objective can be calculated efﬁciently. A similar method can be used to calculate
the change in the dual objective in the max-margin case.

We measure the performance of each training algorithm (the EG algorithms, as well as the
batch gradient and stochastic gradient methods) as a function of the amount of computation spent.
Speciﬁcally, we measure computation in terms of the number of times each training example is
visited. For EG, an example is considered to be visited for every value of h
that is tested on it. For
L-BFGS, all examples are visited for every evaluation performed by the line-search routine. We
deﬁne the measure of effective iterations to be the number of examples visited, divided by n. In
the following sections we compare the algorithms in terms of their performance as a function of the
effective number of iterations. A comparison in terms of running time is provided in Appendix F;
there is little difference between the timed comparisons and the results presented in this section.

We ﬁrst conducted multi-class classiﬁcation experiments on the MNIST classiﬁcation task. Exam-
ples in this data set are images of handwritten digits represented as 784-dimensional vectors. We
used a training set of 59k examples, and a validation set of 10k examples.11 Note that since we

10. We also experimented with conjugate gradient algorithms, but since these resulted in worse performance than L-

BFGS, we do not report these results here.

11. In reporting results, we consider only validation error; that is, error computed during the training process on a
validation set. This measure is often used in early-stopping of algorithms, and is therefore of interest in the current
context. We do not report test error since our main focus is algorithmic.

Models were trained for various values of the regularization parameter C: speciﬁcally, we tried
values of C equal to 1000, 100, 10, 1, 0:1, and 0:01. Convergence of the EG algorithm for low
values of C (i.e., 0:1 and 0:01) was found to be slow; we discuss this issue more in Section 7.1.1,
arguing that it is not a serious problem.

Figure 4 shows plots of the validation error versus computation for C equal to 1000, 100, 10,
and 1, when using the EG algorithm. For C equal to 10 or more, convergence is fast. For C = 1
convergence is somewhat slower. Note that there is little to choose between C = 10 and C = 1 in
terms of validation error.

Figure 5 shows plots of the primal and dual objective functions for different values of C. To
obtain the primal objective values, we used the EG weight vector 1
C w(ut). Note that EG does not
explicitly minimize the primal objective function, so the EG primal will not necessarily decrease
at every iteration. Nevertheless, our experiments show that the EG primal decreases quite quickly.
Figure 6 shows how the duality gap decreases with the amount of computation spent (the duality gap
is the difference between the primal and dual values at each iteration). The log of the duality gap
decreases more-or-less linearly with the amount of computation spent, as predicted by the O(log( 1
e ))
bounds on the rate of convergence.12

Finally, we compare the deterministic and randomized versions of the EG algorithm. Figure 7
shows the primal and dual objectives for both algorithms. It can be seen that the randomized algo-
rithm is clearly much faster to converge. This is even more evident when plotting the duality gap,
which converges much faster to zero in the case of the randomized algorithm. These results give
empirical evidence that the randomized strategy is to be preferred over a ﬁxed ordering of the train-
ing examples (note that we have been able to prove bounds on convergence rate for the randomized
algorithm, but have not been able to prove similar bounds for the deterministic case).

As mentioned in the previous section, convergence of the EG algorithm for low values of C can be
very slow. This is to be expected from the bounds on convergence, which predict that convergence
time should scale linearly with 1
C (other algorithms, e.g., see Shalev-Shwartz et al., 2007, also
require O( 1
C ) time for convergence). This is however, not a serious problem on the MNIST data,
where validation error has reached a minimum point for around C = 10 or C = 1.

If convergence for small values of C is required, one strategy we have found effective is to start
C at a higher value, then “anneal” it towards the target value. For example, see Figure 8 for results
for C = 1 using one such annealing scheme. For this experiment, if we take t to be the number of
iterations over the training set, where for any t we have processed t (cid:2) n training examples, we set
C = 10 for t (cid:20) 5, and set C = 1 + 9 (cid:2) 0:7t(cid:0)5 for t > 5. Thus C starts at 10, then decays exponentially
quickly towards the target value of 1. It can be seen that convergence is signiﬁcantly faster for
the annealed method. The intuition behind this method is that the solution to the dual problem for

12. The rate results presented in this paper are for dual accuracy, but it is straightforward to obtain an O(log( 1e )) for the

duality gap in the log-linear case.

Figure 4: Validation error results on the MNIST learning task for log-linear models trained using
the EG randomized online algorithm. The X axis shows the number of effective iterations
over the entire data set. The Y axis shows validation error percentages. The left ﬁgure
shows plots for values of C equal to 1, 10, 100, and 1000. The right ﬁgure shows plots
for C equal to 1 and 10 at a larger scale.

C = 10 is a reasonable approximation to the solution for C = 1, and is considerably easier to solve;
in the annealing strategy we start with an easier problem and then gradually move towards the harder
problem of C = 1.

In practice, when estimating parameters using either regularized log-likelihood or hinge-loss, a
range of values for C are tested, with cross-validation or validation on a held-out set being used to
choose the optimal value of C. In the previously described experiments, we independently optimized
log-likelihood-based models for different values of C. In this section we describe a highly efﬁcient
method for training a sequence of models for a range of values of C.

The method is as follows. We pick some maximum value for C; as in our previous experiments,
we will choose a maximum value of C = 1000. We also pick a tolerance value e , and a parameter
0 < v < 1. We then optimize C using the randomized online algorithm, until the duality gap is
less than e (cid:2) p, where p is the primal value. Once the duality gap has converged to within this e
tolerance, we reduce C by a factor of v, and again optimize to within an e
tolerance. We continue
this strategy—for each value of C optimizing to within a factor of e , then reducing C by a factor
of v—until C has reached a low enough value. At the end of the sequence, this method recovers a
series of models for different values of C, each optimized to within a tolerance of e .

It is crucial that each time we decrease C, we take our initial dual values to be the ﬁnal dual
values resulting from optimization for the previous value of C. In practice, if C does not decrease
too quickly, the previous dual values are a very good starting point for the new value of C; this
corresponds to a “warm start” in optimizing values of C that are less than the maximum value. 

Figure 5: Primal and dual objective values on the MNIST learning task for log-linear models trained
using the EG randomized online algorithm. The dual values have been negated so that
the primal and dual problems have the same optimal value. The X axis shows the number
of effective iterations over the entire data set. The Y axis shows the value of the primal
or dual objective functions. The left ﬁgure shows plots for values of C equal to 1000 and
100; the right ﬁgure shows plots for C equal to 10, and 1. In all cases the primal and dual
objectives converge to the same value, with faster convergence for larger values of C.

similar initialization method is used in Koh et al. (2007) in the context of ‘1 regularized logistic
regression.

As one example of this approach, we trained models in this way with the starting (maximum)
value of C set to 1000, e set to 0.001 (i.e., 0.1%), and v set to 0.7. Table 2 shows the number of
iterations of training required for each value of C. The beneﬁts of using the previous dual values at
each new value of C are clear: for 13:84 (cid:20) C (cid:20) 700 at most 5 iterations are required for convergence;
even for C = 0:798 only 15:24 iterations are required; a range of 25 different values of C between
1000 and 0:274 can be optimized with 211.17 effective iterations over the training set.

This section compares performance of the EG algorithms to stochastic gradient descent (SGD) on
the primal objective. In SGD the parameters w are initially set to be 0. At each step an example
index i is chosen at random, and the following update is performed.

Table 2: Table showing number of effective iterations required to optimize a sequence of values for
C for the MNIST task, using the method described in Section 7.1.2. The column C shows
the sequence of decreasing regularizer constants. Iterations shows the number of effective
iterations over the training set required to optimized each value of C. Total iterations shows
the cumulative value of Iterations, and Error shows the validation error obtained for every
C value. It can be seen that the optimal error is reached at C = 1:62841.

Figure 6: Graph showing the duality gap on the MNIST learning task for log-linear models trained
using the EG randomized online algorithm. The X axis shows the number of effective
iterations over the entire data set. The Y axis (with a log scale) shows the value of the
duality gap, as a percentage of the ﬁnal optimal value.

Thus the learning rate decays to 0 with the number of
examples that are updated. This follows the approach described in LeCun et al. (1998); we have
consistently found that it performs better than using a single, ﬁxed learning rate.

We tested SGD for C values of 1000, 100, 10, 1, 0:1 and 0:01. In each case we chose the value
of h 0 as follows. For each value of C we ﬁrst tested values of h 0 equal to 1, 0:1, 0:01, 0:001, and
0:0001, and then chose the value of h 0 which led to the best validation error after a single iteration
of SGD. This strategy resulted in a choice of h 0 = 0:01 for all values of C except C = 1000, where
h 0 = 0:001 was chosen. We have found this strategy to be a robust method for choosing h 0 (note
that we do not want to run SGD for more than one iteration with all (C;h 0) combinations, since
each iteration is costly).

Figure 9 compares validation error rates for SGD and the randomized EG algorithm. For the
initial (roughly 5) iterations of training, SGD has better validation error scores, but beyond this the
EG algorithm is very competitive on this task. Note that the amount of computation for SGD does
not include the iterations required to ﬁnd the optimal value of h 0; if this computation was included
the SGD curves would be shifted 5 iterations to the right.

Figure 7: Results on the MNIST learning task, comparing the randomized and deterministic online
EG algorithms, for C = 1. The left ﬁgure shows primal and dual objective values for both
algorithms. The right ﬁgure shows the normalized value of the duality gap: (primal(t) (cid:0)
dual(t))=opt, where opt is the value of the joint optimum of the primal and dual problems,
and t is the iteration number. The X axis counts the number of effective iterations over
the entire data set.

Figure 8: Results on the MNIST learning task, for C = 1, comparing the regular EG randomized
algorithm with an annealed version of the algorithm (see Section 7.1.1). The left ﬁgure
shows primal objective values calculated for C = 1; the right ﬁgure shows validation error.
The annealed strategy gives signiﬁcantly faster convergence.

Figure 9: Graphs showing validation error results on the MNIST learning task, comparing the EG
randomized algorithm to stochastic gradient descent (SGD). The X axis shows number of
effective training iterations, the Y axis shows validation error in percent. The EG results
are shown for C = 10; SGD results are shown for several values of C. For SGD for C = 1,
C = 0:1, and C = 0:01 the curves were nearly identical, hence we omit the curves for
C = 1 and C = 0:1. Note that the amount of computation for SGD does not include the
iterations required to ﬁnd the optimal value for the learning rate h 0.

strategy for choosing h 0 does not pick the optimal value for h 0 at least when evaluating the primal
objective; see the caption to the ﬁgure for more discussion. EG again appears to out-perform SGD
after the initial few iterations.

One of the standard approaches to training log-linear models is using the L-BFGS gradient-based
algorithm (Sha and Pereira, 2003). L-BFGS is a batch algorithm, in the sense that its updates require
evaluating the primal objective and gradient, which involves iterating over the entire data-set. To
compare L-BFGS to EG, we used the implementation based on Byrd et al. (1995).13

For L-BFGS, a total of n training examples must be processed every time the gradient or objec-
tive function is evaluated; note that because L-BFGS uses a line search, each iteration may involve
several such evaluations.14

13. Speciﬁcally, we used the code by Zhu, Byrd, Lu, and Nocedal (www.ece.northwestern.edu/(cid:24)nocedal/) with L. Stew-
art’s wrapper (www.cs.toronto.edu/(cid:24)liam/). In all the experiments, we used 10 pairs of saved gradient vectors (see
also Sha and Pereira, 2003).

14. The implementation of L-BFGS that we use requires both the gradient and objective when performing the line-search.
In some line-search variants, it is possible to use only objective evaluations. In this case, the EG line search will be
somewhat more costly, since the dual objective requires evaluations of both marginals and partition function, whereas
the primal objective only requires the partition function. This will have an effect on running times only if the EG line
search evaluates more than one point, which happened for less than 10%.

Figure 10: Graphs showing primal objective values on the MNIST learning task, comparing the EG
randomized algorithm to stochastic gradient descent (SGD). The X axis shows number
of effective training iterations, the Y axis shows primal objective. The graphs are for
C equal to 1000, 100, 10, and 1. For C = 1 we show EG results with and without the
annealed strategy described in Section 7.1.1. For C = 1 we also show two SGD curves,
for learning rates 0:01 and 0:1: in this case h 0 = 0:01 was the best-performing learning
rate after one iteration for both validation error and primal objective, however a post-hoc
analysis shows that h 0 = 0:1 converges to a better value in the limit. Thus our strategy
for choosing h 0 was not optimal in this case, although it is difﬁcult to know how h 0 = 0:1
could be chosen without post-hoc analysis of the convergence for the different values of
h 0. For other values of C our strategy for picking h 0 was more robust.

Figure 11: Results on the MNIST learning task, comparing the EG algorithm to L-BFGS. The
ﬁgures on the ﬁrst and second row show the primal objective for both algorithms, for
various values of C. The bottom curve shows validation error for L-BFGS for various
values of C and for EG with C = 10.

As in Section 7.1.3, we calculated primal values for EG. Figure 11 shows the primal objective
for EG, and L-BFGS. It can be seen that the primal value for EG converges considerably faster than
the L-BFGS one. Also shown is a curve of validation error for both algorithms. Here we show the
results for EG with C = 10 and L-BFGS with various C values. It can be seen that L-BFGS does
not outperform the EG curve for any value of C.

Parsing of natural language sentences is a challenging structured learning task. Dependency parsing
(McDonald et al., 2005) is a simpliﬁed form of parsing where the goal is to map sentences x into
projective directed spanning trees over the set of words in x. Each label y is a set of directed arcs
(dependencies) between pairs of words in the sentence. Each dependency is a pair (h; m) where h is
the index of the head word of the dependency, and m is the index of the modiﬁer word. Assuming
we have a function f(x; h; m) that assigns a feature vector to dependencies (h; m), we can use a
weight vector w to score a given tree y by w (cid:1) (cid:229)
(h;m)2y f(x; h; m). Dependency parsing corresponds to
a structured problem where the parts r are dependencies (h; m); the approach described in Section 4
can be applied efﬁciently to dependency structures. For projective dependency trees (e.g., see Koo
et al., 2007), the required marginals can be computed efﬁciently using a variant of the inside-outside
algorithm (Baker, 1979).

In the experiments below we use a feature set f(x; h; m) similar to that in McDonald et al. (2005)
and Koo et al. (2007), resulting in 2;500;554 features. We report results on the Spanish data-
set which is part of the CoNLL-X Shared Task on multilingual dependency parsing (Buchholz
and Marsi, 2006). The training data consists of 2;306 sentences (58;771 tokens). To evaluate
validation error, we use 1;000 sentences (30;563 tokens) and report accuracy (rate of correct edges
in a predicted parse tree) on these sentences.15 Since we used only sentences from the training set,
results are not directly comparable to the CoNLL-X shared task results. However, our previous
work on this data set (Koo et al., 2007) shows that regularized max-margin and log-linear models
typically outperform the averaged perceptron, which is not explicitly regularized.

As in the multi-class experiments, we compare to SGD and L-BFGS. The implementation of
the algorithms is similar to that described in Section 7.1. The gradients for SGD and L-BFGS were
obtained by calculating the relevant marginals of the model, using the inside-outside algorithm that
was also used for EG. The learning rate for SGD was chosen as in the previous section; that is, we
tested several learning rates (h 0 = 1;0:1;0:001;0:0001) and chose the one that yielded the minimum
validation error after one iteration.

Figure 12 shows results for EG and L-BFGS on the parsing task. We experiment with values
of C in the set f0:1;1;10;100;1000g. Of these, the value that results in optimal validation error
was C = 10. The performance of L-BFGS, SGD and EG is demonstrated in terms of the primal
objective for a subset of the C values. L-BFGS and EG both converge to the optimal value, and EG
is signiﬁcantly faster. On the other hand, SGD does not converge to the optimum for all C values
(e.g., for C = 1;10), and when it does converge to the optimum, it is slower than EG.

Figure 12 also shows the validation error for EG at the optimal C value, compared to validation
error for L-BFGS and SGD at various C values. Again, it can be seen that EG signiﬁcantly outper-
forms L-BFGS. 

Table 3: Table showing number of effective iterations required to optimize a sequence of values for
C for the parsing task, using the method described in Section 7.1.2. The column C shows
the sequence of decreasing regularizer constants. Iterations shows the number of effective
iterations over the training set required to optimize each value of C. Total iterations shows
the cumulative value of Iterations, and Accuracy shows the validation accuracy obtained
for every C value. It can be seen that the optimal accuracy is reached at C = 13:841.

in fact does not successfully optimize the primal objective for low values of C, and for higher values
of C the SGD primal objective is slower to converge.

As in the multi-class experiments (see Figure 10), it is possible to ﬁnd learning rates for SGD
such that it converges to the primal optimum for C = 1;10. However, the optimality of these rates
only becomes evident after 10 iterations or more (results not shown). Thus, to ﬁnd a learning rate
for SGD that actually solves the optimization problem would typically require an additional few
tens of iterations, making it signiﬁcantly slower than EG.

Finally, it is possible to use EG to efﬁciently optimize over a set of regularization constants, as

in Section 7.1.2. Table 3 shows results for a sequence of regularization constants.

Figure 12: Results on the dependency-parsing task, comparing the EG algorithm to L-BFGS and
SGD. All algorithms are trained on the log-linear objective function. The ﬁgures on
the ﬁrst and second rows show the primal objective for the three algorithms, for various
values of C. The left bottom plot shows validation accuracy (measured as the fraction of
correctly predicted edges) for L-BFGS for various values of C and for EG with C = 10.
The right bottom plot show validation accuracy for EG (with C = 10) and SGD.

The max-margin loss (Eq. 3) has a discontinuity in its derivative. This makes optimization of max-
margin models somewhat more involved than log-linear ones, since gradient algorithms such as
L-BFGS cannot be used. This difﬁculty is exacerbated in the case of structured prediction models,
since maximization in Eq. 3 is potentially over an exponentially large set.

In this section, we apply the EG algorithm to the max-margin problem, and compare its per-
formance to the SVM-Struct algorithm presented in Tsochantaridis et al. (2004).16 SVM-Struct is
based on a cutting-plane algorithm that operates on the dual max-margin problem (D-MM) and re-
sults in monotone improvement in this dual. In this sense, it is similar to our EG algorithm. In order
to facilitate a fair comparison, we report the performance of the two algorithms as a function of
time. We do not report results by iteration since EG and SVM-struct involve different computation
per iteration (e.g., SVM-Struct solves a QP per iteration).

We applied SVM-Struct and EG to the dependency parsing problem described in Section 7.2. To
apply SVM-Struct to this problem, we supply it with a routine that ﬁnds the y 2 Y which attains the
maximum of the hinge-loss in Eq. 3. This maximum can be found using a Viterbi-style algorithm.
For the value of C we experimented with C 2 f1;10;100;1000;10000g. The optimal value in terms
of validation error was C = 100.

Figure 13 shows results in terms of primal and dual objective and in terms of accuracy. It can
be seen that EG is considerably faster than SVM-Struct for most C values. The performance is
comparable only for C = 1, where convergence is slow for both algorithms.

We have presented novel algorithms for large-scale learning of log-linear and max-margin models,
which provably converge to the optimal value of the respective loss functions. Although the algo-
rithms have both batch and online variants, the online version turns out to be much more effective,
both in theory and in practice. Our theoretical results (see Section 5.1) suggest that the online algo-
rithm requires a factor of n less iterations to achieve a desired accuracy e
in the dual objective. This
factor results from the fact that the online algorithm can use a learning rate h
that is n times larger
than the batch case to obtain updates that decrease the dual objective. Intuitively, this difference
is associated with the fact that the batch algorithm updates all u values simultaneously. The dual
objective has a term uT Au which involves all the ui variables and second order interactions between
them. It turns out that for batch updates only a relatively small change in the ui is allowed, if one
still requires an improvement in the dual objective after the update. It is possible that our bounds
for the batch convergence rate are more conservative than those for the online case. However, we
have observed in practice that the batch algorithm is much slower to converge. Furthermore, we
also observed that other batch-based algorithms such as L-BFGS and conjugate gradient converge
more slowly than the online EG algorithm.

Figure 13: Results on the dependency-parsing task, comparing the EG algorithm to SVM-Struct.
Both algorithms are trained on a max-margin model. The ﬁgures on the ﬁrst and second
rows show the primal objective for both algorithms, for various values of C. The bottom
curve shows validation accuracy (measured as the fraction of correctly predicted edges)
for SVM-Struct for various values of C and for EG with C = 100 (the value that yielded
the highest validation accuracy). The X axis on all curves is running time in hours.

Our convergence rates are with respect to accuracy in the dual objective. Some previous work
(e.g., Shalev-Shwartz et al., 2007) has considered the accuracy with respect to the primal objective.
It is relatively easy to show that in order to obtain e accuracy in the primal, the EG algorithms require
O(log( 1
e 2 ) for the max-margin case. It is possible that
a more reﬁned analysis of the max-margin case will result in O( 1
e ) (e.g., see List et al., 2007), but
we leave this for further study.

e )) updates for the log-linear problem and O( 1

Most of our proofs rely on a relation between BQ and the KL divergence. This relation holds
for max-margin learning as well, a fact that simpliﬁes previous results in this setting (Bartlett et al.,
2005). We expect a similar analysis to hold for other functions Q.

An interesting extension of our method is to using second order derivative information, or its
approximations, as in L-BFGS (Byrd et al., 1995). Such information may be used to obtain more
accurate minimization for each ui and may speed up convergence. Another possible improvement is
to the line search method. In the experiments reported here we use a crude mechanism for adapting
the learning rate, and it is possible that a more careful procedure will improve convergence rates in
practice.

Parallelization is becoming increasingly relevant as multi-core CPUs become available. For the
batch EG algorithm, it is straightforward to distribute the computation among k processors. One
method for distributing the online EG algorithm would be to update k examples in parallel on k
different processors. It should be possible to analyze this setting in a similar way to our proofs for
the online case, but we leave this to future work.

Finally, our results show that the EG algorithms are highly competitive with state-of-the-art
methods for training log-linear and max-margin models. We thus expect them to become useful as
learning algorithms, particularly in the structured prediction setting.

The authors gratefully acknowledge the following sources of support. Amir Globerson was sup-
ported by a fellowship from the Rothschild Foundation - Yad Hanadiv. Terry Koo was funded by
a grant from the NSF (DMS-0434222) and a grant from NTT, Agmt. Dtd. 6/21/1998. Xavier Car-
reras was supported by the Catalan Ministry of Innovation, Universities and Enterprise, and by a
grant from NTT, Agmt. Dtd. 6/21/1998. Michael Collins was funded by NSF grants 0347631 and
DMS-0434222. Peter Bartlett was funded by a grant from the NSF (DMS-0434383).

In this section we compare EG to SGD and L-BFGS in terms of running time. The experiments in
the main text provide comparison in terms of “effective” iterations, which do not take into account
the computational cost of processing a single example. Here we show that EG maintains its advan-
tages over the other learning algorithms when running time is used as a performance measure, with
similar relative improvements to those reported in the main text.

Clearly, any timed comparison depends on the quality of the implementations being compared.
Data processing and gradient and objective calculations were performed using the same C++ code
for all three algorithms: EG, SGD, and L-BFGS. For L-BFGS, we used the implementation based
on Byrd et al. (1995).18 This code is available online and is written in Fortran. The SGD update is
straightforward and we implemented it ourselves in our C++ package. All the timing experiments
were performed on a 1.8GHz AMD OpteronTM CPU.

We focus on the log-linear case here, since timing results for the max-margin case were provided

in Section 8.

Figures 14 and 15 show results for the MNIST multi-class (see Section 7.1), and the parsing
tasks (see Section 7.2) respectively. As in the results in the main text, it can be seen that the EG
objective converges faster than the two other algorithms. Also, as in the main text, SGD converges
quickly in terms of accuracy, but its objective converges very slowly to the optimum.

Note that the timing of the EG experiments includes the time required to convert the dual pa-
rameters to the primal representation. We have found that the EG algorithm is quite fast in practice;
in the MNIST task, for example, the EG algorithm requires on average only 10% more time per
iteration (including the step-size search) than SGD and L-BFGS. To help explain why EG is able to
run almost as fast as SGD, Figure 16 presents pseudocode for the SGD and online EG algorithms.
Both SGD and EG share the following operations: (a) inner products between the feature vectors
and the primal vector, (b) computation of part-wise marginals, and (c) addition of scaled feature
vectors to the primal vector. In the EG algorithm, we require two additional loops over R(xi) in
order to update the dual variables and compute the dual entropy term. In practice, however, the cost
of the two additional loops is dominated by the three shared operations mentioned above. Thus,
processing a single example takes roughly the same time for EG and SGD. Similar arguments can
be used to explain why EG can run almost as fast as L-BFGS.

Figure 15: Timing results on the dependency-parsing task, comparing the EG algorithm to L-BFGS
and SGD. All algorithms are trained on the log-linear objective function with C = 10.
The left ﬁgure shows objective values and the right ﬁgure shows accuracy (see Fig-
ure 12). The results roughly correspond to 100 effective iterations.

