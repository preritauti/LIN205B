sentence,yngve_score,frazier_score,dependency_distance_score
This article considers approaches which rerank the output of an existing probabilistic parser.,2,1.0,2
"The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses.",2,0.9423076923076923,2
"A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence.",2,0.9285714285714286,2
"The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account.",3,1.0204081632653061,3
"We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al.",3,0.8958333333333334,2
(1998).,1,0,1
We apply the boosting method to parsing the Wall Street Journal treebank.,2,0.6923076923076923,2
"The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model.",3,0.8055555555555556,3
"The new model achieved 89.75% F-measure, a 13% relative decrease in Fmeasure error over the baseline models score of 88.2%.",3,0.76,2
The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data.,2,0.9230769230769231,2
Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach.,2,0.6944444444444444,2
We argue that the method is an appealing alternativein terms of both simplicity and efficiencyto work on feature selection methods within log-linear (maximum-entropy) models.,2,0.7777777777777778,4
"Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation.",3,0.7553191489361702,3
Machine-learning approaches to natural language parsing have recently shown some success in complex domains such as news wire text.,2,0.725,2
"Many of these methods fall into the general category of history-based models, in which a parse tree is represented as a derivation (sequence of decisions) and the probability of the tree is then calculated as a product of decision probabilities.",3,0.875,3
"While these approaches have many advantages, it can be awkward to encode some constraints within this framework.",2,0.9210526315789473,2
"In the ideal case, the designer of a statistical parser would be able to easily add features to the model  that are believed to be useful in discriminating among candidate trees for a sentence.",2,1.0694444444444444,2
"In practice, however, adding new features to a generative or history-based model can be awkward.",3,0.6944444444444444,6
"The derivation in the model must be altered to take the new features into account, and this can be an intricate task.",4,0.7291666666666666,2
This article considers approaches which rerank the output of an existing probabilistic parser.,2,1.0,2
"The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses.",2,0.9423076923076923,2
"A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence.",2,0.9285714285714286,2
"The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation which takes these features into account.",3,1.0666666666666667,3
"We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al.",3,0.8958333333333334,2
(1998).,1,0,1
"The algorithm can be viewed as a feature selection method, optimizing a particular loss function (the exponential loss function) that has been studied in the boosting literature.",3,0.7258064516129032,3
"We applied the boosting method to parsing the Wall Street Journal (WSJ) treebank (Marcus, Santorini, and Marcinkiewicz 1993).",3,0.8,3
"The method combines the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model.",4,0.7777777777777778,3
The baseline model achieved 88.2% F-measure on this task.,2,0.6818181818181818,3
"The new model achieves 89.75% Fmeasure, a 13% relative decrease in F-measure error.",2,0.6764705882352942,2
"Although the experiments in this article are on natural language parsing, the approach should be applicable to many other natural language processing (NLP) problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation.",4,0.7040816326530612,4
"See Collins (2002a) for an application of the boosting approach to named entity recognition, and Walker, Rambow, and Rogati (2001) for the application of boosting techniques for ranking in the context of natural language generation.",3,0.9767441860465116,4
"The article also introduces a new, more efficient algorithm for the boosting approach which takes advantage of the sparse nature of the feature space in the parsing data.",2,0.8333333333333334,2
Other NLP tasks are likely to have similar characteristics in terms of sparsity.,2,1.0714285714285714,1
"Experiments show an efficiency gain of a factor of 2,600 for the new algorithm over the obvious implementation of the boosting approach.",2,0.8043478260869565,2
"Efficiency issues are important, because the parsing task is a fairly large problem, involving around one million parse trees and over 500,000 features.",3,0.7884615384615384,3
"The improved algorithm can perform 100,000 rounds of feature selection on our task in a few hours with current processing speeds.",2,0.7045454545454546,3
"The 100,000 rounds of feature selection require computation equivalent to around 40 passes over the entire training set (as opposed to 100,000 passes for the naive implementation).",2,0.9833333333333333,4
The problems with history-based models and the desire to be able to specify features as arbitrary predicates of the entire tree have been noted before.,2,1.0192307692307692,2
"In particular, previous work (Ratnaparkhi, Roukos, and Ward 1994; Abney 1997; Della Pietra, Della Pietra, and Lafferty 1997; Johnson et al.",4,0,2
1999; Riezler et al.,1,0,1
2002) has investigated the use of Markov random fields (MRFs) or log-linear models as probabilistic models with global features for parsing and other NLP tasks.,3,0.7413793103448276,3
(Log-linear models are often referred to as maximum-entropy models in the NLP literature.),3,0.8125,3
"Similar methods have also been proposed for machine translation (Och and Ney 2002) and language understanding in dialogue systems (Papineni, Roukos, and Ward 1997, 1998).",3,0.7424242424242424,3
"Previous work (Friedman, Hastie, and Tibshirani 1998) has drawn connections between log-linear models and  boosting for classification problems.",3,0.8043478260869565,2
One contribution of our research is to draw similar connections between the two approaches to ranking problems.,2,0.8888888888888888,2
"We argue that the efficient boosting algorithm introduced in this article is an attractive alternative to maximum-entropy models, in particular, feature selection methods that have been proposed in the literature on maximum-entropy models.",2,0.9861111111111112,3
"The earlier methods for maximum-entropy feature selection methods (Ratnaparkhi, Roukos, and Ward 1994; Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997; Papineni, Roukos, and Ward 1997, 1998) require several full passes over the training set for each round of feature selection, suggesting that at least for the parsing data, the improved boosting algorithm is several orders of magnitude more efficient.1 In section 6.4 we discuss our approach in comparison to these earlier methods for feature selection, as well as the more recent work of McCallum (2003); Zhou et al.",6,0.7478632478632479,5
(2003); and Riezler and Vasserman (2004).,2,0.7916666666666666,2
The remainder of this article is structured as follows.,2,1.1,1
Section 2 reviews historybased models for NLP and highlights the perceived shortcomings of history-based models which motivate the reranking approaches described in the remainder of the article.,2,0.9821428571428571,2
"Section 3 describes previous work (Friedman, Hastie, and Tibshirani 2000; Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002) that derives connections between boosting and maximum-entropy models for the simpler case of classification problems; this work forms the basis for the reranking methods.",8,0.7424242424242424,4
Section 4 describes how these approaches can be generalized to ranking problems.,1,1.0,2
We introduce loss functions for boosting and MRF approaches and discuss optimization methods.,3,0.6785714285714286,3
We also derive the efficient algorithm for boosting in this section.,2,0.9166666666666666,2
"Section 5 gives experimental results, investigating the performance improvements on parsing, efficiency issues, and the effect of various parameters of the boosting algorithm.",3,0.7407407407407407,2
Section 6 discusses related work in more detail.,1,0.8333333333333334,2
"Finally, section 7 gives conclusions.",2,0.7857142857142857,1
The reranking models in this article were originally introduced in Collins (2000).,2,0.9,2
"In this article we give considerably more detail in terms of the algorithms involved, their justification, and their performance in experiments on natural language parsing.",4,0.8392857142857143,2
"Before discussing the reranking approaches, we describe history-based models (Black et al.",2,0.9333333333333333,2
1992).,1,0,1
They are important for a few reasons.,1,0.8125,1
"First, several of the best-performing parsers on the WSJ treebank (e.g., Ratnaparkhi 1997; Charniak 1997, 2000; Collins 1997, 1999; Henderson 2003) are cases of history-based models.",4,0.7083333333333334,7
"Many systems applied to part-of-speech tagging, speech recognition, and other language or speech tasks also fall into this class of model.",4,0.6875,3
"Second, a particular history-based model (that of Collins [1999]) is used as the initial model for our approach.",3,0.7291666666666666,3
"Finally, it is important to describe history-based modelsand to explain their limitationsto motivate our departure from them.",2,1.0526315789473684,4
"Parsing can be framed as a supervised learning task, to induce a function f .",2,0.90625,3
"X YY given training examples xi, yi, where xi Z X , yi Z Y.",3,0.8529411764705882,2
We define GENxY to be the set of candidates for a given input x.,1,1.2142857142857142,2
A particular characteristic of the problem is the complexity of GENx .,2,0.875,1
"GENx can be very large, and each member of GENx has a rich internal structure.",3,0,2
"This contrasts with typical classification problems in which GENx is a fixed, small set, for example, f",4,0.8928571428571429,4
"In probabilistic approaches, a model is defined which assigns a probability Px, y to each x, y pair.2 The most likely parse for each sentence x is then arg maxyZGEN(x) Px, y.",2,0.9125,2
"This leaves the question of how to define Px, y.",1,1.3636363636363635,1
"In history-based approaches, a one-to-one mapping is defined between each pair x, y and a decision sequence bd1... dn.",4,0.5434782608695652,4
"The sequence bd1... dn can be thought of as the sequence of moves that build x, y in some canonical order.",2,1.0,2
"Lexicalization leads to a very large number of rules; to make the number of parameters manageable, the generation of the right-hand side of a rule is broken down into a number of decisions, as follows.",3,0.9230769230769231,4
First the head nonterminal (VBD in the above example) is chosen.,3,0.8928571428571429,3
"Finally, the lexical heads of the modifiers are chosen (her and today).",2,0.78125,2
Figure 1 illustrates this process.,1,0.75,1
Each of the above decisions has an associated probability conditioned on the left-hand side of the rule (VP(saw)) and other information in some cases.,3,0.7833333333333333,4
History-based approaches lead to models in which the log-probability of a parse tree can be written as a linear sum of parameters ak multiplied by features hk.,2,1.0535714285714286,4
"Each feature hkx, y is the count of a different event or fragment within the tree.",3,0.6944444444444444,2
"All models considered in this article take this form, although in the boosting models the score for a parse is not a log-probability.",3,0.86,3
The features hk define an m-dimensional vector of counts which represent the tree.,2,1.0714285714285714,2
The parameters ak represent the influence of each feature on the score of a tree.,2,1.0625,2
A drawback of history-based models is that the choice of derivation has a profound influence on the parameterization of the model.,2,0.9545454545454546,2
"(Similar observations have been made in the related cases of belief networks [Pearl 1988], and language models for speech recognition [Rosenfeld 1997].)",5,0.5833333333333334,3
"When designing a model, it would be desirable to have a framework in which features can be easily added to the model.",2,1.1666666666666667,2
"Unfortunately, with history-based models adding new features often requires a modification of the underlying derivations in the model.",2,0.825,3
Modifying the derivation to include a new feature type can be a laborious task.,2,0.6333333333333333,2
"In an ideal situation we would be able to encode arbitrary features hk, without having to worry about formulating a derivation that included these features.",2,1.2777777777777777,2
"To take a concrete example, consider part-of-speech tagging using a hidden Markov model (HMM).",2,0.6388888888888888,3
We might have the intuition that almost every sentence has at least one verb and therefore that sequences including at least one verb should have increased scores under the model.,3,1.1774193548387097,3
Encoding this constraint in a compact way in an HMM takes some ingenuity.,2,0,2
The obvious approachto add to each state the information about whether or not a verb has been generated in the historydoubles  the number of states (and parameters) in the model.,3,0.8333333333333334,5
"In contrast, it would be trivial to implement a feature hkx, y which is 1 if y contains a verb, 0 otherwise.",2,1.25,4
We now turn to machine-learning methods for the ranking task.,2,0.8636363636363636,2
In this section we review two methods for binary classification problems.,2,0.7916666666666666,2
logistic regression (or maximum-entropy) models and boosting.,2,0.95,2
These methods form the basis for the reranking approaches described in later sections of the article.,2,1.0,2
"Maximum-entropy models are a very popular method within the computational linguistics community; see, for example, Berger, Della Pietra, and Della Pietra (1996) for an early article which introduces the models and motivates them.",4,0.6951219512195121,3
Boosting approaches to classification have received considerable attention in the machine-learning community since the introduction of AdaBoost by Freund and Schapire (1997).,3,0.86,3
"Boosting algorithms, and in particular the relationship between boosting algorithms and maximum-entropy models, are perhaps not familiar topics in the NLP literature.",3,0.78,3
"However there has recently been much work drawing connections between the two methods (Friedman, Hastie, and Tibshirani 2000; Lafferty 1999; Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002); in this section we review this work.",8,0,4
"Much of this work has focused on binary classification problems, and this section is also restricted to problems of this type.",3,0,2
Later in the article we show how several of the ideas can be carried across to reranking problems.,2,1.0526315789473684,2
"Here aa I 7x is the inner or dot product between the vectors aa and 7x, and sign(z) = 1 if z  0, sign(z) = ",6,0.9722222222222222,3
"Geometrically, the examples x are represented as vectors 7(x) in some m-dimensional vector space, and the parameters aa define a hyperplane which passes through the origin4 of the space and has aa as its normal.",4,0.8048780487804879,3
Points lying on one side of this hyperplane are classified as +1; points on the other side are classified as ,3,0,2
"The central question in learning is how to set the parameters aa, given the training examples bx1, y1,x2, y2, .",3,0.8076923076923077,3
.,0,1,0
.,0,1,0
",xn, yn.",2,0,1
"Logistic regression and boosting involve different algorithms and criteria for training the parameters aa, but recent work (Friedman, Hastie, and Tibshirani 2000; Lafferty 1999; Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002) has shown that the methods have strong similarities.",7,0.6615384615384615,4
The next section describes parameter estimation methods.,2,0.5625,1
"3.2 Loss Functions for Logistic Regression and Boosting A central idea in both logistic regression and boosting is that of a loss function, which drives the parameter estimation methods of the two approaches.",3,0.8714285714285714,3
This section describes loss functions for binary classification.,1,0.8333333333333334,1
"Later in the article, we introduce loss functions for reranking tasks which are closely related to the loss functions for classification tasks.",2,0.9791666666666666,2
"First, consider a logistic regression model.",2,0.5625,2
Some form of maximum-likelihood estimation is often used for parameter estimation.,2,0.875,2
The parameters are chosen to maximize the log-likelihood of the training set; equivalently.,2,0.8666666666666667,4
we talk (to emphasize the similarities to the boosting approach) about minimizing the negative log-likelihood.,3,0.9166666666666666,3
"There are many methods in the literature for minimizing LogLoss( aa) with respect to aa, for example, generalized or improved iterative scaling (Berger, Della Pietra, and  4 It might seem to be a restriction to have the hyperplane passing through the origin of the space.",3,1.009433962264151,2
"However if a constant bias feature hm1x  1 for all x is added to the representation, a hyperplane passing through the origin in this new space is equivalent to a hyperplane in general position in the original m-dimensional space.",4,0.8048780487804879,3
"In the next section we describe feature selection methods, as described in Berger, Della Pietra, and Della Pietra (1996) and Della Pietra, Della Pietra, and Lafferty (1997).",5,0.7027027027027027,2
Thus we see that the logistic regression model implements a hyperplane classifier.,2,0.7692307692307693,2
"This loss function is minimized using a feature selection method, which we describe in the next section.",2,0.868421052631579,2
There are strong similarities between LogLoss (equation (4)) and ExpLoss (equation (6)).,3,0.8333333333333334,4
"In making connections between the two functions, it is useful to consider a third function of the parameters and training examples,  where gp is one if p is true, zero otherwise.",4,1.0142857142857142,4
Error( aa) is the number of incorrectly classified training examples under parameter values aa.,2,0.9117647058823529,2
3.3 Feature Selection Methods In this article we concentrate on feature selection methods.,3,0.6071428571428571,1
"algorithms which aim to make progress in minimizing the loss functions LogLoss aa and ExpLoss aa while using a small number of features (equivalently, ensuring that most parameter values in  Feature selection methods have been proposed in the maximum-entropy literature by several authors (Ratnaparkhi, Roukos, and Ward 1994; Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997; Papineni, Roukos, and Ward 1997, 1998; McCallum 2003; Zhou et al.",6,0.8186813186813187,4
2003; Riezler and Vasserman 2004).,2,0,2
"The most basic approachfor example see Ratnaparkhi, Roukos, and Ward (1994) and Berger, Della Pietra, and Della Pietra (1996)involves selection of a single feature at each iteration, followed by an update to the entire model, as follows.",4,0.7959183673469388,5
Step 1.,1,1,1
"Throughout the algorithm, maintain a set of active features.",2,0.7727272727272727,1
Initialize this set to be empty.,1,1.4285714285714286,2
Step 2.,1,1,1
"Choose a feature from outside of the set of active features which has the largest estimated impact in terms of reducing the loss function LogLoss, and add this to the active feature set.",4,0.8714285714285714,4
Step 3.,1,1,1
"Minimize LogLoss aa with respect to the set of active features; that is, allow only the active features to take nonzero parameter values when minimizing LogLoss.",3,0.9137931034482759,2
Return to Step 2.,1,1.3,1
"Methods in the boosting literature (see, for example, Schapire and Singer [1999]) can be considered to be feature selection methods of the following form.",3,0.8870967741935484,3
Step 1.,1,1,1
Start with all parameter values set to zero.,2,0.9444444444444444,2
Step 2.,1,1,1
Choose a feature which has largest estimated impact in terms of reducing the loss function ExpLoss.,2,1.2058823529411764,2
Step 3.,1,1,1
Update the parameter for the feature chosen at Step 2 in such a way as to minimize ExpLoss aa with respect to this one parameter.,2,0.8653846153846154,2
All other parameter values are left fixed.,2,0.875,1
Return to Step 2.,1,1.3,1
"The difference with this latter boosting approach is that in Step 3, only one parameter value is adjusted, namely, the parameter corresponding to the newly chosen feature.",2,0.8064516129032258,3
"Note that in this framework, the same feature may be chosen at more than one iteration.5 The maximum-entropy feature selection method can be quite inefficient, as the entire model is updated at each step.",2,0.7894736842105263,3
"For example, Ratnaparkhi (1998) quotes times of around 30 hours for 500 rounds of feature selection on a prepositionalphrase attachment task.",2,0.82,2
"These experiments were performed in 1998, when processors were no doubt considerably slower than those available today.",2,0.9473684210526315,2
"However, the PP attachment task is much smaller than the parsing task that we are addressing.",2,0.7777777777777778,2
"Our task involves around 1,000,000 examples, with perhaps a few hundred features per example, and 100,000 rounds of feature selection; this compares to 20,000 examples, 16 features per example, and 500 rounds of feature selection for the PP attachment task in Ratnaparkhi (1998).",4,0,3
"As an estimate, assuming that computational 500  200,000 complexity scales linearly in these factors,6 our task is 1,000,000  5 That is, the feature may be repeatedly updated, although the same feature will never be chosen  in consecutive iterations, because after an update the model is minimized with respect to the selected feature.",4,0.8793103448275862,3
"6 We believe this is a realistic assumption, as each round of feature selection takes Onf time, where n  is the number of training examples, and f is the number of active features on each example.",4,0.9375,3
Discriminative Reranking for NLP  as large as the PP attachment task.,2,0,2
These figures suggest that the maximum-entropy feature selection approach may be infeasible for large-scale tasks such as the one in this article.,2,0.782608695652174,3
The fact that the boosting approach does not update the entire model at each round of feature selection may be a disadvantage in terms of the number of features or the test data accuracy of the final model.,3,0.8205128205128205,2
"There is reason for concern that Step 2 will at some iterations mistakenly choose features which are apparently useful in reducing the loss function, but which would have little utility if the entire model had been optimized at the previous iteration of Step 3.",3,1.0434782608695652,4
"However, previous empirical results for boosting have shown that it is a highly effective learning method, suggesting that this is not in fact a problem for the approach.",2,0.967741935483871,3
"Given the previous strong results for the boosting approach, and for reasons of computational efficiency, we pursue the boosting approach to feature selection in this article.",4,0.7586206896551724,3
"3.4 Statistical Justification for the Methods Minimization of LogLoss is most often justified as a parametric, maximum-likelihood (ML) approach to estimation.",3,0.78,3
Thus this approach benefits from the usual guarantees for ML estimation.,2,0,2
"If the distribution generating examples is within the class of distributions specified by the log-linear form, then in the limit as the sample size goes to infinity, the model will be optimal in the sense of convergence to the true underlying distribution generating examples.",4,0.8829787234042553,3
"As far as we are aware, behavior of the models for finite sample sizes is less well understood.",3,0.95,2
"In particular, while feature selection methods have often been proposed for maximum-entropy models, little theoretical justification (in terms of guarantees about generalization) has been given for them.",4,0.90625,4
"It seems intuitive that a model with a smaller number of parameters will require fewer samples for convergence, but this is not necessarily the case, and at present this intuition lacks a theoretical basis.",4,0.7162162162162162,4
"Feature selection methods can probably be motivated either from a Bayesian perspective (through a prior favoring models with a smaller number of nonzero parameters) or from a frequentist/goodness-of-fit perspective (models with fewer parameters are less likely to fit the data by chance), but this requires additional research.",6,0.7735849056603774,5
The statistical justification for boosting approaches is quite different.,2,1.0,1
"Boosting algorithms were originally developed within the PAC framework (Valiant 1984) for machine learning, specifically to address questions regarding the equivalence of weak and strong learning.",3,0.8666666666666667,3
Freund and Schapire (1997) originally introduced AdaBoost and gave a first set of statistical guarantees for the algorithm.,3,0.7857142857142857,2
Schapire et al.,1,0,1
(1998) gave a second set of guarantees based on the analysis of margins on training examples.,2,0.9736842105263158,2
"Both papers assume that a fixed distribution D(x, y) is generating both training and test examples and that the goal is to find a hypothesis with a small number of expected errors with respect to this distribution.",3,0.8333333333333334,3
"The form of the distribution is not assumed to be known, and in this sense the guarantees are nonparametric, or distribution free. Freund and Schapire (1997) show that if the weak learning assumption holds (i.e., roughly speaking, a feature with error rate better than chance can be found for any distribution over the sample space X  {",6,0.7,14
Schapire et al.,1,0,1
"(1998) show that under the same assumption, minimization of ExpLoss using the feature selection method ensures that the distribution of margins on training data develops in such a way that good generalization performance on test examples is guaranteed.",3,0.9285714285714286,4
Thus far in this article we have presented boosting as a feature selection approach.,2,0.8666666666666667,2
"In this section, we note that there is an alternative view of boosting in which it is described as a method for combining multiple models, for example, as a method for forming a linear combination of decision trees.",2,1.0238095238095237,3
"We consider only the simpler, feature selection view of boosting in this article.",2,0.8,2
"This section is included for completeness and because the more general view of boosting may be relevant to future work on boosting approaches for parse reranking (note, however, that the discussion in this section is not essential to the rest of the article, so the reader may safely skip this section if she or he wishes to do so).",3,1.0,4
"In feature selection approaches, as described in this article, the set of possible features hkx for k = 1, .",3,0.8043478260869565,3
.,0,1,0
.,0,1,0
", m is taken to be a fixed set of relatively simple functions.",1,1.0714285714285714,2
"In particular, we have assumed that m is relatively small (for example, small enough for algorithms that require O(m) time or space to be feasible).",3,1.0909090909090908,3
"More generally, however, boosting can be applied in more complex settings.",2,0.8214285714285714,2
"For example, a common use of boosting is to form a linear combination of decision trees.",2,0.9722222222222222,2
"In this case each example x is represented as a number of attribute-value pairs, and each feature hk(x) is a complete decision tree built on predicates over the attribute values in x.",2,0,2
In this case the number of features m is huge.,2,0.8636363636363636,2
"There are as many features as there are decision trees over the given set of attributes, thus m grows exponentially quickly with the number of attributes that are used to represent an example x.",2,1.042857142857143,2
Boosting may even be applied in situations in which the number of features is infinite.,2,1.25,2
"For example, it may be used to form a linear combination of neural networks.",2,0.9375,2
In this case each feature hk(x) corresponds to a different parameter setting within the (infinite) set of possible parameter settings for the neural network.,3,0.7166666666666667,2
"In more complex settings such as boosting of decision trees or neural networks, it is generally not feasible to perform an exhaustive search (with O(m) time complexity) for the feature which has the greatest impact on the exponential7 loss function.",3,0.851063829787234,2
"Instead, an approximate search is performed.",2,0.6875,2
"In boosting approaches, this approximate search is achieved through a protocol in which at each round of boosting, a distribution over the training examples is maintained.",3,1.0,4
"The distribution can be interpreted as assigning an importance weight to each training example, most importantly giving higher weight to examples which are incorrectly classified.",2,0.9629629629629629,3
"At each round of boosting the distribution is passed to an algorithm such as a decision tree or neural network learning method, which attempts to return a feature (a decision tree, or a neural network parameter setting) which has a relatively low error rate with respect to the distribution.",3,0.8981481481481481,2
The feature that is returned is then incorporated into the linear combination of features.,2,1.1333333333333333,2
"The algorithm which generates a classifier given a distribution over the examples (for example, the decision tree induction method) is usually referred to as the weak learner. The weak learner generally uses an approximate (for example, greedy) method to find a function with a low error rate with respect to the distribution.",3,0.8548387096774194,6
"Freund and Schapire (1997) show that provided that at each round of boosting the weak learner returns a feature with greater than (50 + &) % accuracy for some fixed &, the number of training errors falls exponentially quickly with the number of rounds of boosting.",4,1.1538461538461537,2
This fast drop in training errors translates to statistical bounds on generalization performance (Freund and Schapire 1997).,2,0.825,2
"Under this view of boosting, the feature selection methods in this article are a particularly simple case in which the weak learner can afford to exhaustively search through the space of possible features.",3,0.9428571428571428,2
Future work on reranking approaches might consider other approachessuch as boosting of decision treeswhich can effectively consider more complex features.,2,1.1428571428571428,3
This section describes how the ideas from classification problems can be extended to reranking tasks.,2,1.0,3
A baseline statistical parser is used to generate N-best output both for its training set and for test data sentences.,2,0.6666666666666666,3
"Each candidate parse for a sentence is represented as a feature vector which includes the log-likelihood under the baseline model, as well as a large number of additional features.",2,0.8548387096774194,3
The additional features can in principle be any predicates over sentence/tree pairs.,2,0.8076923076923077,2
Evidence from the initial loglikelihood and the additional features is combined using a linear model.,2,0.8125,2
Parameter estimation becomes a problem of learning how to combine these different sources of information.,2,1.15625,2
The boosting algorithm we use is related to the generalization of boosting methods to ranking problems in Freund et al.,2,1.1666666666666667,2
"(1998); we also introduce an approach related to the conditional log-linear models of Ratnaparkhi, Roukos, and Ward (1994), Papineni, Roukos, and Ward (1997, 1998), Johnson et al.",5,0.5952380952380952,4
"(1999), Riezler et al.",2,0,1
"(2002), and Och and Ney (2002).",2,0,2
Section 4.1 gives a formal definition of the reranking problem.,2,0.6818181818181818,2
Section 4.2 introduces loss functions for reranking that are analogous to the LogLoss and ExpLoss functions in section 3.2.,2,0.975,2
Section 4.3 describes a general approach to feature selection methods with these loss functions.,2,0.7333333333333333,2
Section 4.4 describes a first algorithm for the exponential loss (ExpLoss) function; section 4.5 introduces a more efficient algorithm for the case of ExpLoss.,3,0,2
"Finally, section 4.6 describes issues in feature selection algorithms for the LogLoss function.",2,0.7666666666666667,2
"Thus Fx, aa can be interpreted as a measure of how plausible a parse x is, with higher scores meaning that x is more plausible.",2,0.9464285714285714,3
Competing parses for the same sentence are ranked in order of plausibility by this function.,2,0.90625,2
We can recover the base ranking functionthe log-likelihood Lxby setting a0 to a positive constant and setting all other parameter values to be zero.,3,0.78,5
Our intention is to use the training examples to pick parameter values which improve upon this initial ranking.,2,1.0526315789473684,2
We now discuss how to set these parameters.,1,1.2222222222222223,2
First we discuss loss functions Loss aa which can be used to drive the training process.,2,1.0294117647058822,2
We then go on to describe feature selection methods for the different loss functions.,2,0.8,2
"8 In the event that multiple parses get the (same) highest score, the parse with the highest value of loglikelihood L under the baseline model is taken as xi, 1.",3,0.8857142857142857,3
In the event that two parses have the same score and the same log-likelihoodwhich occurred rarely if ever in our experimentswe make a random choice between the two parses.,3,0.8833333333333333,3
9 This is not necessarily a significant issue if an application using the output from the parser is sensitive to improvements in evaluation measures such as precision and recall that give credit for partial matches between the parsers output and the correct parse.,2,0.875,3
"In this case, it is important only that the precision/ recall for xi, 1 is significantly higher than that of the baseline parser, that is, that there is some head room for the reranking module in terms of precision and recall.",3,0.9787234042553191,3
"10 In particular, this restriction allows closed-form parameter updates for the models based on ExpLoss that we consider.",2,0.9,2
"Note that features tracking the counts of different rules can be simulated through several features which take value one if a rule is seen  1 time,  2 times  3 times, and so on.",3,0.9305555555555556,3
Ranking Errors and Margins.,2,0,1
The loss functions we consider are all related to the number of ranking errors a function F makes on the training set.,3,0.8695652173913043,2
The ranking error rate is the number of times a lower-scoring parse is (incorrectly) ranked above the best parse.,3,0.9772727272727273,3
4.2.3 Exponential Loss.,1,0,1
The next loss function is based on the boosting method described in Schapire and Singer (1999).,2,0.6578947368421053,2
It is a special case of the general ranking methods described in Freund et al.,2,0.78125,2
"(1998), with the ranking feedback being a simple binary distinction between the highest-scoring parse and the other parses.",3,0,3
"Again, the loss function is a function of the margins on training data.",2,0.7666666666666667,2
"11 Note that LogLoss is not a direct upper bound on the number of ranking errors, although it can be shown that it is a (relatively loose) upper bound on the number of times the correct parse is not the highestranked parse on the model.",2,0.9897959183673469,3
The latter observation follows from the property that the correct parse must be highest ranked if its probability is greater than 0.5.,2,1.0,2
"The original definition of ExpLoss in equation (10) can be recovered by setting Si,j = 1 for all i, j (i.e., by giving equal weight to all examples).",3,1.0135135135135136,2
"In our experiments we found that a definition of Si,j such as that in equation (11) gave improved performance on development data, presumably because it takes into account the relative cost of different ranking errors in trainingdata examples.",4,0.9886363636363636,4
At this point we have definitions for ExpLoss and LogLoss which are analogous to the definitions in section 3.2 for binary classification tasks.,2,0.9583333333333334,2
Section 3.3 introduced the idea of feature selection methods; the current section gives a more concrete description of the methods used in our experiments.,3,0,2
The goal of feature selection methods is to find a small subset of the features that contribute most to reducing the loss function.,2,1.0416666666666667,2
"The methods we consider are greedy, at each iteration picking the feature hk with additive weight d which has the most impact on the loss function.",2,0.9464285714285714,2
"In general, a separate set of instances is used in cross-validation to choose the stopping point, that is, to decide on the number of features in the model.",2,1.03125,3
At this point we introduce some notation concerning feature selection methods.,2,0.7916666666666666,2
"We define Upd aa, k, d to be an updated parameter vector, with the same parameter values as aa with the exception of ak, which is incremented by d.  Note that this is essentially the idea behind the boosting approach to feature selection introduced in section 3.3.",2,1.0288461538461537,3
"In contrast, the feature selection method of Berger, Della Pietra, and Della Pietra (1996), also described in section 3.3, would involve updating parameter values for all selected features at step 2b.",4,0.6923076923076923,3
"The main computation for both loss functions involves searching for the optimal feature/weight pair k, d.",3,0.8333333333333334,3
In both cases we take a two-step approach to solving this problem.,2,0.9230769230769231,3
In the first step the optimal update for each feature hk is calculated.,2,0.9285714285714286,2
"We define BestWtk, aa as the optimal update for the kth feature (it must be calculated for all features k = 1, .",2,1.0192307692307692,2
.,0,1,0
.,0,1,0
", m).",1,0,1
In our implementation a0 was optimized using simple bruteforce search.,2,0.9090909090909091,2
"All values of a0 between 0.001 and 10 at increments of 0.001 were tested, and the value which minimized the function in equation (12) was chosen.12  Feature selection then proceeds to search for values of the remaining parameters, a1, .",4,0.9891304347826086,4
.,0,1,0
.,0,1,0
", am.",1,1.1666666666666667,0
(Note that it might be preferable to also allow a0 to be adjusted as features are added; we leave this to future work.),4,1.0740740740740742,2
"This requires calculation of the terms BestWtk, aa and BestLossk, aa for each feature.",3,0.9117647058823529,3
"For binary-valued features these values have closed-form solutions, which is computationally very convenient.",2,0.8666666666666667,3
We now describe the form of these updates.,1,0.9444444444444444,1
See appendix A for how the updates can be derived (the derivation is essentially the same as that in Schapire and Singer [1999]).,2,0.9464285714285714,3
"First, we note that for any feature, hkxi,1 ",3,0.9,4
"+1, ",3,0.6944444444444444,3
For each k we define the following sets.,2,0.7222222222222222,2
"This section presents a new algorithm which is equivalent to the ExpLoss algorithm in Figure 3, but can be vastly more efficient for problems with sparse feature spaces.",3,0.8333333333333334,3
"In the experimental section of this article we show that it is almost 2,700 times more efficient for our task than the algorithm in Figure 3.",3,0.8518518518518519,2
The efficiency of the different algorithms is important in the parsing problem.,2,0.7307692307692307,2
"The training data we eventually used contained around 36,000 sentences, with an average of 27 parses per sentence, giving around 1,000,000 parse trees in total.",3,0.9464285714285714,3
"There were over 500,000 different features.",1,0.7857142857142857,2
"The new algorithm is also applicable, with minor modifications, to boosting approaches for classification problems in which the representation also involves sparse binary features (for example, the text classification problems in Schapire and Singer [2000]).",3,0.7790697674418605,4
"As far as we are aware, the new algorithm has not appeared elsewhere in the boosting literature.",3,0.7894736842105263,2
A naive algorithm for the boosting loss function.,2,0,2
The relative efficiency of the two algorithms depends on the value of C/ T at each iteration.,2,0.868421052631579,2
"In the worst case, when every feature chosen appears on every training example, then C/ T = 1, and the two algorithms essentially have the same running time.",5,0.6666666666666666,3
However in sparse feature spaces there is reason to believe that C/ T will be small for most iterations.,2,1.0238095238095237,2
In section 5.4.3 we show that this is the case for our experiments.,2,1.0,2
We now describe an approach that was implemented for LogLoss.,1,1.2727272727272727,2
"At the first iteration, a0 is set to one.",2,0.7727272727272727,2
"Feature selection then searches for values of the remaining parameters, a1, .",3,0.8214285714285714,2
.,0,1,0
.,0,1,0
", am.",1,1.1666666666666667,0
We now describe how to calculate the optimal update for a feature k with the LogLoss function.,2,0.9722222222222222,3
"Unfortunately, unlike the case of ExpLoss, in general an analytic solution for BestWt does not exist.",4,0.8157894736842105,4
"However, we can define an iterative solution using techniques from iterative scaling (Della Pietra, Della Pietra, and Lafferty 1997).",3,0.72,2
"We first define hhk, the number of times that feature k is seen in the best parse, and ppk aa, the expected number of times under the model that feature k is seen.",4,1.0405405405405406,4
Unfortunately there does not appear to be an efficient algorithm for LogLoss that is analogous to the ExpLoss algorithm in Figure 4 (at least if the feature selection method is required to pick the feature with highest impact on the loss function at each iteration).,3,0.90625,4
"A similar observation for LogLoss can be made, in that when the model is updated with a feature/weight pair k, d, many features will have their values for BestWt and BestLoss unchanged.",4,0.7916666666666666,4
Only those features which co-occur with k on some example will need to have their values of BestWt and BestLoss updated.,3,0.9545454545454546,3
"However, this observation does not lead to an efficient algorithm.",2,0.625,2
Updating these values is much more expensive than in the ExpLoss case.,2,0,2
"The procedure for finding the optimal value BestWtk, aa must be applied for each feature which co-occurs with the chosen feature k.",3,0.8541666666666666,2
"For example, the iterative scaling procedure described above must be applied for a number of features.",2,0.8611111111111112,2
"For each feature, this will involve recalculation of the distribution fPxi, 1 j si, Pxi,2 j si, .",3,0.717391304347826,3
.,0,1,0
"., Pxi,ni j sig for each example i on which the feature occurs.13 It takes only one feature that is seen on all training examples for the algorithm to involve recalculation of Pxi,j j si for the entire training set.",0,1,144
This contrasts with the simple updates in the improved boosting algorithm W k  D.,1,0.7857142857142857,2
"In fact in the parsing experiments, we were forced to give up on the LogLoss feature selection methods because of their inefficiency (see section 6.4 for more discussion about efficiency).",3,0.8970588235294118,3
"Note, however, that approximate methods for finding the best feature and updating its weight may lead to efficient algorithms.",2,0.9318181818181818,3
"Appendix B gives a sketch of one such approach, which is based on results from Collins, Schapire, and Singer (2002).",3,0.8076923076923077,2
We did not test this method; we leave this to future work.,2,0,2
"We used the Penn Wall Street treebank (Marcus, Santorini, and Marcinkiewicz 1993) as training and test data.",3,0.5681818181818182,3
"Sections 221 inclusive (around 40,000 sentences) were used as training data, section 23 was used as the final test set.",4,0,2
"Of the 40,000 training sentences, the first 36,000 were used as the main training set.",3,0.5,2
"The remaining 4,000 sentences were used as development data and to cross-validate the number of rounds (features) in the model.",2,0.8695652173913043,2
"Model 2 of Collins (1999) was used to parse both the training and test data, producing multiple hypotheses for each sentence.",3,0.9,3
"We achieved this by disabling dynamic programming in the parser and choosing a relatively narrow beam width of 1,000.",3,0,2
The resulting parser returns all parses that fall within the beam.,2,0.7083333333333334,2
The number of such parses varies sentence by sentence.,2,0.95,1
"In order to gain a representative set of training data, the 36,000 training sentences were parsed in 2,000 sentence chunks, each chunk being parsed with a model trained on the remaining 34,000 sentences (this prevented the initial model from being unrealistically good on the training sentences).",3,0.7647058823529411,3
"The 4,000 development sentences were parsed with a model trained on the 36,000 training sentences.",2,0.65625,2
"Section 23 was parsed with a model trained on all 40,000 sentences.",2,0.8076923076923077,2
The following types of features were included in the model.,2,0.8636363636363636,1
We will use the rule VP PP VBD NP NP SBAR with head VBD as an example.,4,0.5833333333333334,3
Note that the output of our baseline parser produces syntactic trees with headword annotations (see Collins [1999]) for a description of the rules used to find headwords).,3,1.0,4
"Note that in the rare cases in which the baseline parser produces no constituents, the precision is  undefined; in these cases we defined the F-measure to be 0.",4,0.9032258064516129,4
These are adjacent pairs of nonterminals to the left and right of the head.,2,0.9,2
"All head-modifier pairs, with the grandparent nonterminal also included.",3,0.6818181818181818,3
"An adj flag is also included, which is one if the modifier is adjacent to the head, zero otherwise.",2,0.9772727272727273,3
"As an example, say the nonterminal dominating the example rule is S.   Lexical trigrams involving the heads of arguments of prepositional phrases.",2,0.8333333333333334,2
"The example shown at right would contribute the trigram (NP, NP, PP, NP, president, of, U.S.), in addition to the relation (NP, NP, PP, NP, of, U.S.), which ignores the headword of the constituent being modified by the PP.",5,0.6666666666666666,3
"The three nonterminals (for example, NP, NP, PP) identify the parent of the entire phrase, the nonterminal of the head of the phrase, and the nonterminal label for the PP.",4,0.618421052631579,3
Distance head modifiers.,1,0,1
Features involving the distance between headwords.,1,1,1
"For example, assume dist is the number of words between the headwords of the VBD and SBAR in the (VP, VBD, SBAR) head-modifier relation in the above rule.",3,0.7205882352941176,3
"In order to generate more features, a second pass was made in which all nonterminals were augmented with their lexical heads when these headwords were closed-class words.",2,0.896551724137931,3
"All features apart from head modifiers, PPs, and distance head modifiers were then generated with these augmented nonterminals.",3,0.6428571428571429,2
"All of these features were initially generated, but only features seen on at least one parse for at least five different sentences were included in the final model (this count cutoff was implemented to keep the number of features down to a tractable number).",3,0.8854166666666666,3
The ExpLoss method was trained with several values for the smoothing parameter &.,2,0.8571428571428571,2
"{0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075}.",7,0.5833333333333334,4
"For each value of &, the method was run for 100,000 rounds on the training data.",3,0.8055555555555556,2
"The implementation was such that the feature updates for all 100,000 rounds for each training run were recorded in a file.",2,0.8181818181818182,3
"This made it simple to test the model on development data for all values of N between 0 and 100,000.",2,1.0714285714285714,2
"The &, N values which maximized this quantity were used to define the final model applied to the test data (section 23 of the treebank).",2,1.0862068965517242,2
"The optimal values were &  0.0025 and N  90,386, at which point 11,673 features had nonzero values (note that the feature selection techniques may result in a given feature being updated more than once).",3,0.8289473684210527,3
The computation took roughly 34 hours on a machine with a 1.6 GHz pentium processor and around 2 GB of memory.,3,0.7045454545454546,3
Table 1 shows results for the method.,1,0.9375,1
The model of Collins (1999) was the base model; the ExpLoss model gave a 1.5% absolute improvement over this method.,3,0,2
"The method gives very similar accuracy to the model of Charniak (2000), which also uses a rich set of initial features in addition to Charniaks (1997) original model.",3,0.8823529411764706,3
The LogLoss method was too inefficient to run on the full data set.,2,0.7142857142857143,2
"Instead we made some tests on a smaller subset of the data (5,934 sentences, giving 200,000 parse trees) and 52,294 features.15 On an older machine (an order of magnitude or more slower than the machine used for the final tests) the boosting method took 40 minutes for 10,000 rounds on this data set.",3,0.8666666666666667,3
"The LogLoss method took 20 hours to complete 3,500 rounds (a factor of about 85 times slower).",2,0.75,2
This was in spite of various heuristics that were implemented in an attempt to speed up LogLoss.,2,1.1944444444444444,2
"for example, selecting multiple features at each round or recalculating the statistics for only the best K features for some small K at the previous round of feature selection.",3,0.6290322580645161,3
"In initial experiments we found ExpLoss to give similar, perhaps slightly better, accuracy than LogLoss.",3,0.9444444444444444,2
This section describes further experiments investigating various aspects of the boosting algorithm.,2,0.8076923076923077,1
"the effect of the & and N parameters, learning curves, the choice of the Si,j weights, and efficiency issues.",4,0.7,3
The Effect of the & and N Parameters.,2,0,2
Figure 5 shows the learning curve on development data for the optimal value of & (0.0025).,3,1.0,3
"The accuracy shown is the performance relative to the baseline method of using the probability from the generative model alone in ranking parses, where the measure in equation (21) is used to measure performance.",2,1.0,3
"For example, a score of 101.5 indicates a 1.5% increase in this score.",2,0.78125,2
"The learning curve is initially steep, eventually flattening off, but reaching its peak value after a large number (90,386) of rounds of feature selection.",2,0.8275862068965517,2
Table 2 indicates how the peak performance varies with the smoothing parameter &.,2,1.1071428571428572,2
Figure 6 shows learning curves for various values of &.,2,1.0454545454545454,1
It can be seen that values other than &  0.0025 can lead to undertraining or overtraining of the model.,2,1.175,3
All features described above except distance head modifiers and further lexicalization were included.,3,0.75,2
Results on section 23 of the WSJ Treebank.,2,0,2
LR is labeled recall; LP is labeled precision; CBs is the average number of crossing brackets per sentence; 0 CBs is the percentage of sentences with 0 crossing brackets; 2 CBs is the percentage of sentences with two or more crossing brackets.,5,0.9042553191489362,3
"All the results in this table are for models trained and tested on the same data, using the same evaluation metric.",3,0.6956521739130435,3
Note that the ExpLoss results are very slightly different from the original results published in Collins (2000).,2,0.85,2
"We recently reimplemented the boosting code and reran the experiments, and minor differences in the code and & values tested on development data led to minor improvements in the results.",3,0,3
Learning curve on development data for the optimal value for & (0.0025).,2,1.0,2
"The y-axis is the level of accuracy (100 is the baseline score), and the x-axis is the number of rounds of boosting.",3,0.8846153846153846,2
Peak performance achieved for various values of &.,2,1.0555555555555556,2
Best N refers to the number of rounds at which peak development set accuracy was reached.,2,1.1764705882352942,2
"Best score indicates the relative performance, compared to the baseline method, at the optimal value for N.  5.4.2 The Effect of the Si,j Weights on Examples.",4,0.6935483870967742,2
"In section 4.2.3 we introduced the idea of weights Si,j representing the importance of examples.",2,1.0277777777777777,2
"Using this definition, we trained the ExpLoss method on the same training set for several values of the smoothing parameter & and evaluated the performance on development data.",3,0.7166666666666667,4
"Table 3 compares the peak performance achieved under the two definitions of Si,j on the development set.",2,0.825,3
It can be seen that the definition in equation (22) outperforms the simpler method in equation (23).,2,1.0454545454545454,3
Figure 7 shows the learning curves for the optimal values of & for the two methods.,3,0.8235294117647058,2
"It can be seen that the learning curve for the definition of Si,j in equation (22) consistently dominates the curve for the simpler definition.",4,0.9137931034482759,4
5.4.3 Efficiency Gains.,1,0,1
Section 4.5 introduced an efficient algorithm for optimizing ExpLoss.,2,0.9,2
In this section we explore the empirical gains in efficiency seen on the parsing data sets in this article.,2,0.775,2
Learning curves on development data for various values of &.,2,0.9545454545454546,1
"In each case the y-axis is the level of accuracy (100 is the baseline score), and the x-axis is the number of rounds of boosting.",4,0.8620689655172413,2
"The three graphs compare the curve for & = 0.0025 (the optimal value) to (from top to bottom) & = 0.0001, & = 0.0075, and & = 0.001.",5,0,5
The top graph shows that & = 0.0001 leads to undersmoothing (overtraining).,2,1.0666666666666667,1
"Initially the graph is higher than that for & = 0.0025, but on later rounds the performance starts to decrease.",3,0.8409090909090909,2
The middle graph shows that & = 0.0075 leads to oversmoothing (undertraining).,2,1.0666666666666667,2
The graph shows consistently lower performance than that for & = 0.0025.,2,1.1153846153846154,2
The bottom graph shows that there is little difference in performance for & = 0.001 versus & = 0.0025.,2,0.95,2
This is a measure of the number of updates to the W k variables required in making a pass over the entire training set.,2,0.88,2
"Thus it is a measure of the amount of computation that the naive algorithm for ExpLoss, presented in Figure 3, requires for each round of feature selection.",3,0.9333333333333333,3
"Next, say the improved algorithm in Figure 4 selects feature k* on the t th round of  feature selection.",3,0.9318181818181818,2
This is a measure of the number of summations required by the improved algorithm in Figure 4 at the t th round of feature selection.,2,0.9423076923076923,2
"Performance versus number of rounds of boosting for Si,j  Scorexi,1 ",3,1.0,6
We are now in a position to compare the running times of the two algorithms.,2,0.875,2
"Here, Workn is the computation required for n rounds of feature selection, where a single unit of computation corresponds to a pass over the entire training set.",2,0.9,2
"Savingsn tracks the relative efficiency of the two algorithms as a function of the number of features, n. For example, if Savings100  1,200, this signifies that for the first 100 rounds of feature selection, the improved algorithm is 1,200 times as efficient as the naive algorithm.",4,0.7647058823529411,3
"Finally, Savingsa, b indicates the relative efficiency between rounds a and b, inclusive, of feature selection.",3,0.7857142857142857,2
"For example, Savings11, 100  83 signifies that between rounds 11 and 100 inclusive of the algorithm, the improved algorithm was 83 times as efficient.",3,0.8214285714285714,4
Figures 8 and 9 show graphs of Workn and Savingsn versus n. The savings from the improved algorithm are dramatic.,3,0,2
"In 100,000 rounds of feature selection, the improved algorithm requires total computation that is equivalent to a mere 37.1 passes over the training set.",3,0.9038461538461539,2
"This is a saving of a factor of 2,692 over the naive algorithm.",2,0.9642857142857143,2
"Table 4 shows the value of Savingsa,b for various values of a,b.",1,1.09375,2
"It can be seen that the performance gains are significantly larger in later rounds of feature selection, presumably because in later stages relatively infrequent features are being selected.",2,0.9166666666666666,4
"Even so, there are still savings of a factor of almost 50 in the early stages of the method.",2,0.9285714285714286,2
"Charniak (2000) describes a parser which incorporates additional features into a previously developed parser, that of Charniak (1997).",2,1.0416666666666667,2
The method gives substantial improvements over the original parser and results which are very close to the results of the boosting method we have described in this article (see section 5 for experimental results comparing the two methods).,3,0.9512195121951219,3
Our features are in many ways similar to those of Charniak (2000).,2,1.0333333333333334,1
"The model in Charniak (2000) is quite different, however.",3,0.8846153846153846,2
"The additional features are incorporated using a method inspired by maximum-entropy models (e.g., the model of Ratnaparkhi [1997]).",3,0.875,2
"Both approaches still rely on decomposing a parse tree into a sequence of decisions, and we would argue that the techniques described in this article have more flexibility in terms of the features that can be included in the model.",3,0.9642857142857143,3
Abney (1997) describes the application of log-linear models to stochastic headdriven phrase structure grammars (HPSGs).,2,0.775,3
"Della Pietra, Della Pietra, and Lafferty (1997) describe feature selection methods for log-linear models, and Rosenfeld (1997) describes application of these methods to language modeling for speech recognition.",4,0,3
These methods all emphasize models which define a joint probability over the space of all parse trees (or structures in question).,2,1.0,3
For this reason we describe these approaches.,2,0.8125,1
"Here Z is the (infinite) set of possible trees, and the denominator cannot be calculated explicitly.",4,0,3
"This is a problem for parameter estimation, in which an estimate of the denominator is required, and Monte Carlo methods have been proposed (Della Pietra, Della Pietra, and Lafferty 1997; Abney 1997; Rosenfeld 1997) as a technique for estimation of this value.",4,0.803921568627451,3
Our sense is that these methods can be computationally expensive.,1,0.9090909090909091,2
"Notice that the joint likelihood in equation (27) is not a direct function of the margins on training examples, and its relation to error rate is therefore not so clear as in the discriminative approaches described in this article.",3,0.8372093023255814,4
"Ratnaparkhi, Roukos, and Ward (1994), Johnson et al.",3,0,2
"(1999), and Riezler et al.",2,0,1
"(2002) suggest training log-linear models (i.e., the LogLoss function in equation (9)) for parsing problems.",3,0.8478260869565217,4
"Ratnaparkhi, Roukos, and Ward (1994) use feature selection techniques for the task.",3,0.5588235294117647,2
"Closed-form updates under iterative scaling are not possible with this objective function; instead, optimization algorithms such as gradient descent or conjugate gradient methods are used to estimate parameter values.",3,0.609375,3
"In more recent work, Lafferty, McCallum, and Pereira (2001) describe the use of conditional Markov random fields (CRFs) for tagging tasks such as named entity recognition or part-of-speech tagging (hidden Markov models are a common method applied to these tasks).",3,0.67,4
CRFs employ the objective function in equation (28).,2,0.9545454545454546,2
"A key insight of Lafferty, McCallum, and Pereira (2001) is that when features are of a significantly local nature, the gradient of the function in equation (28) can be calculated efficiently using dynamic programming, even in cases in which the set of candidates involves all possible tagged sequences and is therefore exponential in size.",3,0.9761904761904762,4
See also Sha and Pereira (2003) for more recent work on CRFs.,3,0.9,2
"Optimizing a log-linear model with a Gaussian prior (i.e., choosing parameter values which achieve the global minimum of the objective function in equation (28)) is a plausible alternative to the feature selection approaches described in the current article or to the feature selection methods previously applied to log-linear models.",4,0.8636363636363636,3
"The Gaussian prior (i.e., the k penalty) has been found in practice to be very effective in combating overfitting of the parameters to the training data (Chen and Rosenfeld 1999; Johnson et al.",3,0.8846153846153846,3
"1999; Lafferty, McCallum, and Pereira 2001; Riezler et al.",3,0,3
2002).,1,0,1
"The function in equation (28) can be optimized using variants of gradient descent, which in practice require tens or at most hundreds of passes over the training data (see, e.g., Sha and Pereira 2003).",3,0.9761904761904762,2
"Thus log-linear models with a Gaussian prior are likely to be comparable in terms of efficiency to the feature selection approach described in this article (in the experimental section, we showed that for the parsereranking task, the efficient boosting algorithm requires computation that is equivalent to around 40 passes over the training data).",3,1.0172413793103448,3
"Note, however, that the two methods will differ considerably in terms of the sparsity of the resulting reranker.",2,0.9047619047619048,3
"Whereas the feature selection approach leads to around 11,000 (2%) of the features in our model having nonzero parameter values, log-linear models with Gaussian priors typically have very few nonzero parameters (see, e.g., Riezler and Vasserman 2004).",5,0.7065217391304348,3
"This may be important in some domains, for example, those in which there are a very large number of features and this large number leads to difficulties in terms of memory requirements or computation time.",3,0.9078947368421053,3
"6.4 Feature Selection Methods A number of previous papers (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1998; Della Pietra, Della Pietra, and Lafferty 1997; McCallum 2003; Zhou et al.",5,0,3
2003; Riezler and Vasserman 2004) describe feature selection approaches for log-linear models applied to NLP problems.,2,0.8157894736842105,3
"Earlier work (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1998; Della Pietra, Della Pietra, and Lafferty 1997) suggested methods that added a feature at a time to the model and updated all parameters in the current model at each step (for more detail, see section 3.3).",4,0.7796610169491526,4
"Assuming that selection of a feature takes one pass over the training set and that fitting a model takes p passes over the training set, these methods require f  p + 1 passes over the training set, where f is the number of features selected.",5,0.9583333333333334,3
"In our experiments, f , 10,000.",3,0,2
"It is difficult to estimate the value for p, but assuming (very conservatively) that p = 2, selecting 10,000 features would require 30,000 passes over the training set.",3,0.9696969696969697,4
"This is around 1,000 times as much computation as that required for the efficient boosting algorithm applied to our data, suggesting that the feature selection methods in Berger, Della Pietra, and Della Pietra (1996), Ratnaparkhi (1998), and Della Pietra, Della Pietra, and Lafferty (1997) are not sufficiently efficient for the parsing task.",4,0.8106060606060606,4
More recent work (McCallum 2003; Zhou et al.,2,0,2
"2003; Riezler and Vasserman 2004) has considered methods for speeding up the feature selection methods described in Berger, Della Pietra, and Della Pietra (1996), Ratnaparkhi (1998), and Della Pietra, Della Pietra, and Lafferty (1997).",3,0.7551020408163265,3
"McCallum (2003) and Riezler and Vasserman (2004) describe approaches that add k features at each step, where k is some constant greater than one.",2,0.9833333333333333,2
The running time for these methods is therefore O f  p  1=k.,2,0.9642857142857143,1
"Riezler and Vasserman (2004) test a variety of values for k, finding that k = 100 gives optimal performance.",2,1.0869565217391304,3
"McCallum (2003) uses a value of k = 1,000.",2,1.125,2
Zhou et al.,1,0,1
(2003) use a different heuristic that avoids having to recompute the gain for every feature at every iteration.,2,1.1428571428571428,2
We would argue that the alternative feature selection methods in the current article may be preferable on the grounds of both efficiency and simplicity.,2,0.76,3
"Even with large values of k in the approach of McCallum (2003) and Riezler and Vasserman (2004) (e.g., k = 1,000), the approach we describe is likely to be at least as efficient as these alternative approaches.",4,1.0543478260869565,3
"In terms of simplicity, the methods in McCallum (2003) and Riezler and Vasserman (2004) require selection of a number of free parameters governing the behavior of the algorithm.",3,0.8970588235294118,3
"the value for k, the value for a regularizer constant (used in both McCallum [2003] and Riezler and Vasserman [2004]), and the precision with which the model is optimized at each stage of feature selection (McCallum [2003] describes using just a few BFGS iterations at each stage).",5,0.8666666666666667,6
"In contrast, our method requires a single parameter to be chosen (the value for the & smoothing parameter) and makes a single approximation (that only a single feature is updated at each round of feature selection).",3,0.7738095238095238,4
"The latter approximation is particularly important, as it leads to the efficient algorithm in Figure 4, which avoids a pass over the training set at each iteration of feature selection (note that in sparse feature spaces, f rounds of feature selection in our approach can take considerably fewer than f passes over the training set, in contrast to other work on feature selection within log-linear models).",3,0.8972602739726028,4
Note that there are other important differences among the approaches.,2,1.0,2
"Both Della Pietra, Della Pietra, and Lafferty (1997) and McCallum (2003) describe methods that induce conjunctions of base features, in a way similar to decision tree learners.",3,0.8857142857142857,3
Thus a relatively small number of base features can lead to a very large number of possible conjoined features.,2,0.925,2
In future work it might be interesting to consider these kinds of approaches for the parsing problem.,2,1.0,2
"Another difference is that both McCallum, and Riezler and Vasserman, describe approaches that use a regularizer in addition to feature selection.",2,1.0416666666666667,3
McCallum uses a two-norm regularizer; Riezler and Vasserman use a one-norm regularizer.,2,0,3
Freund et al.,1,0,1
(1998) introduced a formulation of boosting for ranking problems.,2,1.0833333333333333,1
The problem we have considered is a special case of the problem in Freund et al.,2,1.1176470588235294,2
"(1998), in that we have considered a binary distinction between candidates (i.e., the best parse vs. other parses), whereas Freund et al.",3,0.9666666666666667,4
consider learning full or partial orderings over candidates.,2,0.8888888888888888,2
"The improved algorithm that we introduced in Figure 4 is, however, a new algorithm that could perhaps be generalized to the full problem of Freund et al.",2,0.9666666666666667,2
(1998); we leave this to future research.,2,0.7727272727272727,1
"Altun, Hofmann, and Johnson (2003) and Altun, Johnson, and Hofmann (2003) describe experiments on tagging tasks using the ExpLoss function, in contrast to the LogLoss function used in Lafferty, McCallum, and Pereira (2001).",4,0.7340425531914894,4
"Altun, Hofmann, and Johnson (2003) describe how dynamic programming methods can be used to calculate gradients of the ExpLoss function even in cases in which the set of candidates again includes all possible tagged sequences, a set which grows exponentially in size with the length of the sentence being tagged.",2,1.0357142857142858,2
"Results in Altun, Johnson, and Hofmann (2003) suggest that the choice of ExpLoss versus LogLoss does not have a major impact on accuracy for the tagging task in question.",3,0.8235294117647058,3
"Perceptron-based algorithms, or the voted perceptron approach of Freund and Schapire (1999), are another alternative to boosting and LogLoss methods.",3,0.86,2
"See Collins (2002a, 2002b) and Collins and Duffy (2001, 2002) for applications of the perceptron algorithm.",3,0.9347826086956522,5
"Collins (2002b) gives convergence proofs for the methods; Collins (2002a) directly compares the boosting and perceptron approaches on a named entity task; and Collins and Duffy (2001, 2002) use a reranking approach with kernels, which allow representations of parse trees or labeled sequences in very-high-dimensional spaces.",4,0.8157894736842105,3
"Shen, Sarkar, and Joshi (2003) describe support vector machine approaches to ranking problems and apply support vector machines (SVMs) using tree-adjoining grammar (Joshi, Levy, and Takahashi 1975) features to the parsing data sets we have described in this article, with good empirical results.",4,0.7545454545454545,4
"See Collins (2004) for a discussion of many of these methods, including an overview of statistical bounds for the boosting, perceptron, and SVM methods, as well as a discussion of the computational issues involved in the different algorithms.",4,0.8111111111111111,4
"This article has introduced a new algorithm, based on boosting approaches in machine learning, for ranking problems in natural language processing.",3,0.7916666666666666,3
The approach gives a 13% relative reduction in error on parsing Wall Street Journal data.,3,0.7647058823529411,2
"While in this article the experimental focus has been on parsing, many other problems in natural language processing or speech recognition can also be framed as reranking problems, so the methods described should be quite broadly applicable.",5,0.675,3
"The boosting approach to ranking has been applied to named entity segmentation (Collins 2002a) and natural language generation (Walker, Rambow, and Rogati 2001).",3,0.7166666666666667,2
The key characteristics of the approach are the use of global features and of a training criterion (optimization  accuracy).,2,0.75,2
"In addition, the article introduced a new algorithm for the boosting approach which takes advantage of the sparse nature of the feature space in the parsing data that we use.",2,0.890625,2
Other NLP tasks are likely to have similar characteristics in terms of sparsity.,2,1.0714285714285714,1
"Experiments show an efficiency gain of a factor of over 2,600 on the parsing data for the new algorithm over the obvious implementation of the boosting approach.",2,0.8035714285714286,3
We would argue that the improved boosting algorithm is a natural alternative to maximum-entropy or (conditional) log-linear models.,3,0.9761904761904762,3
"The article has drawn connections between boosting and maximum-entropy models in terms of the optimization problems that they involve, the algorithms used, their relative efficiency, and their performance in empirical tests.",4,0.9428571428571428,3
Thanks to Rob Schapire and Yoram Singer for useful discussions on boosting algorithms and to Mark Johnson for useful discussions about linear models for parse ranking.,4,0.9074074074074074,2
Steve Abney and Fernando Pereira gave useful feedback on earlier drafts of this work.,2,0.6333333333333333,2
"Finally, thanks to the anonymous reviewers for several useful comments.",2,0,2
