sentence,yngve_score,frazier_score,dependency_distance_score,count,year
Ranking Algorithms for NamedEntity Extraction.,1,0,2,6,2002
"Boosting and the Voted Perceptron  This paper describes algorithms which rerank the top N hypotheses from a maximum-entropy tagger, the application being the recovery of named-entity boundaries in a corpus of web data.",3,0.8714285714285714,3,35,2002
The rst approach uses a boosting algorithm for ranking problems.,2,0.6818181818181818,1,11,2002
The second approach uses the voted perceptron algorithm.,2,0.5,2,9,2002
"Both algorithms give comparable, signicant improvements over the maximum-entropy baseline.",2,0.625,2,12,2002
"The voted perceptron algorithm can be considerably more efcient to train, at some cost in computation on test examples.",2,0.9761904761904762,2,21,2002
Recent work in statistical approaches to parsing and tagging has begun to consider methods which incorporate global features of candidate structures.,2,1.1136363636363635,2,22,2002
Examples of such techniques are Markov Random Fields (Abney 1997; Della Pietra et al.,3,0.9117647058823529,2,17,2002
1997; Johnson et al.,1,0,1,6,2002
"1999), and boosting algorithms (Freund et al.",2,0.7727272727272727,2,11,2002
1998; Collins 2000; Walker et al.,2,0,2,9,2002
2001).,1,0,1,3,2002
One appeal of these methods is their exibility in incorporating features into a model.,2,1.0,1,15,2002
essentially any features which might be useful in discriminating good from bad structures can be included.,2,1.1470588235294117,2,17,2002
"A second appeal of these methods is that their training criterion is often discriminative, attempting to explicitly push the score or probability of the correct structure for each training sentence above the score of competing structures.",2,0.868421052631579,2,38,2002
This discriminative property is shared by the methods of (Johnson et al.,2,0.8928571428571429,2,14,2002
"1999; Collins 2000), and also the Conditional Random Field methods of (Lafferty et al.",3,0,3,19,2002
2001).,1,0,1,3,2002
"In a previous paper (Collins 2000), a boosting algorithm was used to rerank the output from an ex isting statistical parser, giving signicant improvements in parsing accuracy on Wall Street Journal data.",3,0.6891891891891891,3,37,2002
"Similar boosting algorithms have been applied to natural language generation, with good results, in (Walker et al.",2,0.8809523809523809,3,21,2002
2001).,1,0,1,3,2002
In this paper we apply reranking methods to named-entity extraction.,2,0.7727272727272727,2,11,2002
"A state-ofthe-art (maximum-entropy) tagger is used to generate 20 possible segmentations for each input sentence, along with their probabilities.",2,0.8260869565217391,3,23,2002
We describe a number of additional global features of these candidate segmentations.,2,0.8076923076923077,2,13,2002
These additional features are used as evidence in reranking the hypotheses from the max-ent tagger.,2,0.8125,2,16,2002
We describe two learning algorithms.,1,1.3333333333333333,1,6,2002
"the boosting method of (Collins 2000), and a variant of the voted perceptron algorithm, which was initially described in (Freund & Schapire 1999).",4,0.8166666666666667,4,30,2002
We applied the methods to a corpus of over one million words of tagged web data.,2,0.7941176470588235,2,17,2002
"The methods give significant improvements over the maximum-entropy tagger (a 17.7% relative reduction in error-rate for the voted perceptron, and a 15.6% relative improvement for the boosting method).",4,0.6617647058823529,4,34,2002
"One contribution of this paper is to show that existing reranking methods are useful for a new domain, named-entity tagging, and to suggest global features which give improvements on this task.",3,0.9705882352941176,3,34,2002
"We should stress that another contribution is to show that a new algorithm, the voted perceptron, gives very credible results on a natural language task.",2,1.0178571428571428,3,28,2002
"It is an extremely simple algorithm to implement, and is very fast to train (the testing phase is slower, but by no means sluggish).",3,0.8448275862068966,3,29,2002
It should be a viable alternative to methods such as the boosting or Markov Random Field algorithms described in previous work.,3,0.7045454545454546,2,22,2002
Over a period of a year or so we have had over one million words of named-entity data annotated.,2,0.925,2,20,2002
"The  Data is drawn from web pages, the aim being to support a question-answering system over web data.",2,0.85,2,20,2002
A number of categories are annotated.,2,1.0714285714285714,1,7,2002
"the usual people, organization and location categories, as well as less frequent categories such as brand-names, scientic terms, event titles (such as concerts) and so on.",4,0,3,33,2002
"From this data we created a training set of 53,609 sentences (1,047,491 words), and a test set of 14,717 sentences (291,898 words).",3,0.7321428571428571,3,28,2002
The task we consider is to recover named-entity boundaries.,2,1.35,2,10,2002
We leave the recovery of the categories of entities to a separate stage of processing.1 We evaluate different methods on the task through pre cision and recall.,3,0,2,29,2002
"The problem can be framed as a tagging task  to tag each word as being either the start of an entity, a continuation of an entity, or not to be part of an entity at all (we will use the tags S, C and N respectively for these three cases).",4,0.8571428571428571,4,56,2002
"As a baseline model we used a maximum entropy tagger, very similar to the ones described in (Ratnaparkhi 1996; Borthwick et.",3,0.78,3,25,2002
al 1998; McCallum et al.,2,0,1,7,2002
2000).,1,0,1,3,2002
"Max-ent taggers have been shown to be highly competitive on a number of tagging tasks, such as part-of-speech tagging (Ratnaparkhi 1996), named-entity recognition (Borthwick et.",2,0.8709677419354839,3,31,2002
"al 1998), and information extraction tasks (McCallum et al.",2,0,2,13,2002
2000).,1,0,1,3,2002
Thus the maximumentropy tagger we used represents a serious baseline for the task.,2,1.0,2,14,2002
We used the following features (several of the features were inspired by the approach of (Bikel et.,1,1.0,2,20,2002
"al 1999), an HMM model which gives excellent results on named entity extraction).",3,0.8823529411764706,3,17,2002
"1In initial experiments, we found that forcing the tagger to recover categories as well as the segmentation, by exploding the number of tags, reduced performance on the segmentation task, presumably due to sparse data problems.",4,0.8625,3,40,2002
The parameters are trained using Generalized Iterative Scaling.,1,0.8888888888888888,1,9,2002
"Following (Ratnaparkhi 1996), we only include features which occur 5 times or more in training data.",3,0.825,2,20,2002
"In decoding, we use a beam search to recover 20 candidate tag sequences for each sentence (the sentence is decoded from left to right, with the top 20 most probable hypotheses being stored at each point).",3,0.7804878048780488,3,41,2002
"As a baseline we trained a model on the full 53,609 sentences of training data, and decoded the 14,717 sentences of test data.",3,0.7,3,25,2002
"This gave 20 candidates per  test sentence, along with their probabilities.",2,0.8076923076923077,2,13,2002
"The baseline method is to take the most probable candidate for each test data sentence, and then to calculate precision and recall gures.",4,0.68,3,25,2002
"Our aim is to come up with strategies for reranking the test data candidates, in such a way that precision and recall is improved.",3,0.9230769230769231,3,26,2002
"the 53,609 sentences of training data were split into a 41,992 sentence training portion, and a 11,617 sentence development set.",3,0.5227272727272727,2,22,2002
"The training portion was split into 5 sections, and in each case the maximum-entropy tagger was trained on 4/5 of the data, then used to decode the remaining 1/5.",3,0.734375,3,32,2002
"The top 20 hypotheses under a beam search, together with their log probabilities, were recovered for each training sentence.",4,0.6590909090909091,2,22,2002
"In a similar way, a model trained on the 41,992 sentence set was used to produce 20 hypotheses for each sentence in the development set.",2,0.7777777777777778,3,27,2002
The module we describe in this section generates global features for each candidate tagged sequence.,2,1.21875,2,16,2002
"As input it takes a sentence, along with a proposed segmentation (i.e., an assignment of a tag for each word in the sentence).",2,0.8035714285714286,3,28,2002
"As output, it produces a set of feature strings.",2,0.8636363636363636,1,11,2002
We will use the following tagged sentence as a running example in this section.,2,0.9,2,15,2002
An example feature type is simply to list the full strings of entities that appear in the tagged input.,2,0.975,2,20,2002
"In this example, this would give the three features.",2,0.6818181818181818,2,11,2002
Here WE stands for whole entity.,1,0.9285714285714286,2,7,2002
"Throughout this section, we will write the features in this format.",2,0.7307692307692307,2,13,2002
"The start of the feature string indicates the feature type (in this case WE), followed by =.",3,0.7380952380952381,3,21,2002
"Following the type, there are generally 1 or more words or other .",3,0.75,2,14,2002
"symbols, which we will separate with the symbol implementation takes the strings produced by the global-feature  A seperate module in our  generator, and hashes them to integers.",3,0.8666666666666667,4,30,2002
"For example, suppose the three strings WE=Gen Xer, WE=The Day They Shot John Lennon, WE=Dougherty Arts Center were hashed to 100, 250, and 500 respectively.",5,0,2,37,2002
"Conceptually, is represented by a large number is the number of distinct feature strings in training data.",2,0.7631578947368421,2,19,2002
We now introduce some notation with which to describe the full set of global features.,2,1.0,2,16,2002
"First, we assume the following primitives of an input candidate.",2,0.7083333333333334,2,12,2002
sequence.,0,0,0,2,2002
Each character in the word is  character types are not repeated in the mapped string.,2,1.125,2,16,2002
"For example, Animal would be mapped to Aa in this feature, G.M.",1,0.9642857142857143,2,14,2002
would again be mapped to A.A..,1,1.0714285714285714,1,7,2002
The ag indicates whether or not the word appears in a dictionary of words which appeared more often lower-cased than capitalized in a large corpus of text.,2,1.0357142857142858,2,28,2002
Most of the features we describe are anchored on entity boundaries in the candidate segmentation.,2,1.125,2,16,2002
We will use feature templates to describe the features that we used.,2,1.1153846153846154,2,13,2002
"As an example, suppose that an entity.",2,0.7222222222222222,2,9,2002
Figure 1.,1,1,1,3,2002
The full set of entity-anchored feature templates.,2,0,2,8,2002
One of these features is generated for each entity  Applying this template to the three entities in the running example generates the three feature strings described in the previous section.,2,0.8064516129032258,3,31,2002
"For the full set of feature tem plates that are anchored around entities, see gure 1.",5,0.8611111111111112,2,18,2002
A second set of feature templates is anchored around quotation marks.,2,0.7916666666666666,1,12,2002
"In our corpus, entities (typically with long names) are often seen surrounded by quotes.",3,0.9444444444444444,3,18,2002
"For example, The Day They Shot John Lennon, the name of a band, appears in the running to be the index of any double quoto be the index of the next (matching) double quotation marks if they apto be the index of the last word beginning with a lower case letter, upper case letter, or digit within the quotation marks.",3,0.8602941176470589,3,68,2002
The rst set of feature templates tracks  example.,2,0.8333333333333334,1,9,2002
"At this point, we have fully described the representation used as input to the reranking algorithms.",2,0.8055555555555556,2,18,2002
The maximum-entropy tagger gives 20 proposed segmentations for each input sentence.,2,0.7083333333333334,2,12,2002
This section introduces notation for the reranking task.,1,0.8333333333333334,1,9,2002
The framework is derived by the transformation from ranking problems to a margin-based classication problem in (Freund et al.,2,0.8333333333333334,2,21,2002
1998).,1,0,1,3,2002
It is also related to the Markov Random Field methods for parsing suggested in (Johnson et al.,2,1.0526315789473684,2,19,2002
"1999), and the boosting methods for parsing in (Collins 2000).",2,0,2,15,2002
We consider the following set-up.,1,0.75,2,6,2002
correct sequence of tags for that sentence.,2,0,1,8,2002
pairs.,0,0,0,2,2002
In tagging we would have training examples  outputs from a maximum entropy tagger are used as the set of candidates.,2,1.0714285714285714,2,21,2002
The features could be arbitrary functions of the candidates; our hope is to include features which help in discriminating good candidates from bad ones.,2,1.0192307692307692,2,26,2002
"This function assigns a real-valued number to a canIt will be taken to be a measure of the plausibility of a candidate, higher scores meaning higher plausibility.",2,0.9827586206896551,2,29,2002
The rst algorithm we consider is the boosting algorithm for ranking described in (Collins 2000).,2,1.0,2,18,2002
The algorithm is a modication of the method in (Freund et al.,2,0.9642857142857143,2,14,2002
1998).,1,0,1,3,2002
"Intuitively, it is useful to note that this loss function is an upper bound on the number of ranking errors, a ranking error being a case where an incorrect candidate gets a higher value than a correct candidate.",2,0.8902439024390244,2,41,2002
Figure 2 shows an algorithm which implements this greedy procedure.,2,1.0,2,11,2002
"See (Collins 2000) for a full description of the method, including justication that the algorithm does in fact implement the update in Eq.",2,1.037037037037037,3,27,2002
1 at each iteration.,1,0,1,5,2002
"Figure 3 shows the training phase of the perceptron algorithm, originally introduced in (Rosenblatt 1958).",2,0.7105263157894737,3,19,2002
"The algorithm maintains a parameter vector , which is initially set to be all zeros.",2,1.03125,2,16,2002
"In this case the update is very simple, involving adding the difference of the offending examples representations in the gure).",3,0.9782608695652174,3,23,2002
"See (Cristianini and Shawe-Taylor 2000) chapter 2 for discussion of the perceptron algorithm, and theory justifying this method for setting the parameters.",4,0.8846153846153846,4,26,2002
"(Freund & Schapire 1999) describe a renement of the perceptron, the voted perceptron.",4,0,2,17,2002
The training phase is identical to that in gure 3.,2,0.8636363636363636,1,11,2002
"Note, how are stored.",1,1.3333333333333333,1,6,2002
Thus the training phase can be thought different parameter settings.,2,0.6818181818181818,2,11,2002
We applied the voted perceptron and boosting algorithms to the data described in section 2.3.,2,0,2,16,2002
Only features occurring on 5 or more distinct training sentences were included in the model.,3,0.75,2,16,2002
"This resulted 5Note that, for reasons of explication, the decoding algorithm we present is less efcient than necessary.",3,1.0714285714285714,4,21,2002
"The two methods were trained on the training portion (41,992 sentences) of the training set.",2,0.6944444444444444,2,18,2002
We used the development set to pick the best values for tunable parameters in each algorithm.,2,0.9411764705882353,2,17,2002
"For boosting, the main parameter to pick is .",3,1.05,2,10,2002
"We ran the algorithm for a total of 300,000 rounds, and found that the optimal value for F-measure on the development set occurred after 83,233 rounds.",3,0.8571428571428571,4,28,2002
Figure 5 shows the results for the three methods on the test set.,2,0.6785714285714286,2,14,2002
Both of the reranking algorithms show signicant improvements over the baseline.,2,0.875,2,12,2002
"a 15.6% relative reduction in error for boosting, and a 17.7% relative error reduction for the voted perceptron.",4,0.7045454545454546,3,22,2002
"In our experiments we found the voted perceptron algorithm to be considerably more efcient in training, at some cost in computation on test examples.",2,0.8076923076923077,2,26,2002
"Another attractive property of the voted perceptron is that it can be used with kernels, for example the kernels over parse trees described in (Collins and Duffy 2001; Collins and Duffy 2002).",3,0.9459459459459459,3,37,2002
"(Collins and Duffy 2002) describe the voted perceptron applied to the named-entity data in this paper, but using kernel-based features rather than the explicit features described in this paper.",4,0.6818181818181818,3,33,2002
"See (Collins 2002) for additional work using perceptron algorithms to train tagging models, and a more thorough description of the theory underlying the perceptron algorithm applied to ranking problems.",3,0.803030303030303,3,33,2002
"A question regarding the approaches in this paper is whether the features we have described could be incorporated in a maximum-entropy tagger, giving similar improvements in accuracy.",2,1.0344827586206897,3,29,2002
This section discusses why this is unlikely to be the case.,1,1.2083333333333333,2,12,2002
The problem described here is closely related to the label bias problem described in (Lafferty et al.,3,0.9736842105263158,2,19,2002
2001).,1,0,1,3,2002
One straightforward way to incorporate global features into the maximum-entropy model would be  to introduce new features whether the tagging decision which indicated in the history   cre ates a particular global feature.,2,0.8333333333333334,3,33,2002
"The decision which effectively created the entity University for was the decision to tag for as C, and this has already been made.",4,0.92,3,25,2002
The independence assumptions in maximum-entropy taggers of this form often lead points of local ambiguity (in this example the tag for the word for) to create globally implausible structures with unreasonably high scores.,3,1.0,2,36,2002
See (Collins 1999) section 8.4.2 for a discussion of this problem in the context of parsing.,2,1.105263157894737,2,19,2002
Acknowledgements Many thanks to Jack Minisi for annotating the named-entity data used in the exper iments.,2,0.8823529411764706,2,17,2002
"Thanks also to Nigel Duffy, Rob Schapire and Yoram Singer for several useful discussions.",3,0,3,16,2002
