sentence,yngve_score,frazier_score,dependency_distance_score,count,year
 PARAMETER ESTIMATION FOR STATISTICAL PARSING MODELS.,1.8571428571428572,0.5714285714285714,1.75,7,2005
THEORY AND PRACTICE OF DISTRIBUTION-FREE METHODS  A fundamental problem in statistical parsing is the choice of criteria and algorithms used to estimate the parameters in a model.,2.607142857142857,0.9642857142857143,2.129032258064516,28,2005
The predominant approach in computational linguistics has been to use a parametric model with some variant of maximum-likelihood estimation.,2.5,0.85,2.3636363636363638,20,2005
The assumptions under which maximum-likelihood estimation is justied are arguably quite strong.,2.6153846153846154,1.0,2.6,13,2005
"This paper discusses the statistical theory underlying various parameter-estimation methods, and gives algorithms which depend on alternatives to (smoothed) maximumlikelihood estimation.",3.68,0.92,3.5925925925925926,25,2005
We rst give an overview of results from statistical learning theory.,2.25,0.875,1.9166666666666667,12,2005
"We then show how important concepts from the classication literature  specically, generalization results based on margins on training data  can be derived for parsing models.",2.740740740740741,1.0925925925925926,4.0,27,2005
"Finally, we describe parameter estimation algorithms which are motivated by these generalization bounds.",2.2666666666666666,0.9333333333333333,2.3333333333333335,15,2005
A fundamental problem in statistical parsing is the choice of criteria and algorithms used to estimate the parameters in a model.,2.5,0.8636363636363636,2.3181818181818183,22,2005
"The predominant approach in computational linguistics has been to use a parametric model with maximum-likelihood estimation, usually with some method for smoothing parameter estimates to deal with sparse data problems.",2.6875,0.875,3.1944444444444446,32,2005
"Methods falling into this category include Probabilistic Context-Free Grammars and Hidden Markov Models, Maximum Entropy models for tagging and parsing, and recent work on Markov Random Fields.",4.666666666666667,0.6166666666666667,3.21875,30,2005
"This paper discusses the statistical theory underlying various parameterestimation methods, and gives algorithms which depend on alternatives to  (smoothed) maximum-likelihood estimation.",3.68,0.92,3.5714285714285716,25,2005
"The assumptions under which maximum-likelihood estimation is justied are arguably quite strong  in particular, an assumption is made that the structure of the statistical process generating the data is known (for example, maximumlikelihood estimation for PCFGs is justied providing that the data was actually generated by a PCFG).",3.7358490566037736,0.9716981132075472,3.9310344827586206,53,2005
"In contrast, work in computational learning theory has concentrated on models with the weaker assumption that training and test examples are generated from the same distribution, but that the form of the distribution is unknown.",3.5789473684210527,0.8289473684210527,3.9473684210526314,38,2005
in this sense the results hold across all distributions and are called distribution-free.,2.7142857142857144,0.9285714285714286,2.7777777777777777,14,2005
"The result of this work  which goes back to results in statistical learning theory by Vapnik (1998) and colleagues, and to work within Valiants PAC model of learning (Valiant, 1984)  has been the development of algorithms and theory which provide radical alternatives to parametric maximum-likelihood methods.",4.264150943396227,0.9622641509433962,4.0344827586206895,53,2005
"These algorithms are appealing in both theoretical terms, and in their impressive results in many experimental studies.",3.210526315789474,0.6052631578947368,3.0,19,2005
"In the rst part of this paper (sections 2 and 3) we describe linear models for parsing, and give an example of how the usual maximum-likelihood estimates for PCFGs can be sub-optimal.",4.166666666666667,0.8055555555555556,3.425,36,2005
"Sections 4, 5 and 6 describe the basic framework under which we will analyse parameter estimation methods.",2.8947368421052633,0.8947368421052632,3.0526315789473686,19,2005
"This is essentially the framework advocated by several books on learning theory (see Devroye et al., 1996; Vapnik, 1998; Cristianini and Shawe-Taylor, 2000).",3.6129032258064515,0.9838709677419355,4.970588235294118,31,2005
As a warm-up section 5 describes statistical theory for the simple case of nite hypothesis classes.,2.6470588235294117,0.6764705882352942,2.789473684210526,17,2005
Section 6 then goes on to the important case of hyperplane classiers.,2.230769230769231,0.8076923076923077,2.076923076923077,13,2005
"Section 7 describes how concepts from the classication literature  specically, generalization results based on margins on training data  can be derived for linear models for parsing.",2.6785714285714284,1.125,3.9,28,2005
Section 8 describes parameter estimation algorithms motivated by these results.,2.090909090909091,0.7727272727272727,1.9090909090909092,11,2005
"Section 9 gives pointers to results in the literature using the algorithms, and also discusses relationships to Markov Random Fields or maximum-entropy models (Ratnaparkhi et al., 1994; Johnson et al., 1999; Lafferty et al., 2001).",4.090909090909091,0.875,6.020408163265306,44,2005
In this section we introduce the framework for the learning problem that is studied in this paper.,2.5,0.9444444444444444,2.2777777777777777,18,2005
The task is to learn a function F .,1.8888888888888888,0.8888888888888888,1.8888888888888888,9,2005
"X  Y where X is some set of possible inputs (for example a set of possible sentences), and Y is a domain of possible outputs (for example a set of parse trees).",3.9473684210526314,0.9736842105263158,4.128205128205129,38,2005
this can be achieved by xing some arbitrary ordering on the set Y.),1.9285714285714286,1.1785714285714286,2.2142857142857144,14,2005
"Several natural language problems can be seen to be special cases of this framework, through different denitions of GEN and .",2.772727272727273,0.9090909090909091,1.9090909090909092,22,2005
In the next section we show how weighted context-free grammars are one special case.,2.6,0.8,2.764705882352941,15,2005
"Tagging problems can also be framed in this way (e.g., Collins, 2002b).",2.6470588235294117,0.8529411764705882,2.588235294117647,17,2005
in this case GEN(x) is all possible tag sequences for an input sentence x.,2.0,0.7941176470588235,2.4,17,2005
"In (Johnson et al., 1999), GEN(x) is the set of parses for a sentence x under an LFG grammar, and the representation  can track arbitrary features of these parses.",4.921052631578948,0.8026315789473685,3.5789473684210527,38,2005
"In (Ratnaparkhi et al., 1994; Collins, 2000; Collins and Duffy, 2002) GEN(x) is the top N parses from a rst pass statistical model, and the representation  tracks the log-probability assigned by the rst pass model together with arbitrary additional features of the parse trees.",5.553571428571429,0.7946428571428571,3.4827586206896552,56,2005
Walker et al.,1.25,0.75,1.0,4,2005
(2001) show how the approach can be applied to NLP generation.,2.0714285714285716,1.0714285714285714,2.5714285714285716,14,2005
"in this case x is a semantic representation, y is a surface string, and GEN is a deterministic system that maps x to a number of candidate surface realizations.",4.3125,0.71875,2.5625,32,2005
"The framework can also be considered to be a generalization of multi-class classication problems, where for all inputs x, GEN(x) is a xed set of k labels {1, 2, .",3.289473684210526,0.8947368421052632,2.5,38,2005
.,0.0,1.0,0.0,1,2005
.,0.0,1.0,0.0,1,2005
", k} (e.g., see Crammer and Singer, 2001; Elisseeff et al., 1999).",3.0,0.725,4.142857142857143,20,2005
"Say we have a context-free grammar (see (Hopcroft and Ullman, 1979) for a formal denition) G = (N, , R, S) where N is a set of non-terminal symbols,  is an alphabet, R is a set of rules of the form X  Y1Y2    Yn for n  0, X  N, Yi  (N  ), and S is a distinguished start symbol in N. The grammar denes a set of possible strings, and possible string/tree pairs, in a language.",8.272727272727273,0.8051948051948052,3.6576576576576576,77,2005
"We use GEN(x) for all x   to denote the set of possible trees  For convenience we will take the rules in R to be placed in some arbitrary ordering r1, .",2.8857142857142857,1.1,2.5833333333333335,35,2005
.,0.0,1.0,0.0,1,2005
.,0.0,1.0,0.0,1,2005
", rn.",1.0,0.3333333333333333,0.6666666666666666,3,2005
"A weighted grammar G = (N, , R, S, ) also includes a parameter vector   n which assigns a weight to each rule in R. the i-th component of  is the weight of rule ri.",4.425,0.825,3.8297872340425534,40,2005
"Given a sentence x and a tree y spanning the sentence, we assume a function (x, y) which tracks the counts of the rules in (x, y).",3.7142857142857144,0.8571428571428571,3.235294117647059,35,2005
"Specically, the i-th component of (x, y) is the number of times rule ri is seen in (x, y).",2.888888888888889,1.0,3.5714285714285716,27,2005
"Under these denitions, the weighted contextfree grammar denes a function h from sentences to trees.",2.9411764705882355,0.6764705882352942,2.5294117647058822,17,2005
"Finding h(x), the parse with the largest weight, can be achieved in polynomial time using the CKY parsing algorithm (in spite of a possibly exponential number of members of GEN(x)), assuming that the weighted CFG can be converted to an equivalent weighted CFG in Chomsky Normal Form.",3.983050847457627,0.7966101694915254,4.109090909090909,59,2005
"In this paper we consider the structure of the grammar to be xed, the learning problem being reduced to setting the values of the parameters .",2.3333333333333335,0.9444444444444444,2.5185185185185186,27,2005
A basic question is as follows.,2.0,1.0,1.4285714285714286,7,2005
"given a training sample of sentence/tree pairs {(x1, y1), .",3.4,0.6333333333333333,3.8947368421052633,15,2005
.,0.0,1.0,0.0,1,2005
.,0.0,1.0,0.0,1,2005
", (xm, ym)}, what criterion should be used to set the weights in the grammar?",3.0,1.0476190476190477,3.0952380952380953,21,2005
"A very common method  that of Probabilistic Context-Free Grammars (PCFGs)  uses the parameters to dene a distribution P (x, y|) over possible sentence/tree pairs in the grammar.",3.6470588235294117,0.7941176470588235,2.5641025641025643,34,2005
Maximum likelihood estimation is used to set the weights.,2.0,0.9,1.7,10,2005
"We will consider the assumptions under which this method is justied, and argue that these assumptions are likely to be too strong.",2.2916666666666665,1.0833333333333333,3.5833333333333335,24,2005
We will also give an example to show how PCFGs can be badly mislead when the assumptions are violated.,2.05,1.25,2.9,20,2005
"As an alternative we will propose distribution-free methods for estimating the weights, which are justied under much weaker assumptions, and can give quite different estimates of the parameter values in some situations.",3.057142857142857,0.8714285714285714,3.972972972972973,35,2005
"We would like to generalize weighted context-free grammars by allowing the representation (x, y) to be essentially any feature-vector representation of the tree.",2.7037037037037037,0.9629629629629629,3.1666666666666665,27,2005
"There is still a grammar G, dening a set of candidates GEN(x) for each sentence.",2.95,0.75,3.388888888888889,20,2005
The parameters of the parser are a vector .,2.2222222222222223,0.8333333333333334,1.6666666666666667,9,2005
The parsers output is dened in the same way as equation (1.1).,2.2666666666666666,0.8333333333333334,2.0,15,2005
The important thing in this generalization is that the representation  is now not necessarily directly tied to the productions in the grammar.,2.5652173913043477,0.8260869565217391,3.1666666666666665,23,2005
"This is essentially the approach advocated by (Ratnaparkhi et al., 1994; Abney, 1997; Johnson et al., 1999), although the criteria that we will propose for setting the parameters  are quite different.",4.45,1.0,5.744186046511628,40,2005
"While supercially this might appear to be a minor change, it introduces two major challenges.",3.764705882352941,0.9117647058823529,2.3529411764705883,17,2005
The rst problem is how to set the parameter values under these general representations.,2.3333333333333335,0.8,2.466666666666667,15,2005
"The PCFG method described in the next  section, which results in simple relative frequency estimators of rule weights, is not applicable to more general representations.",4.178571428571429,0.75,2.7586206896551726,28,2005
"A generalization of PCFGs, Markov Random Fields (MRFs), has been proposed by several authors (Ratnaparkhi et al., 1994; Abney, 1997; Johnson et al., 1999; Della Pietra et al., 1997).",4.627906976744186,0.8488372093023255,3.760869565217391,43,2005
"In this paper we give several alternatives to MRFs, and we describe the theory and assumptions which underly various models.",3.772727272727273,0.8181818181818182,2.727272727272727,22,2005
The second challenge is that now that the parameters are not tied to rules in the grammar the CKY algorithm is not applicable  in the worst case we may have to enumerate all members of GEN(x) explicitly to nd the highestscoring tree.,3.152173913043478,0.9565217391304348,4.066666666666666,46,2005
One practical solution is to dene the grammar G as a rst pass statistical parser which allows dynamic programming to enumerate its top N candidates.,2.6923076923076925,0.8076923076923077,3.0,26,2005
A second pass uses the more complex representation  to choose the best of these parses.,2.4375,0.8125,2.2941176470588234,16,2005
"This is the approach used in several papers (e.g., Ratnaparkhi et al., 1994; Collins, 2000; Collins and Duffy, 2002).",4.607142857142857,0.9107142857142857,5.0344827586206895,28,2005
This section reviews the basic theory underlying Probabilistic Context-Free Grammars (PCFGs).,2.357142857142857,0.75,2.875,14,2005
"Say we have a context-free grammar G = (N, , R, S) as dened in section 2.1.",5.409090909090909,1.0227272727272727,2.44,22,2005
"We will use T to denote the set of all trees generated by G. Now say we assign a weight p(r) in the range 0 to 1 to each rule r in R. Assuming some arbitrary ordering r1, .",2.0,1.1333333333333333,3.317073170731707,15,2005
.,0.0,1.0,0.0,1,2005
.,0.0,1.0,0.0,1,2005
", rn of the n rules in R, we use  to denote a vector of parameters,  = hlog p(r1), log p(r2), .",3.96875,0.765625,4.366666666666666,32,2005
.,0.0,1.0,0.0,1,2005
.,0.0,1.0,0.0,1,2005
", log p(rn)i.",2.142857142857143,0.7142857142857143,1.0,7,2005
"If c(T, r) is the number of times rule r is seen in a tree T , then the probability of a tree T can be written as.",3.4545454545454546,0.9848484848484849,3.8484848484848486,33,2005
We can now study how to train the grammar from a training sample of trees.,2.1875,1.0625,2.375,16,2005
"Say there is a training set of trees {T1, T2, .",2.7142857142857144,1.0,3.142857142857143,14,2005
.,0.0,1.0,0.0,1,2005
.,0.0,1.0,0.0,1,2005
", Tm}.",1.0,0.5,0.5,4,2005
The log-likelihood  of the training set given parameters  is L() = Pj log P (Tj|).,2.9523809523809526,0.7619047619047619,2.260869565217391,21,2005
"The  maximum-likelihood estimates are to take  = arg max L(), where  is the set of allowable parameter settings (i.e., the parameter settings which obey the constraints in Booth and Thompson, 1973).",3.3846153846153846,0.8205128205128205,3.046511627906977,39,2005
So under what circumstances is maximum-likelihood estimation justied?,1.8888888888888888,1.2777777777777777,2.3636363636363638,9,2005
"Say there is a true set of weights , which dene an underlying distribution P (T |), and that the training set is a sample of size m from this distribution.",3.942857142857143,0.9428571428571428,4.5,35,2005
"Then it can be shown that as m increases to innity, then with probability 1 the parameter estimates  converge to values which give the same distribution over trees as the true parameter values .",3.142857142857143,1.1,3.6052631578947367,35,2005
"To illustrate the deciencies of PCFGs, we give a simple example.",3.3076923076923075,0.6923076923076923,2.0,13,2005
"Say we have a random process which generates just 3 trees, with probabilities {p1, p2, p3}, as shown in gure 1.1a.",2.392857142857143,1.0,3.7142857142857144,28,2005
The training sample will consist of a set of trees drawn from this distribution.,2.066666666666667,0.9,1.8,15,2005
"A test sample will be generated from the same distribution, but in this case the trees will be hidden, and only the surface strings will be seen (i.e., haaaai, haaai and hai with probabilities p1, p2, p3 respectively).",4.787234042553192,0.6382978723404256,3.382978723404255,47,2005
We would like to learn a weighted CFG with as small error as possible on a randomly drawn test sample.,2.4761904761904763,0.8571428571428571,3.1904761904761907,21,2005
"As the size of the training sample goes to innity, the relative frequencies of trees {T1, T2, T3} in the training sample will converge to {p1, p2, p3}.",4.157894736842105,0.7105263157894737,3.5277777777777777,38,2005
This makes it easy to calculate the rule weights that maximum-likelihood estimation converges to  see gure 1.1b.,1.2222222222222223,1.25,3.2857142857142856,18,2005
We will call the PCFG with these asymptotic weights the asymptotic PCFG.,2.230769230769231,0.7307692307692307,2.8461538461538463,13,2005
"Notice that the grammar generates trees never seen in training data, shown in gure 1.1c.",1.2941176470588236,0.9705882352941176,2.6470588235294117,17,2005
The grammar is ambiguous for strings haaaai (both T1 and T4 are possible) and haaai (T2 and T5 are possible).,3.52,0.98,3.12,25,2005
"In fact, under certain conditions T4 and T5 will get higher probabilities under the asymptotic PCFG than T1 and T2, and both strings haaaai and haaai will be mis-parsed.",4.71875,0.578125,3.5,32,2005
"Figure 1.1d shows the distribution of the asymptotic PCFG over the 8 trees when p1 = 0.2, p2 = 0.1 and p3 = 0.7.",4.666666666666667,1.037037037037037,3.576923076923077,27,2005
"In this case both ambiguous strings are mis-parsed by the asymptotic PCFG, resulting in an expected error rate of (p1 + p2) = 30% on newly drawn test examples.",3.3529411764705883,0.7352941176470589,4.138888888888889,34,2005
Figure 1.1d.,0.6666666666666666,1.0,1.0,3,2005
"The probabilities assigned to the trees as the training size goes to innity, for p1 = 0.2, p2 = 0.1, p3 = 0.7.",3.185185185185185,0.9814814814814815,3.1481481481481484,27,2005
"Notice that P (T4) > P (T1), and P (T5) > P (T2), so the induced PCFG will incorrectly map haaaai to T4 and haaai to T2.",4.868421052631579,0.9210526315789473,2.3421052631578947,38,2005
"This is a striking failure of the PCFG when we consider that it is easy to derive weights on the grammar rules which parse both training and test examples with no errors.2 On this example there exist weighted grammars which make no errors, but the maximum likelihood estimation method will fail to nd these weights, even with unlimited amounts of training data.",4.893939393939394,0.9318181818181818,2.476923076923077,66,2005
The next 4 sections of this chapter describe theoretical results underlying the parameter estimation algorithms in section 8.,2.9473684210526314,0.6842105263157895,2.210526315789474,19,2005
In sections 4.1 to 4.3 we describe the basic framework under which we will analyse the various learning approaches.,2.75,0.975,2.7,20,2005
"In section 5 we describe analysis for a simple case, nite hypothesis classes, which will be useful for illustrating ideas and intuition underlying the methods.",2.9285714285714284,0.9285714285714286,2.8214285714285716,28,2005
In section 6 we describe analysis of hyperplane classiers.,2.2,0.95,1.7,10,2005
In section 7 we describe how the results for hyperplane classiers can be generalized to apply to the linear models introduced in section 2.,2.4,1.02,2.88,25,2005
This section introduces a general framework for supervised learning problems.,2.1818181818181817,0.6818181818181818,2.090909090909091,11,2005
"There are several books (Devroye et al., 1996; Vapnik, 1998; Cristianini and Shawe-Taylor, 2000) which cover the material in detail.",4.285714285714286,0.9642857142857143,4.419354838709677,28,2005
"We will use this framework to analyze both parametric methods (PCFGs, for example), and the distributionfree methods proposed in this paper.",3.9615384615384617,0.8846153846153846,4.678571428571429,26,2005
We assume the following.,1.4,0.9,1.4,5,2005
An input domain X and an output domain Y.,2.2222222222222223,0.6666666666666666,2.0,9,2005
The task will be to learn a function mapping each element of X to an element of Y.,1.6666666666666667,1.0833333333333333,1.7222222222222223,18,2005
"In parsing, X is a set of possible sentences and Y is a set of possible trees.",3.1052631578947367,0.8157894736842105,2.4210526315789473,19,2005
"There is some underlying probability distribution D(x, y) over X  Y.",2.466666666666667,0.8333333333333334,2.0714285714285716,15,2005
The distribution is used to generate both training and test examples.,2.0833333333333335,0.8333333333333334,1.9166666666666667,12,2005
"It is an unknown distribution, but it is constant across training and test examples  both training and test examples are drawn independently, identically distributed from D(x, y).",3.588235294117647,0.7794117647058824,3.3333333333333335,34,2005
"There is a loss function L(y, y) which measures the cost of proposing an output y when the true output is y.",1.9615384615384615,1.0384615384615385,2.5,26,2005
"A commonly used cost is the 0-1 loss L(y, y) = 0 if y = y, and L(y, y) = 1 otherwise.",4.71875,0.796875,2.5,32,2005
We will concentrate on this loss function in this paper.,2.0,0.8636363636363636,2.0,11,2005
Under 0-1 loss this is the expected proportion of errors that the hypothesis makes on examples drawn from the distribution D. We would like to learn a function whose expected loss is as low as possible.,2.5454545454545454,0.9090909090909091,2.769230769230769,22,2005
Er(h) is a measure of how successful a function h is.,2.4,1.0,2.4615384615384617,15,2005
"Unfortunately, because we do not have direct access to the distribution D, we cannot explicitly calculate the expected loss of a hypothesis.",3.8076923076923075,0.7307692307692307,3.8076923076923075,26,2005
"The training set is a sample of m pairs {(x1, y1), .",3.176470588235294,0.6176470588235294,2.823529411764706,17,2005
.,0.0,1.0,0.0,1,2005
.,0.0,1.0,0.0,1,2005
", (xm, ym)} drawn from the distribution D. This is the only information we have about D.  Finally, a useful concept is the Bayes Optimal hypothesis, which we will denote as hB.",3.0,0.8461538461538461,2.1025641025641026,13,2005
"It is dened as hB(x) = arg maxyY D(x, y).",2.611111111111111,0.9166666666666666,1.7142857142857142,18,2005
The Bayes optimal hypothesis simply outputs the most likely y under the distribution D for each input x.,1.9444444444444444,0.75,2.3333333333333335,18,2005
It is easy to prove that this function minimizes the expected loss Er(h) over the space of all possible functions  the Bayes optimal hypothesis cannot be improved upon.,2.727272727272727,0.8787878787878788,4.34375,33,2005
"Unfortunately, in general we do not know D(x, y), so the Bayes optimal hypothesis, while useful as a theoretical construct, cannot be obtained directly in practice.",4.055555555555555,0.7361111111111112,4.470588235294118,36,2005
"Given that the only access to the distribution D(x, y) is indirect, through a training sample of nite size m, the learning problem is to nd a hypothesis whose expected risk is low, using only the training sample as evidence.",4.645833333333333,0.8229166666666666,3.608695652173913,48,2005
"Parametric models attempt to solve the supervised learning problem by explicitly modeling either the joint distribution D(x, y) or the conditional distributions D(y|x) for all x.",3.3142857142857145,0.7428571428571429,2.6551724137931036,35,2005
"In the joint distribution case, there is a parameterized probability distribution P (x, y|).",3.6,0.525,2.9473684210526314,20,2005
As the parameter values  are varied the distribution will also vary.,3.0833333333333335,0.9166666666666666,1.7692307692307692,12,2005
"The parameter space  is a set of possible parameter values for which  P (x, y|) is a well-dened distribution (i.e., for which Px,y P (x, y|) = 1).",3.6904761904761907,0.8214285714285714,4.7272727272727275,42,2005
"A crucial assumption in parametric approaches is that there is some    such that D(x, y) = P (x, y|).",2.8214285714285716,0.9107142857142857,2.7857142857142856,28,2005
"Because of this, if we consider the function h(x) = arg maxyY P (x, y| ), then in the limit h(x) will converge to the Bayes optimal function hB(x).",4.75,0.7272727272727273,5.0,44,2005
"So under the assumption that D(x, y) = P (x, y|) for some   , and with innite amounts of training data, the maximumlikelihood method is provably optimal.",6.675675675675675,0.7837837837837838,4.837837837837838,37,2005
Methods which model the conditional distribution D(y|x) are similar.,3.3333333333333335,1.0666666666666667,2.727272727272727,15,2005
"The parameters now dene a conditional distribution P (y|x, ).",3.4,0.7,2.9285714285714284,15,2005
"The assumption is that there is some  such that x, D(y|x) = P (y|x, ).",3.8846153846153846,0.8461538461538461,1.9545454545454546,26,2005
"Maximumlikelihood estimates can be dened in a similar way, and in this case the function h(x) = arg maxyY P (y|x, ) will converge to the Bayes optimal function hB(x) as the sample size goes to innity.",4.551020408163265,0.7346938775510204,3.4545454545454546,49,2005
"From the arguments in the previous section, parametric methods are optimal  provided that two assumptions hold.",3.2777777777777777,0.8333333333333334,2.473684210526316,18,2005
1 The distribution generating the data is in the class of distributions being  considered.,2.3333333333333335,1.1,2.3125,15,2005
"2 The training set is large enough for the distribution dened by the maximum likelihood estimates to converge to the true distribution D(x, y) (in general the guarantees of ML estimation are asymptotic, holding only in the limit as the training data size goes to innity).",3.4814814814814814,0.9259259259259259,3.962962962962963,54,2005
This paper proposes alternatives to maximum-likelihood methods which give theoretical guarantees without making either of these assumptions.,2.111111111111111,1.1388888888888888,2.2,18,2005
"There is no assumption that the distribution generating the data comes from some predened class  the only assumption is that the same, unknown distribution generates both training and test examples.",2.71875,0.8125,4.121212121212121,32,2005
"The methods also provide bounds suggesting how many training samples are required for learning, dealing with the case where there is only a nite amount of training data.",3.2,1.0166666666666666,2.6,30,2005
A crucial idea in distribution-free learning is that of a hypothesis space.,2.4615384615384617,0.7307692307692307,2.1333333333333333,13,2005
"This is a set of functions under consideration, each member of the set being a function h .",2.1052631578947367,1.0,3.1578947368421053,19,2005
X  Y.,0.5,1.5,1.0,2,2005
"For example, in weighted context-free grammars the hypothesis space is.",3.4166666666666665,0.625,2.7142857142857144,12,2005
"So each possible parameter setting denes a different function from sentences to trees, and H is the innite set of all such functions as  ranges over the parameter space n.",2.870967741935484,0.7096774193548387,3.3636363636363638,31,2005
Learning is then usually framed as the task of choosing a good function in H on the basis of a training sample as evidence.,2.88,0.96,2.7777777777777777,25,2005
This strategy is called Empirical Risk Minimization (ERM) by Vapnik (1998).,2.5625,0.84375,2.7222222222222223,16,2005
Two questions which arise are.,2.1666666666666665,1.5,1.3333333333333333,6,2005
"In the limit, as the training size goes to innity, does the error of the ERM method Er(h) approach the error of the best function in the set, Er(h), regardless of the underlying distribution D(x, y)?",3.843137254901961,0.7941176470588235,4.0,51,2005
"In other words, is this method of choosing a hypothesis always consistent?",2.857142857142857,0.9285714285714286,2.4285714285714284,14,2005
The answer to this depends on the nature of the hypothesis space H. For nite hypothesis spaces the ERM method is always consistent.,2.8333333333333335,0.875,1.8333333333333333,24,2005
"For many innite hypothesis spaces, such as the hyperplane classiers described in section 6 of this paper, the method is also consistent.",4.72,0.7,2.48,25,2005
"However, some innite hypothesis spaces can lead to the method being inconsistent  specically, if a measure called the Vapnik-Chervonenkis (VC) dimension (Vapnik and Chervonenkis, 1971) of H is innite, the ERM method may be inconsistent.",5.954545454545454,0.9318181818181818,5.1063829787234045,44,2005
"Intuitively, the VC dimension can be thought of as a measure of the complexity of an innite set of hypotheses.",2.409090909090909,0.7954545454545454,2.5,22,2005
"If the method is consistent, how quickly does Er(h) converge to Er(h)?",2.9,1.15,2.125,20,2005
"In other words, how much training data is needed to have a good chance of getting close to the best function in H?",2.64,1.0,2.44,25,2005
We will see in the next section that the convergence rate depends on various measures of the size of the hypothesis space.,2.347826086956522,0.8260869565217391,3.0,23,2005
"For nite sets, the rate of convergence depends directly upon the size of H. For innite sets, several measures have been proposed  we will concentrate on rates of convergence based on a concept called the margin of a hypothesis on training examples.",2.7333333333333334,1.0666666666666667,2.130434782608696,45,2005
This section gives results and analysis for situations where the hypothesis space H is a nite set.,2.5,0.7777777777777778,2.7777777777777777,18,2005
This is in some ways an unrealistically simple situation  many hypothesis spaces used in practice are innite sets  but we give the results and proofs because they can be useful in developing intuition for the nature of convergence bounds.,3.775,0.8625,3.7142857142857144,40,2005
In the following sections we consider innite hypothesis spaces such as weighted context-free grammars.,2.6666666666666665,0.6333333333333333,2.6470588235294117,15,2005
A couple of basic results from probability theory will be very useful.,2.4615384615384617,0.8846153846153846,1.8461538461538463,13,2005
The rst results are the Chernoff bounds.,2.125,0.5625,1.75,8,2005
"Consider a binary random variable X (such as the result of a coin toss) which has probability p of being 1, and (1  p) of being 0.",3.6666666666666665,1.0151515151515151,4.5,33,2005
"Now consider a sample of size m, {x1, x2, .",2.7857142857142856,0.8214285714285714,2.857142857142857,14,2005
.,0.0,1.0,0.0,1,2005
.,0.0,1.0,0.0,1,2005
", xm} drawn from this process.",2.375,0.75,1.75,8,2005
Dene the relative frequency of xi = 1 (the coin coming  up heads) in this sample to be p = Pi xi/m.,2.8,1.02,3.5357142857142856,25,2005
"The relative frequency p is a  very natural estimate of the underlying probability p, and by the law of large numbers p will converge to p as the sample size m goes to innity.",4.027777777777778,0.6527777777777778,3.5405405405405403,36,2005
"Chernoff bounds give results concerning how quickly p converges to p. Thus Chernoff bounds go a step further than the law of large numbers, which is an asymptotic result (a result concerning what happens as the sample size goes to innity).",3.2,1.011111111111111,2.8,45,2005
The bounds are.,1.5,0.875,0.75,4,2005
Theorem 1 (Chernoff Bounds).,2.0,0.5714285714285714,1.5714285714285714,7,2005
"For all p  [0, 1],  > 0, with the probability P being taken over the distribution of training samples of size m generated with underlying parameter p.  The rst bound states that for all values of p, and for all values of , if we repeatedly draw training samples of size m of a binary variable with underlying probability p, the relative proportion of training samples for which the value (p p) exceeds  is at most3 e2m2.",3.5454545454545454,0.8636363636363636,3.902173913043478,33,2005
"It is always possible for p to diverge substantially from p  it is possible to draw an extremely unrepresentative training sample, such as a sample of all heads when p = 0.7, for example  but as the sample size is increased the chances of us being this unlucky become increasingly unlikely.",4.592592592592593,1.0833333333333333,3.8035714285714284,54,2005
A second useful result is the Union Bound.,2.4444444444444446,0.7222222222222222,1.8888888888888888,9,2005
Here we use the notation P [AB] to mean the probability of A or B occurring.,2.526315789473684,0.9473684210526315,2.9473684210526314,19,2005
The Union Bound follows directly from the axioms of probability theory.,2.1666666666666665,0.9583333333333334,1.8333333333333333,12,2005
"For example, if n = 2, then P [A1  A2] = P [A1] + P [A2]  P [A1A2]  P [A1] + P [A2], where P [A1A2] means the probability of both A1 and A2 occurring.",5.452830188679245,0.5754716981132075,3.5714285714285716,53,2005
The more general result for all n follows by induction on n.  We are now in a position to apply these results to learning problems.,2.6923076923076925,0.8846153846153846,1.8148148148148149,13,2005
"First, consider just a single member of H, a function h. Say we draw a training sample {(x1, y1), .",3.925925925925926,0.7407407407407407,2.740740740740741,27,2005
.,0.0,1.0,0.0,1,2005
.,0.0,1.0,0.0,1,2005
", (xm, ym)} from some unknown distribution D(x, y).",3.2222222222222223,0.8333333333333334,3.5,18,2005
"We can calculate the relative frequency of errors of h on this sample,  So for any single member of H, the Chernoff bound describes how its observed error on the training set is related to its true probability of error.",3.883720930232558,0.8604651162790697,3.6363636363636362,43,2005
"Now consider the entire set of hypotheses H. Say we assign an arbitrary ordering to the n = |H| hypotheses, so that H = {h1, h2, .",3.606060606060606,0.9848484848484849,5.032258064516129,33,2005
.,0.0,1.0,0.0,1,2005
.,0.0,1.0,0.0,1,2005
", hn}.",1.0,0.5,0.5,4,2005
Consider the probability of any one of the hypotheses hi having its estimated loss Er(hi) diverge by more than  from its expected loss Er(hi).,3.129032258064516,0.9838709677419355,4.714285714285714,31,2005
"Thus for all hypotheses h in the set H, Er(h) converges to Er(h) as the sample size m goes to innity.",3.5517241379310347,0.8275862068965517,3.48,29,2005
"This result is known as a Uniform Convergence Result, in that it describes how a whole set of empirical error rates converge to their respective expected errors.",2.8620689655172415,0.7758620689655172,3.586206896551724,29,2005
Note that this result holds for the hypothesis with minimum error on the training sample.,2.375,0.875,2.4375,16,2005
"It can be shown that this implies that the ERM method for nite hypothesis spaces  choosing the hypothesis h which has minimum error on the training sample  is consistent, in that in the limit as m  , the error of h converges to the error of the minimum error hypothesis.",4.865384615384615,0.9134615384615384,3.482142857142857,52,2005
Another important result is how the rate of convergence depends on the size of the hypothesis space.,2.3333333333333335,0.9444444444444444,2.5555555555555554,18,2005
"Qualitatively, the bound implies that to avoid overtraining the number of training samples should scale with log |H|.",2.4545454545454546,1.0,3.95,22,2005
"Ideally, we would like a learning method to have expected error that is close Now consider the ERM  to the loss of the bayes-optimal hypothesis hB.",2.392857142857143,1.0,3.0,28,2005
method.,0.5,0.5,0.5,2,2005
Breaking the error down in this way suggests that there are two components to the difference from the optimal loss Er(hB).,2.64,0.82,3.0434782608695654,25,2005
"The rst term captures the errors due to a nite sample size  if the hypothesis space is too large, then theorem 3 states that there is a good chance that the ERM method will pick a hypothesis that is far from the best in the hypothesis space, and the rst term will be large.",5.175438596491228,0.7631578947368421,3.9827586206896552,57,2005
"Thus the rst term indicates a pressure to keep H small, so that there is a good   chance of nding the best hypothesis in the set.",2.7857142857142856,0.8571428571428571,2.7241379310344827,28,2005
"In contrast, the second term reects a pressure to make H large, so that there is a good chance that at least one of the hypotheses is close to the Bayes optimal hypothesis.",2.9722222222222223,0.8333333333333334,3.3333333333333335,36,2005
"The two terms can be thought of as being analogues to the familiar biasvariance trade-off, the rst term being a variance term, the second being the bias.",4.733333333333333,0.6166666666666667,3.7222222222222223,30,2005
In this section we describe a method which explicitly attempts to model the trade-off between these two types of errors.,2.3333333333333335,1.119047619047619,2.4782608695652173,21,2005
"Rather than picking a single hypothesis class, Structural Risk Minimization (Vapnik, 1998) advocates picking a set of hypothesis classes H1, H2, .",3.75,0.6428571428571429,3.2142857142857144,28,2005
.,0.0,1.0,0.0,1,2005
.,0.0,1.0,0.0,1,2005
", Hs of increasing size (i.e., such that |H1| < |H2| <    < |Hs|).",3.375,0.9166666666666666,3.4285714285714284,24,2005
"The following theorem then applies (it is an extension of theorem 3, and is derived in a similar way through application of the Chernoff and Union bounds).",3.903225806451613,0.6935483870967742,3.6774193548387095,31,2005
"Theorem 4 Assume a set of nite hypothesis classes {H1, H2, .",3.3333333333333335,0.7,2.533333333333333,15,2005
.,0.0,1.0,0.0,1,2005
.,0.0,1.0,0.0,1,2005
", Hs}, and some distribution D(x, y).",3.0,0.7142857142857143,2.6666666666666665,14,2005
"For all i = 1, .",1.8571428571428572,0.8571428571428571,1.4285714285714286,7,2005
.,0.0,1.0,0.0,1,2005
.,0.0,1.0,0.0,1,2005
", s, for all hypotheses h  Hi, with probability at least 1   over the choice of training set of size m drawn from D.  This theorem is very similar to theorem 3, except that the second term in the bound now varies depending on which Hi a function h is drawn from.",3.4444444444444446,0.7777777777777778,3.433333333333333,27,2005
Note also that we pay an extra price of log(s) for our hedging over which of the hypothesis spaces the function is drawn from.,2.7857142857142856,1.0178571428571428,3.423076923076923,28,2005
The SRM principle is then as follows.,2.125,1.0,1.625,8,2005
"1 Pick a set of hypothesis classes, Hi for i = 1, .",2.533333333333333,1.0333333333333334,3.533333333333333,15,2005
.,0.0,1.0,0.0,1,2005
.,0.0,1.0,0.0,1,2005
", s, of increasing size.",2.0,1.1428571428571428,1.7142857142857142,7,2005
This must be done independently of the training data for the above bound to apply.,2.3125,1.0,2.625,16,2005
2 Choose the hypothesis h which minimizes the bound in theorem 4.,2.076923076923077,1.1538461538461537,2.230769230769231,13,2005
"Thus rather than simply choosing the hypothesis with the lowest error on the training sample, there is now a trade-off between training error and the size of the hypothesis space of which h is a member.",3.8421052631578947,0.8289473684210527,3.0,38,2005
The SRM method advocates picking a compromise between keeping the number of training errors small versus keeping the size of the hypothesis class small.,3.0,1.0,2.2,25,2005
Note that this approach has a somewhat similar avour to Bayesian approaches.,2.230769230769231,0.9230769230769231,2.6153846153846154,13,2005
The Maximum A-Posteriori (MAP) estimates in a Bayesian approach involve choosing the parameters which maximize a combination of the data likelihood and a prior over the parameter values.,2.6774193548387095,0.9354838709677419,2.8484848484848486,31,2005
The rst term is a measure of how well the parameters  t the data.,2.466666666666667,0.8666666666666667,2.375,15,2005
The second term is a prior which can be interpreted as a term which penalizes more complex parameter settings.,2.25,1.075,2.4,20,2005
The SRM approach in our example implies choosing the hypothesis that minimizes the bound in theorem 4.,2.388888888888889,1.1388888888888888,2.0555555555555554,18,2005
"The function indicating the goodness of a hypothesis h again has two terms, one measuring how well the hypothesis ts the data, the second penalizing hypotheses which are too complex.",2.9393939393939394,0.9242424242424242,3.5405405405405403,33,2005
Here complexity has a very specic meaning.,2.0,0.8125,2.125,8,2005
it is a direct measure of how quickly the training data error Er(h) converges to its true value Er(h).,3.0,0.7692307692307693,3.727272727272727,26,2005
"This section describes analysis applied for binary classiers, where the set Y = {1, +1}.",3.0,0.95,3.263157894736842,20,2005
"We consider hyperplane classiers, where a linear separator in some feature space is used to separate examples into the two classes.",2.5217391304347827,0.9347826086956522,3.260869565217391,23,2005
This section describes uniform convergence bounds for hyperplane classiers.,2.1,0.75,1.9,10,2005
Algorithms which explicitly minimize these bounds  namely the Support Vector Machine and Boosting algorithms  are described in section 8.,3.6,0.8,3.6363636363636362,20,2005
There has been a large amount of research devoted to the analysis of hyperplane classiers.,2.0625,0.96875,2.1875,16,2005
"They go back to one of the earliest learning algorithms, the Perceptron algorithm (Rosenblatt, 1958).",2.75,0.825,3.05,20,2005
They are similar to the linear models for parsing we proposed in section 2 (in fact the framework of section 2 can be viewed as a generalization of hyperplane classiers).,2.787878787878788,1.0606060606060606,2.9696969696969697,33,2005
"We will initially review some results applying to linear classiers, and then discuss how various results may be applied to linear models for parsing.",3.1923076923076925,1.0,3.1538461538461537,26,2005
"We will discuss a hypothesis space of n-dimensional hyperplane classiers,  dened as follows.",2.4,0.9333333333333333,3.4444444444444446,15,2005
Each instance x is represented as a vector (x) in n.,1.6153846153846154,0.9615384615384616,2.6153846153846154,13,2005
There is a clear geometric interpretation of this classier.,2.2,0.85,2.3,10,2005
The points (x) are in n-dimensional Euclidean space.,2.3636363636363638,0.7727272727272727,2.1666666666666665,11,2005
"The parameters , b dene a hyperplane through the   space, the hyperplane being the set of points z such that (z   + b) = 0.",2.9655172413793105,0.9655172413793104,3.272727272727273,29,2005
"This is a hyperplane with normal , at distance b/|||| from the origin, j .",3.2857142857142856,0.9761904761904762,4.117647058823529,21,2005
This hyperplane is used to classify points.,1.75,1.125,1.5,8,2005
"all points falling on one side of the hyperplane are classied as +1, points on the other side are classied as 1.",4.125,0.8333333333333334,2.24,24,2005
"It can be shown that the ERM method is consistent for hyperplanes, through a method called VC analysis (Vapnik and Chervonenkis, 1971).",2.5555555555555554,0.9629629629629629,3.0,27,2005
"We will not go into details here, but roughly speaking, the VC-dimension of a hypothesis space is a measure of its size or complexity.",3.4074074074074074,0.6296296296296297,3.103448275862069,27,2005
A set of hyperplanes in n has VC dimension of (n + 1).,2.5625,0.96875,2.0625,16,2005
For any hypothesis space with nite VC dimension the ERM method is consistent.,3.357142857142857,0.6785714285714286,2.2142857142857144,14,2005
An alternative to VC-analysis is to analyse hyperplanes through properties of margins on training examples.,2.125,1.1875,2.75,16,2005
"For any hyperplane dened by parameters (, b), for a training sample {(x1, y1), .",3.739130434782609,0.782608695652174,3.4166666666666665,23,2005
.,0.0,1.0,0.0,1,2005
.,0.0,1.0,0.0,1,2005
", (xm, ym)}, the margin on the i-th training example is dened as  The margin ,b has a simple geometric interpretation.",5.571428571428571,0.6428571428571429,3.8666666666666667,28,2005
"it is the minimum distance of any training point to the hyperplane dened by , b.",1.5625,0.90625,2.823529411764706,16,2005
The following theorem then holds.,2.5,0.75,1.1666666666666667,6,2005
Theorem 5 (Shawe-Taylor et al.,1.7142857142857142,1.0,2.0,7,2005
1998).,1.0,0.3333333333333333,1.0,3,2005
"Assume the hypothesis class H is a set of hyperplanes, and that there is some distribution D(x, y) generating examples.",3.730769230769231,0.9807692307692307,3.6666666666666665,26,2005
"Dene R to be a constant such that x, ||(x)||  R. For all  The bound is minimized for the hyperplane with maximum margin (i.e., maximum value for ,b) on the training sample.",3.8636363636363638,0.8863636363636364,2.923076923076923,44,2005
"This bound suggests that if the training data is separable, the hyperplane with maximum margin should be chosen as the hypothesis with the best bound on its expected error.",2.903225806451613,0.967741935483871,3.806451612903226,31,2005
"It can be shown that the maximum margin hyperplane is unique, and can be found efciently using algorithms described in section 8.1.",2.4166666666666665,0.9791666666666666,2.7916666666666665,24,2005
"Search for the maximummargin hyperplane is the basis of Support Vector Machines (hard-margin version; Vapnik, 1998).",2.761904761904762,0.8095238095238095,2.28,21,2005
The previous theorem does not apply when the training data cannot be classied with 0 errors by a hyperplane.,2.380952380952381,0.7619047619047619,3.0,21,2005
"There is, however, a similar theorem that can be applied in the non-separable case.",2.411764705882353,0.9705882352941176,3.0,17,2005
"First, dene L(, b, ) to be the proportion of examples on training data with margin less than  for the hyperplane h,b.",2.107142857142857,1.0178571428571428,2.892857142857143,28,2005
The following theorem can now be stated.,2.125,0.8125,1.75,8,2005
"Theorem 6 Cristianini and Shawe-Taylor, 2000, Theorem 4.19.",3.1818181818181817,0.5454545454545454,4.0,11,2005
"Assume the hypothesis class H is a set of hyperplanes, and that there is some distribution D(x, y) generating examples.",3.730769230769231,0.9807692307692307,3.6666666666666665,26,2005
"Let R be a constant such that x, ||(x)||  R. For all h,b  H, for all  > 0, with probability at least 1   over the choice of training set of size m drawn from D.  (The rst result of the form of Theorem 6 was given in (Bartlett 1998).",4.546875,0.8203125,5.112903225806452,64,2005
This was a general result for large margin classiers; the immediate corollary that implies the above theorem was given in (Anthony and Bartlett 1999).,3.2142857142857144,0.7321428571428571,3.2142857142857144,28,2005
Note that Zhang (2002) proves a related theorem where the log2 m factor is replaced by log m. Note also that the square-root in the second term of theorem 6 means that this bound is in general a looser bound than the bound in theorem 5.,2.8979591836734695,1.0612244897959184,3.235294117647059,49,2005
"This is one cost of moving to the case where some training samples are misclassied, or where some training samples are classied with a small margin.)",3.793103448275862,0.9310344827586207,3.7586206896551726,29,2005
"This result is important in cases where a large proportion of training samples can be classied with relatively large margin, but a relatively small number of outliers make the problem inseparable, or force a small margin.",4.410256410256411,0.7948717948717948,3.9743589743589745,39,2005
"The result suggests that in some cases a few examples are worth giving up on, resulting in the rst term in the bound being larger than 0, but the second term being much smaller due to a larger value for .",4.6976744186046515,0.813953488372093,3.311111111111111,43,2005
"The soft margin version of Support Vector Machines (Cortes and Vapnik, 1995), described in section 8.1, attempts to explicitly manage the trade-off between the two terms in the bound.",4.628571428571429,0.7428571428571429,3.6486486486486487,35,2005
"Theorem 7 (Schapire et al., 1998).",2.9,0.6,2.8181818181818183,10,2005
"Assume the hypothesis class H is a set of hyperplanes in n, and that there is some distribution D(x, y) generating examples.",3.8214285714285716,1.0178571428571428,3.8461538461538463,28,2005
"This bound suggests a strategy that keeps the 1-norm of the parameters low, while trying to classify as many of the training examples as possible with large margin.",3.1333333333333333,1.1833333333333333,3.3333333333333335,30,2005
"It can be shown that the AdaBoost algorithm (Freund and Schapire, 1997) is an effective way of achieving this goal; its application to parsing is described in section 8.2.",4.117647058823529,0.8235294117647058,3.4411764705882355,34,2005
We now consider how the theory for hyperplane classiers might apply to the linear models for parsing described in section 2.,2.3636363636363638,1.0227272727272727,2.9545454545454546,22,2005
"The method for converting parsing to a margin-based problem is similar to the method for ranking problems described in (Freund et al., 1998), and to the approach to multi-class classication problems in (Schapire et al., 1998; Crammer and Singer, 2001; Elisseeff et al., 1999).",4.946428571428571,0.8035714285714286,6.761904761904762,56,2005
"As a rst step, we give a denition of the margins on training examples.",2.75,0.78125,2.1875,16,2005
"Assume we have a training sample {(x1, y1), .",2.9285714285714284,0.8571428571428571,3.357142857142857,14,2005
.,0.0,1.0,0.0,1,2005
.,0.0,1.0,0.0,1,2005
", (xm, ym)}.",2.75,1.0625,1.2,8,2005
"Theorem 8 Assume the hypothesis class H is a set of linear models as dened in equation (1.11) and equation (1.12), and that there is some distribution D(x, y) generating examples.",3.8048780487804876,0.926829268292683,3.923076923076923,41,2005
"For all h  H, for all  > 0, with probability at least 1   over the choice of training set of size m drawn from D,  where R is a constant such that x  X , y  GEN(x), z  GEN(x), ||(x, y)  (x, z)||  R. The variable N is the smallest positive integer such that x  X , |GEN(x)|  1  N.  Proof.",7.363636363636363,0.8939393939393939,4.069767441860465,66,2005
"The proof follows from results in (Zhang, 2002).",2.5,0.7916666666666666,2.0833333333333335,12,2005
See the appendix of this chapter for the proof.,1.9,0.95,2.5,10,2005
Note that this is similar to the bound in theorem 6.,2.0,1.0833333333333333,2.4166666666666665,12,2005
"A difference, however, is the dependence on N, a bound on the number of candidates for any example.",2.8181818181818183,0.8863636363636364,2.3636363636363638,22,2005
"Even though this term is logarithmic, the dependence is problematic because the number of candidate parses for a sentence will usually have an exponential dependence on the length of the sentence, leading to log N having linear  dependence on the maximum sentence length.",3.0869565217391304,0.9239130434782609,3.4680851063829787,46,2005
"(For example, the number of labeled binary-branching trees for a sentence of length n, with G non-terminals, is Gn(2n)!",4.111111111111111,0.7592592592592593,5.344827586206897,27,2005
(n+1)!n!,1.8,0.4,0.6666666666666666,5,2005
", the log of this number is O(n log G + n log n).)",3.789473684210526,0.6578947368421053,3.176470588235294,19,2005
"It is an open problem whether tighter bounds  in particular, bounds which do not depend on N  can be proved.",2.6818181818181817,1.1136363636363635,5.125,22,2005
"Curiously, we show in section 8.3 that the perceptron algorithm leads to a margin-based learning bound that is independent of the value for N. This suggests that it may be possible to prove tighter bounds than those in theorem 8.",2.6153846153846154,1.0769230769230769,2.0,26,2005
"The bounds in theorems 8 and 9 suggested a trade-off between keeping the values for L(, ) and L1(, ) low and keeping the value of  high.",3.90625,0.875,3.4,32,2005
The algorithms described in section 8 attempt to nd a hypothesis  which can achieve low values for these quantities with a high value for .,2.8,0.94,1.7692307692307692,25,2005
The algorithms are direct modications of algorithms for learning hyperplane classiers for binary classication.,2.066666666666667,1.0,2.0,15,2005
these classication algorithms are motivated by the bounds in theorems 6 and 7.,2.2857142857142856,0.6785714285714286,1.7857142857142858,14,2005
In this section we describe parameter estimation algorithms which are motivated by the generalization bounds for linear models in section 7 of this paper.,2.64,0.92,2.6,25,2005
"The rst set of algorithms, support vector machines, use constrained optimization problems that are related to the bounds in theorems 8 and 9.",2.769230769230769,0.8461538461538461,2.6538461538461537,26,2005
"The second algorithm we describe is a modication of AdaBoost (Freund and Schapire, 1997), which is motivated by the bound in theorem 9.",3.2857142857142856,1.0178571428571428,3.142857142857143,28,2005
"Finally, we describe a variant of the perceptron algorithm applied to parsing.",2.5,1.0,2.2857142857142856,14,2005
"The perceptron algorithm does not explicitly attempt to optimize the generalization bounds in section 7, but its convergence and generalization properties can be shown to be dependent on the existence of parameter values which separate the training data with large margin under the 2-norm.",3.5652173913043477,0.75,3.0,46,2005
In this sense they are a close relative to support vector machines.,2.4615384615384617,0.8461538461538461,2.3846153846153846,13,2005
We now describe an algorithm which is motivated by the bound in theorem 8.,2.1333333333333333,1.1333333333333333,2.2666666666666666,15,2005
"First, recall the denition of the margin for the parameter values  on the i-th training example.",2.3333333333333335,0.75,2.761904761904762,18,2005
"Vapnik (1998) shows that the hyperplane  is unique4, and gives a method for nding .",3.7777777777777777,0.9444444444444444,2.8421052631578947,18,2005
The method involves solving the following constrained optimization problem.,1.9,1.0,2.2,10,2005
Any hyperplane  satisfying these constraints separates the data with margin  = 1/||||.,2.5555555555555554,0.9166666666666666,1.9333333333333333,18,2005
"By minimizing ||||2 (or equivalently ||||) subject to the constraints, the method nds the parameters  with maximal value for .",3.7333333333333334,0.8666666666666667,2.25,30,2005
Simply nding the maximum-margin hyperplane may not be optimal or even possible.,2.3846153846153846,0.6153846153846154,2.4,13,2005
"the data may not be separable, or the data may be noisy.",3.2857142857142856,0.5714285714285714,1.9285714285714286,14,2005
The bound in theorem 8 suggests giving up on some training examples which may be difcult or impossible to separate.,2.380952380952381,1.0952380952380953,2.3333333333333335,21,2005
"(Cortes and Vapnik, 1995) suggest a rened optimization task for the classication case which addresses this problem; we suggest the following modied optimization problem as a natural analogue of this approach (our approach is similar to the method for multi-class classication problems in Crammer and Singer, 2001).",4.709090909090909,0.6545454545454545,3.087719298245614,55,2005
Here we have introduced a slack variable i for each training example.,2.3076923076923075,0.7307692307692307,3.2666666666666666,13,2005
"At the solution of the optimization problem, the margin on the i-th training example is at least (1i)/||||.",3.3703703703703702,0.7962962962962963,3.347826086956522,27,2005
"On many examples the slack variable i will be zero, and the margin i  will be at least 1/||||.",3.8076923076923075,0.7692307692307693,2.3636363636363638,26,2005
"On some examples the slack variable i will be positive, implying that the algorithm has given up on separating the example with margin 1/||||.",2.6774193548387095,1.032258064516129,2.857142857142857,31,2005
The constant C controls the cost for having non-zero values of i.,1.4166666666666667,1.0,2.0,12,2005
"As C  , the problem becomes the same as the hardmargin SVM problem, and the method attempts to nd a hyperplane which correctly separates all examples with margin at least 1/|||| (i.e., all slack variables are 0).",3.574468085106383,0.851063829787234,2.772727272727273,47,2005
"For smaller C, the training algorithm may give up on some examples (i.e., set i > 0) in order to keep ||||2 low.",3.09375,0.875,3.4,32,2005
"Thus by varying C, the method effectively modies the trade-off between the two terms in the bound in theorem 8.",3.1363636363636362,0.7954545454545454,3.1666666666666665,22,2005
"In practice, a common approach is to train the model for several values of C, and then to pick the classier which has best performance on some held-out set of development data.",3.4857142857142858,0.9285714285714286,3.4054054054054053,35,2005
"Both kinds of SVM optimization problem outlined above have been studied extensively (e.g., see Joachims, 1998; Platt, 1998) and can be solved relatively efciently.",3.935483870967742,1.0161290322580645,3.096774193548387,31,2005
"(A package for SVMs, written by Thorsten Joachims, is available from http.//ais.gmd.de/thorsten/svm light/.)",3.125,0.84375,2.8421052631578947,16,2005
This can be framed as a linear programming problem.,2.0,0.75,2.3,10,2005
"See (Demiriz et al., 2001) for details, and the relationships between linear programming approaches and the boosting algorithms described in the next section.",3.7857142857142856,0.75,4.379310344827586,28,2005
"The AdaBoost algorithm (Freund and Schapire, 1997) is one method for optimizing the bound for hyperplane classiers in theorem 7 (Schapire et al., 1998).",3.4838709677419355,0.8387096774193549,4.46875,31,2005
"This section describes a modied version of AdaBoost, applied to the parsing problem.",2.8666666666666667,0.7666666666666667,2.8,15,2005
Figure 1.2 shows the modied algorithm.,1.8571428571428572,0.6428571428571429,1.8571428571428572,7,2005
The algorithm converts the training set into a set of triples.,2.0,0.9583333333333334,1.9166666666666667,12,2005
"Each member (x, y1, y2) of T is a triple such that x is a sentence, y1 is the correct tree for that sentence, and y2 is an incorrect tree also proposed by GEN(x).",4.840909090909091,0.7727272727272727,3.7142857142857144,44,2005
"AdaBoost maintains a distribution Dt over the training examples such that Dt(x, y1, y2) is proportional to exp{  ((x, y1)  (x, y2))}.",4.421052631578948,0.8947368421052632,4.516129032258065,38,2005
"Members of T which are well discriminated by the current parameter values  are given low weight by the distribution, whereas examples which are poorly discriminated are weighted more highly.",2.967741935483871,1.1451612903225807,4.5625,31,2005
"The magnitude of rs can be taken as a measure of how correlated (s(x, y1)  s(x, y2)) is with the distribution Dt.",3.4545454545454546,1.0,3.3,33,2005
"If it is highly correlated, |rs| will be large, and the s-th parameter will be useful in driving down the margins on the more highly weighted members of T .",3.8823529411764706,0.8676470588235294,2.5,34,2005
"In the classication case, Schapire et al.",2.3333333333333335,0.7777777777777778,2.2222222222222223,9,2005
"(1998) show that the AdaBoost algorithm has direct properties in terms of optimizing the value of L1(, b, ) dened in equation (1.10).",2.7419354838709675,1.064516129032258,3.3225806451612905,31,2005
"Unfortunately it is not possible to show that the algorithm in gure 1.2 has a similar effect on the parsing quantity L1(, ) in equation (1.15).",2.6774193548387095,0.9516129032258065,3.4838709677419355,31,2005
"Instead, we show its effect on a similar quantity5 RL1.",2.4166666666666665,0.625,2.3333333333333335,12,2005
"There is a strong relation between the values of |rs|, and the effect on the values of RL1(, ).",3.4,0.94,4.043478260869565,25,2005
If we dene t = (1  |rst|)/2 then the following theorem holds.,3.526315789473684,1.0263157894736843,2.533333333333333,19,2005
"It can be shown that f (, ) is less than one providing that  < . the implication is that for all  < , RL1(, ) decreases exponentially in the number of iterations, T .",2.235294117647059,1.1764705882352942,3.1136363636363638,17,2005
"So if the AdaBoost algorithm can successfully maintain high values of |rst| for several iterations, it will be successful at minimizing RL1(, ) for a relatively large range of .",3.857142857142857,0.9285714285714286,3.1515151515151514,35,2005
"Given that RL1 is related to L1, we can view this as an approximate method for optimizing the bound in theorem 9.",3.0416666666666665,1.0625,2.2916666666666665,24,2005
"In practice, a set of held-out data is usually used to optimize T , the number of rounds of boosting.",2.5,1.1136363636363635,2.9166666666666665,22,2005
The algorithm states a restriction on the representation .,1.8888888888888888,0.8333333333333334,1.1111111111111112,9,2005
"For all members (x, y1, y2) of T , for s = 1, .",3.210526315789474,0.7894736842105263,3.6315789473684212,19,2005
.,0.0,1.0,0.0,1,2005
.,0.0,1.0,0.0,1,2005
", n, (s(x, y1)  s(x, y2)) must be in the range 1 to +1.",4.423076923076923,0.8846153846153846,3.0416666666666665,26,2005
This is not as restrictive as it might seem.,2.1,1.0,2.4,10,2005
"If  is always strictly positive, it can be rescaled so that its components are always between 0 and +1.",2.857142857142857,0.8809523809523809,2.652173913043478,21,2005
"If some components may be negative, it sufces to rescale the components so that they are always between 0.5 and +0.5.",2.9130434782608696,0.9565217391304348,2.6666666666666665,23,2005
"A common use of the algorithm, as applied in (Collins, 2000), is to have the n components of  to be the values of n indicator functions, in which case all values of  are either 0 or 1, and the condition is satised.",6.06,0.82,3.3653846153846154,50,2005
"The nal parameter estimation algorithm which we will describe is a variant of the perceptron algorithm, as introduced by (Rosenblatt, 1958).",3.3846153846153846,0.8653846153846154,2.8846153846153846,26,2005
Figure 1.3 shows the algorithm.,1.6666666666666667,0.75,1.5,6,2005
Note that the main computational expense is in calculating y = h(xi) for each example in turn.,2.619047619047619,0.9761904761904762,3.210526315789474,21,2005
For weighted context-free grammars  this step can be achieved in polynomial time using the CKY parsing algorithm.,2.6666666666666665,0.7222222222222222,2.6666666666666665,18,2005
"Other representations may have to rely on explicitly calculating (xi, z)   for all z  GEN(xi), and hence depend computationally on the number of candidates |GEN(xi)| for i = 1, .",4.674418604651163,1.2209302325581395,4.2105263157894735,43,2005
.,0.0,1.0,0.0,1,2005
.,0.0,1.0,0.0,1,2005
", m.  Proof.",1.5,0.25,1.4,4,2005
"See (Collins, 2002b) for a proof.",2.7,1.0,2.9,10,2005
"The proof is a simple modication of the proof for hyperplane classiers (Block, 1962; Novikoff, 1962, see also Freund and Schapire, 1999).",3.3,0.8666666666666667,3.3666666666666667,30,2005
"This theorem implies that if the training sample in gure 1.3 is separable, and we iterate the algorithm repeatedly over the training sample, then the algorithm converges to a parameter setting that classies the training set with zero errors.",3.9523809523809526,0.7380952380952381,4.285714285714286,42,2005
"(In particular, we need at most (R/)2 passes over the training sample before convergence.)",3.6818181818181817,0.9090909090909091,2.736842105263158,22,2005
Thus we now have an algorithm for training weighted context-free grammars which will nd a zero error hypothesis if it exists.,2.6363636363636362,1.0227272727272727,2.8333333333333335,22,2005
"For example, the algorithm would nd a weighted grammar with zero expected error on the example problem in section 3.",2.6363636363636362,0.7045454545454546,3.3636363636363638,22,2005
Of course convergence to a zero-error hypothesis on training data says little about how well the method generalizes to new test examples.,2.9130434782608696,0.9130434782608695,2.48,23,2005
Fortunately a second theorem gives a bound on the generalization error of the perceptron method.,2.6875,0.71875,2.25,16,2005
"Theorem 12 (Direct consequence of the sample compression bound in (Littlestone and Warmuth, 1986); see also theorem 4.25, page 70, Cristianini and Shawe-Taylor, 2000).",4.382352941176471,0.75,3.5555555555555554,34,2005
"Say the perceptron algorithm makes d mistakes when run to convergence over a training set of size m.   Given that d  (R/)2, this bound states that if the problem is separable with large margin  i.e., the ratio R/ is relatively small  then the perceptron will converge to a hypothesis with good expected error with a reasonable number of training examples.",3.2794117647058822,0.9117647058823529,3.1323529411764706,68,2005
The perceptron algorithm is remarkable in a few respects.,2.1,0.65,1.7,10,2005
"First, the algorithm in gure 1.3 can be efcient even in cases where GEN(x) is of exponential size in terms of the input x, providing that the highest scoring structure can be found efciently for each training example.",2.8636363636363638,0.9545454545454546,4.214285714285714,44,2005
"For example, nding the arg max can be achieved in polynomial time for context-free grammars, so they can be trained efciently using the algorithm.",4.0,0.8148148148148148,3.3793103448275863,27,2005
"This is in contrast to the support vector machine and boosting algorithms, where we are not aware of algorithms whose computational complexity does not depend on the size of GEN(xi) for i = 1, .",4.2,1.05,5.157894736842105,40,2005
.,0.0,1.0,0.0,1,2005
.,0.0,1.0,0.0,1,2005
", n. Second, the convergence properties (number of updates) of the algorithm are also independent of the size of GEN(xi) for i = 1, .",3.15625,0.921875,3.9,32,2005
.,0.0,1.0,0.0,1,2005
.,0.0,1.0,0.0,1,2005
", n, depending on the maximum achievable margin  on the training set.",2.4285714285714284,1.0714285714285714,2.4,14,2005
"Third, the generalization theorem (theorem 12) shows that the generalization properties are again independent of the size of each GEN(xi), depending only on .",3.4193548387096775,0.8387096774193549,3.310344827586207,31,2005
"This is in contrast to the bounds in theorems 8 and 9, which depended on N, a bound on the number of candidates for any input.",2.689655172413793,1.0,2.413793103448276,29,2005
"The theorems quoted here do not treat the case where the data is not separable, but results for the perceptron algorithm can also be derived in this case.",2.6333333333333333,0.8833333333333333,3.1333333333333333,30,2005
"See (Freund and Schapire, 1999) for analysis of the classication case, and see (Collins, 2002b) for how these results can be carried over to problems such as parsing.",4.027777777777778,0.8055555555555556,4.805555555555555,36,2005
"Collins (2002b) shows how the perceptron algorithm can be applied to tagging problems, with improvements in accuracy over a maximum-entropy tagger on part-of-speech tagging and NP chunking; see this paper for more analysis of the perceptron algorithm, and some modications to the basic algorithm.",3.74,0.84,4.553571428571429,50,2005
In this section we give further discussion of the algorithms in this chapter.,2.2857142857142856,0.8928571428571429,2.357142857142857,14,2005
Section 9.1 describes experimental results using some of the algorithms.,1.9090909090909092,0.9545454545454546,1.8181818181818181,11,2005
Section 9.2 describes relationships to Markov Random Field approaches.,2.2,0.65,2.4,10,2005
There are several papers describing experiments on NLP tasks using the algorithms described in this paper.,2.0588235294117645,0.9705882352941176,2.235294117647059,17,2005
Collins (2000) describes a boosting method  which is related to the algorithm in gure 1.2.,2.2777777777777777,1.0555555555555556,2.526315789473684,18,2005
"In this case GEN(x) is the top N most likely parses from the parser of (Collins, 1999).",3.4166666666666665,0.7708333333333334,2.5,24,2005
"The representation (x, y) combines the log probability under the initial model, together with a large number of additional indicator functions which are various features of trees.",3.21875,0.8125,3.0,32,2005
"The paper describes a boosting algorithm which is particularly efcient when the features are indicator (binary-valued) functions, and the features are relatively sparse.",4.444444444444445,0.8888888888888888,4.068965517241379,27,2005
"The method gives a 13% relative reduction in error over the original parser of (Collins, 1999).",3.2857142857142856,0.7857142857142857,2.6666666666666665,21,2005
"(See (Ratnaparkhi et al., 1994) for an approach which also uses a N-best output from a baseline model combined with global features, but a different algorithm for training the parameters of the model.)",3.975,0.85,4.133333333333334,40,2005
Collins (2002a) describes a similar approach applied to named entity extraction.,2.357142857142857,0.9285714285714286,2.357142857142857,14,2005
GEN(x) is the top 20 most likely hypotheses from a maximum-entropy tagger.,2.875,0.71875,3.125,16,2005
"The representation again includes the log probability under the original model, together with a large number of indicator functions.",3.142857142857143,0.6428571428571429,2.7142857142857144,21,2005
The boosting and perceptron algorithms give relative error reductions of 15.6% and 17.7% respectively.,3.3529411764705883,0.6176470588235294,2.9411764705882355,17,2005
Collins and Duffy (2002) and Collins and Duffy (2001) describe the perceptron algorithm applied to parsing and tagging problems.,3.4583333333333335,0.9166666666666666,3.3333333333333335,24,2005
GEN(x) is again the top N most likely parses from a baseline model.,2.8823529411764706,0.7352941176470589,2.7333333333333334,17,2005
"The particular twist in these papers is that the representation (x, y) for both the tagging and parsing problems is an extremely high-dimensional representation, which tracks all subtrees in the parsing case (in the same way as the DOP approach to parsing, see Bod, 1998), or all sub-fragments of a tagged sequence.",4.451612903225806,0.8387096774193549,4.7846153846153845,62,2005
"The key to making the method computationally efcient (in spite of the high dimensionality of ) is that for any pair of structures (x1, y1) and (x2, y2) it can be shown that the inner product (x1, y1)  (x2, y2) can be calculated efciently using dynamic programming.",4.311475409836065,0.9590163934426229,4.885245901639344,61,2005
"The perceptron algorithm has an efcient dual implementation which makes use of inner products between examples  see (Cristianini and Shawe-Taylor, 2000; Collins and Duffy, 2002).",3.935483870967742,0.9838709677419355,3.9166666666666665,31,2005
"Collins and Duffy (2002) show a 5% relative error improvement for parsing, and a more signicant 15% relative error improvement on the tagging task.",4.633333333333334,0.6833333333333333,3.533333333333333,30,2005
Collins (2002b) describes perceptron algorithms applied to the tagging task.,2.230769230769231,1.0,2.230769230769231,13,2005
GEN(x) for a sentence x of length n is the set of all possible tag sequences of length n (there are T n such sequences if T is the number of tags).,3.3421052631578947,0.881578947368421,2.5277777777777777,38,2005
"The representation used is similar to the feature-vector representations used in maximumentropy taggers, as in (Ratnaparkhi, 1996).",3.227272727272727,0.7954545454545454,3.375,22,2005
"The highest scoring tagged sequence under this representation can be found efciently using the perceptron algorithm, so the weights can be trained using the algorithm in gure 1.3 without having to exhaustively enumerate all tagged sequences.",3.6842105263157894,0.9736842105263158,2.973684210526316,38,2005
The method gives improvements over the maximum-entropy approach.,1.8888888888888888,0.8333333333333334,2.272727272727273,9,2005
"a 12% relative error   reduction for part-of-speech tagging, a 5% relative error reduction for nounphrase chunking.",4.1,0.55,3.6,20,2005
"Another method for training the parameters  can be derived from loglinear models, or Markov Random Fields (otherwise known as maximumentropy models).",2.72,0.84,2.730769230769231,25,2005
"Several approaches (Ratnaparkhi et al., 1994; Johnson et al., 1999; Lafferty et al., 2001) use the parameters  to dene a conditional probability distribution over the candidates y  GEN(x).",4.153846153846154,0.9615384615384616,3.7857142857142856,39,2005
"Once the model is trained, the output on a new sentence x is the highest probability parse, arg maxyGEN(x) P (y | x, ) = arg maxyGEN(x) (x, y)  .",4.744186046511628,0.6976744186046512,3.575,43,2005
So the output under parameters  is identical to the method used throughout this paper.,2.2,0.9,2.0625,15,2005
The differences between this method and the approaches advocated in this paper are twofold.,2.8666666666666667,0.9,1.9333333333333333,15,2005
"First, the statistical justication differs.",2.4285714285714284,0.6428571428571429,2.0,7,2005
"the log-linear approach is a parametric approach (see section 4.2), explicitly attempting to model the conditional distribution D(y | x), and potentially suffering from the problems described in section 4.3.",3.8421052631578947,0.8421052631578947,3.6842105263157894,38,2005
The second difference concerns the algorithms for training the parameters.,2.1818181818181817,0.8181818181818182,1.9090909090909092,11,2005
"In training log-linear models, a rst crucial concept is the log-likelihood of the training data,   Parameter estimation methods in the MRF framework generally involve maximizing the log-likelihood while controlling for overtting the training data.",4.783783783783784,0.6891891891891891,3.0681818181818183,37,2005
"A rst method for controlling the degree of overtting, as used in (Ratnaparkhi et al., 1994), is to use feature selection.",4.481481481481482,0.9629629629629629,4.928571428571429,27,2005
In this case a greedy method is used to minimize the log likelihood using only a small number of features.,2.6666666666666665,0.8095238095238095,2.4285714285714284,21,2005
It can be shown that the boosting algorithms can be considered to be a feature selection method for minimizing the exponential loss.,2.3043478260869565,0.9565217391304348,2.9130434782608696,23,2005
"A second method for controlling overtting, used in (Johnson et al., 1999; Lafferty et al., 2001), is to use a gaussian prior over the parameters.",4.454545454545454,0.8181818181818182,4.714285714285714,33,2005
The method then selects the MAP parameters  the parameters which maximize the objective function  for some constant C which is determined by the variance term in the gaussian prior.,2.5,0.9166666666666666,2.78125,30,2005
"This method has at least a supercial similarity to the SVM algorithm in section 8.1 (2-norm case), which also attempts to balance the norm of the parameters versus a function measuring how well the parameters t the data (i.e., the sum of the slack variable values).",3.056603773584906,0.9150943396226415,3.2830188679245285,53,2005
"We should stress again, however, that in spite of some similarities between the algorithms for MRFs and the boosting and SVM methods, the statistical justication for the methods differs considerably.",4.117647058823529,0.8235294117647058,4.911764705882353,34,2005
This paper has described a number of methods for learning statistical grammars.,2.230769230769231,1.0,1.9230769230769231,13,2005
All of these methods have several components in common.,2.1,1.05,1.8,10,2005
"the choice of a grammar which denes the set of candidates for a given sentence, and the choice of representation of parse trees.",3.04,0.98,3.84,25,2005
"A score indicating the plausibility of competing parse trees is taken to be a linear model, the result of the inner product between a trees feature vector and the vector of model parameters.",4.0285714285714285,0.7857142857142857,2.75,35,2005
The only respect in which the methods differ is in how the parameter values (the weights on different features) are calculated using a training sample as evidence.,3.033333333333333,1.0,4.03125,30,2005
Section 4 introduced a framework under which various parameter estimation methods could be studied.,2.4,0.9333333333333333,3.1333333333333333,15,2005
This framework included two main components.,1.8571428571428572,0.6428571428571429,1.7142857142857142,7,2005
"First, we assume some xed but unknown distribution over sentence/parsetree pairs.",2.076923076923077,1.0,2.7333333333333334,13,2005
Both training and test examples are drawn from this distribution.,2.6363636363636362,0.5909090909090909,1.6363636363636365,11,2005
"Second, we assume some loss function, which dictates the penalty on test examples for proposing a parse which is incorrect.",2.5652173913043477,1.1304347826086956,2.5652173913043477,23,2005
"We focused on a simple loss function, where the loss is 0 if the proposed parse is identical to the correct parse, 1 otherwise.",2.925925925925926,0.8703703703703703,5.0,27,2005
"Under these assumptions, the quality of a parser is its expected loss (expected error rate) on newly drawn test examples.",3.125,0.6875,3.1923076923076925,24,2005
The goal of   learning is to use the training data as evidence for choosing a function which has small expected loss.,2.5454545454545454,1.0909090909090908,2.347826086956522,22,2005
A central idea in the analysis of learning algorithms is that of the margins on examples in training data.,2.5,1.1,2.0,20,2005
We described theoretical bounds which motivate approaches which attempt classify a large proportion of examples in training with a large margin.,2.409090909090909,1.2045454545454546,2.5,22,2005
"Finally, we described several algorithms which can be used to achieve this goal on the parsing problem.",2.210526315789474,1.0263157894736843,2.5789473684210527,19,2005
There are several open problems highlighted in this paper.,2.0,0.85,2.0,10,2005
"The margin bounds for parsing (theorems 8 and 9) both depend on N, a bound on the number of candidates for any input sentence.",3.3214285714285716,0.8392857142857143,2.2142857142857144,28,2005
It is an open question whether bounds which are independent of N can be proved.,2.375,1.21875,3.75,16,2005
"The perceptron algorithm in section 8.3 has generalization bounds which are independent of N, suggesting that this might also be possible for the margin bounds.",2.925925925925926,1.0185185185185186,2.962962962962963,27,2005
The Boosting and Support Vector Machine methods both require enumerating all members of GEN(xi) for each training example xi.,2.9565217391304346,0.9130434782608695,3.142857142857143,23,2005
"The perceptron algorithm avoided this in the case where the highest scoring hypothesis could be calculated efciently, for example using the CKY algorithm.",2.52,0.88,3.76,25,2005
It would be very useful to derive SVM and boosting algorithms whose computational complexity can be shown to depend on the separation  rather than the size of GEN(xi) for each training example xi.,3.2972972972972974,0.9864864864864865,2.4444444444444446,37,2005
"The boosting algorithm in section 8.2 optimized the quantity RL1, rather than the desired quantity L1.",2.9444444444444446,0.5277777777777778,2.8333333333333335,18,2005
It would be useful to derive a boosting algorithm which provably optimized L1.,2.0,1.25,2.357142857142857,14,2005
"I would like to thank Sanjoy Dasgupta, Yoav Freund, John Langford, David McAllester, Rob Schapire and Yoram Singer for answering many of the questions I have had about the learning theory and algorithms in this paper.",5.658536585365853,0.7560975609756098,2.951219512195122,41,2005
Fernando Pereira pointed out several issues concerning analysis of the perceptron algorithm.,2.076923076923077,0.8846153846153846,2.6923076923076925,13,2005
"Thanks also to Nigel Duffy, for many useful discussions while we were collaborating on the use of kernels for parsing problems.",3.652173913043478,0.8913043478260869,3.260869565217391,23,2005
I would like to thank Tong Zhang for several useful insights concerning margin-based generalization bounds for multi-class problems.,2.210526315789474,0.9473684210526315,2.5652173913043477,19,2005
"Thanks to Brian Roark for helpful comments on an initial draft of this paper, and to Patrick Haffner for many useful suggestions.",3.7916666666666665,0.7916666666666666,4.416666666666667,24,2005
"Thanks also to Peter Bartlett, for feedback on the paper, and some useful pointers to references.",2.8421052631578947,0.8421052631578947,3.5789473684210527,19,2005
"The proofs in this section closely follow the framework and results of (Zhang, 2002).",3.111111111111111,0.6944444444444444,2.2222222222222223,18,2005
"The basic idea is to show that the covering number results of (Zhang, 2002) apply to the parsing problem, with the modication that any dependence on m (the sample size) is replaced by a dependence on mN (where N is the smallest integer such that .",2.8867924528301887,0.8584905660377359,3.849056603773585,53,2005
"In the problems in this paper we again assume that sample points are (x, y) pairs, where x  X is an input, and y  Y is the correct structure for that input.",5.026315789473684,0.7894736842105263,3.125,38,2005
There is some function GEN(x) which maps any x  X to a set of candidates.,2.5789473684210527,0.9473684210526315,2.5555555555555554,19,2005
There is also a function  .,1.6666666666666667,0.9166666666666666,2.0,6,2005
"X  Y  n that maps each (x, y) pair to a feature vector.",3.0,0.8529411764705882,3.0526315789473686,17,2005
"We will transform any sample point (x, y) to a matrix Z  N n in the following way.",3.5,0.6590909090909091,2.391304347826087,22,2005
Take N to be a positive integer such that |GEN(x)| = (N + 1).,2.590909090909091,0.8181818181818182,3.5294117647058822,22,2005
"Zhang (2002) shows how bounds on the covering numbers of L lead to the theorems 6 and 8 of (Zhang, 2002), which are similar but tighter bounds than the bounds given in theorems 6 and 7 in section 6 of the current paper.",3.68,0.96,3.4,50,2005
"Theorem A1 below states a relationship between the covering numbers for L and M. Under this result, theorems 8 and 9 in the current paper follow from the covering bounds on M in exactly the same way that theorems 6 and 8 of (Zhang, 2002) are  derived from the covering numbers of L, and theorem 2 of (Zhang, 2002).",4.264705882352941,0.8676470588235294,2.36231884057971,68,2005
So theorem A1 leads almost directly to theorems 8 and 9 in the current paper.,2.4375,0.65625,2.4375,16,2005
