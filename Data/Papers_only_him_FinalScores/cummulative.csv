sentence,yngve_score,frazier_score,dependency_distance_score,count,year
A New Statistical Parser Based on Bigram Lexical Dependencies  This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree.,2,0.9833333333333333,2,30,1996
Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words.,2,1.0588235294117647,1,17,1996
Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al.,4,1.0,3,25,1996
"94), which has the best published results for a statistical parser on this task.",2,0.6764705882352942,2,17,1996
"The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes.",2,0.7941176470588235,2,17,1996
With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy.,2,0.7954545454545454,2,22,1996
"Lexical information has been shown to be crucial for many parsing decisions, such as prepositional-phrase attachment (for example (Hindle and Rooth 93)).",3,0.8214285714285714,2,28,1996
"However, early approaches to probabilistic parsing (Pereira and Schabes 92; Magerman and Marcus 91; Briscoe and Carroll 93) conditioned probabilities on non-terminal labels and part of speech tags alone.",4,0.7857142857142857,4,35,1996
The SPATTER parser (Magerman 95; Jelinek et al.,3,0,2,11,1996
"94) does use lexical information, and recovers labeled constituents in Wall Street Journal text with above 84% accuracy  as far as we know the best published results on this task.",3,0.8529411764705882,3,34,1996
"This paper describes a new parser which is much simpler than SPATTER, yet performs at least as well when trained and tested on the same Wall Street Journal data.",3,0.8709677419354839,3,31,1996
The method uses lexical information directly by modeling head-modier1 relations between pairs of words.,2,0.9,2,15,1996
In this way it is similar to  The aim of a parser is to take a tagged sentence as input (for example Figure 1(a)) and produce a phrase-structure tree as output (Figure 1(b)).,3,0.9886363636363636,4,44,1996
A statistical approach to this problem consists of two components.,2,0.7727272727272727,1,11,1996
"First, the statistical model assigns a probability to every candidate parse tree for a sentence.",2,0.8235294117647058,2,17,1996
"Formally, given a sentence S and a tree T , the model estimates the conditional probability P (T |S).",3,0.6041666666666666,4,24,1996
The most likely parse under the model is then.,2,0.85,1,10,1996
"Second, the parser is a method for nding Tbest.",2,0.7727272727272727,1,11,1996
"This section describes the statistical model, while section 3 describes the parser.",2,0.7142857142857143,2,14,1996
The key to the statistical model is that any tree such as Figure 1(b) can be represented as a set of baseNPs2 and a set of dependencies as in Figure 1(c).,3,0.9210526315789473,3,38,1996
"We call the set of baseNPs B, and the set of dependencies D; Figure 1(d) shows B and D for this example.",3,0,3,28,1996
"For the purposes of our model, T = (B, D), and.",3,0,4,17,1996
S is the sentence with words tagged for part of speech.,1,1.2083333333333333,1,12,1996
"That is, S =< (w1, t1), (w2, t2)...(wn, tn) >.",5,1.06,2,25,1996
For POS tagging we use a maximum-entropy tagger described in (Ratnaparkhi 96).,2,0.7666666666666667,2,15,1996
"The tagger performs at around 97% accuracy on Wall Street Journal Text, and is trained on the rst 40,000 sentences of the Penn Treebank (Marcus et al.",4,0.7096774193548387,3,31,1996
93).,1,0,1,3,1996
"Given S and B, the reduced sentence S is dened as the subsequence of S which is formed by removing punctuation and reducing all baseNPs to their head-word alone.",2,0.8870967741935484,2,31,1996
"2A baseNP or minimal NP is a non-recursive NP, i.e.",2,0.7727272727272727,2,11,1996
none of its child constituents are NPs.,2,0.9375,1,8,1996
The term was rst used in (Ramshaw and Marcus 95).,2,1.0,2,13,1996
Figure 1.,1,1,1,3,1996
An overview of the representation used by the model.,2,0.85,1,10,1996
(a) The tagged sentence; (b) A candidate parse-tree (the correct one); (c) A dependency representation of (b).,4,0,2,30,1996
Square brackets enclose baseNPs (heads of baseNPs are marked in bold).,2,1.1428571428571428,2,14,1996
Arrows show modier  head dependencies.,1,0.75,2,6,1996
Section 2.1 describes how arrows are labeled with non-terminal triples from the parse-tree.,2,1.0,2,14,1996
"Non-head words within baseNPs are excluded from the dependency structure; (d) B, the set of baseNPs, and D, the set of dependencies, are extracted from (c).",3,0.8472222222222222,3,36,1996
Sections 2.1 to 2.4 describe the dependency model.,2,0.8333333333333334,1,9,1996
"Section 2.5 then describes the baseNP model, which uses bigram tagging techniques similar to (Ramshaw and Marcus 95; Church 88).",3,0.8,2,25,1996
The Mapping from Trees to Sets of Dependencies  The dependency model is limited to relationships between words in reduced sentences such as Example 1.,2,1.06,1,25,1996
The mapping from trees to dependency structures is central to the dependency model.,2,0.8928571428571429,2,14,1996
It is dened in two steps.,1,0.9285714285714286,1,7,1996
1.,0,1.25,0,2,1996
For each constituent P < C1...Cn > in the parse tree a simple set of rules3 identies which of the children Ci is the head-child of P .,3,0.7666666666666667,2,30,1996
"For example, NN would be identied as the head-child of NP <DET JJ JJ NN>, VP would be identied as the head-child of S <NP VP>.",3,0.96,2,25,1996
"Head-words propagate up through the tree, each parent receiving its head-word from its head-child.",2,0.78125,2,16,1996
"For example, in S <NP VP>, S gets its head-word, announced,  3The rules are essentially the same as in (Magerman 95; Jelinek et al.",4,0.8,3,30,1996
94).,1,0,1,3,1996
"These rules are also used to nd the head-word of baseNPs, enabling the mapping from S and B to S.  Figure 2.",2,0.8541666666666666,3,24,1996
Parse tree for the reduced sentence in Example 1.,2,0.95,1,10,1996
The head-child of each constituent is shown in bold.,2,0.95,1,10,1996
The head-word for each constituent is shown in parentheses.,2,0.95,1,10,1996
2.,0,1.25,0,2,1996
Head-modier relationships are now extracted from the tree in Figure 2.,2,0.875,1,12,1996
Figure 3 illustrates how each constituent contributes a set of dependency relationships.,2,1.0,2,13,1996
VBD is identied as the head-child of VP  <VBD NP NP>.,1,0.95,3,10,1996
"The head-words of the two NPs, resignation and yesterday, both modify the head-word of the VBD, announced.",5,0.6904761904761905,3,21,1996
"Dependencies are labeled by the modier non-terminal, NP in both of these cases, the parent non-terminal, VP, and nally the head-child non-terminal, VBD.",4,0.7413793103448276,4,29,1996
"The triple of nonterminals at the start, middle and end of the arrow specify the nature of the dependency relationship  <NP,S,VP> represents a subject-verb dependency, <PP,NP,NP> denotes prepositional phrase modication of an NP, and so on4.",4,0.9326923076923077,3,52,1996
Figure 3.,1,1,1,3,1996
Each constituent with n children (in this case n = 3) contributes n  1 dependencies.,3,0.9166666666666666,3,18,1996
"Each word in the reduced sentence, with the exception of the sentential head announced, modies exactly one other word.",3,0.7045454545454546,3,22,1996
"We use the notation  to state that the jth word in the reduced sentence is a modier to the hjth word, with relationship 5.",3,0.7692307692307693,3,26,1996
AF stands for arrow from.,1,1.0833333333333333,1,6,1996
"Rj is the triple Rj of labels at the start, middle and end of the arrow.",3,0.8611111111111112,2,18,1996
"For example, w1 = Smith in this sentence,  The triple can also be viewed as representing a semantic predicate-argument relationship, with the three elements being the type of the argument, result and functor respectively.",3,0.7948717948717948,4,39,1996
"This is particularly apparent in Categorial Grammar formalisms (Wood 93), which make an explicit link between dependencies and functional application.",2,0.875,2,24,1996
"2.2 Calculating Dependency Probabilities This section describes the way P (AF (j)|S, B) is estimated.",4,0.9130434782608695,2,23,1996
"The same sentence is very unlikely to appear both in training and test data, so we need to back-o from the entire sentence context.",3,0.6923076923076923,2,26,1996
"We believe that lexical information is crucial to attachment decisions, so it is natural to condition on the words and tags.",3,0.7608695652173914,2,23,1996
"Let V be the vocabulary of all words seen in training data, T be the set of all part-of-speech tags, and T RAIN be the training set, a set of reduced sentences.",3,0.8055555555555556,3,36,1996
We dene the following functions.,1,0.75,1,6,1996
An estimate based on the identities of the two tokens alone is problematic.,3,0.8928571428571429,1,14,1996
"Additional context, in particular the relative order of the two words and the distance between them, will also strongly inuence the likelihood of one word modifying the other.",3,0.7903225806451613,3,31,1996
For example consider the relationship between sales and the three tokens of of.,2,0.8928571428571429,2,14,1996
"Example 2 Shaw, based in Dalton, Ga., has annual sales of about $ 1.18 billion, and has economies of scale and lower raw-material costs that are expected to boost the protability of Armstrong s brands, sold under the Armstrong and Evans-Black names .",4,0.7916666666666666,4,48,1996
In this sentence sales and of co-occur three times.,2,0,2,10,1996
"The parse tree in training data indicates a relationship in only one of these cases, so this sentence would contribute an estimate of 1 3 that the two words are related.",3,0.7727272727272727,3,33,1996
This seems unreasonably low given that sales of is a strong collocation.,2,1.0,2,13,1996
The latter two instances of of are so distant from sales that it is unlikely that there will be a dependency.,2,0.9772727272727273,2,22,1996
This suggests that distance is a crucial variable when deciding whether two words are related.,2,1.1875,2,16,1996
"It is included in the model by dening an extra distance variable, , and extending C, F and F to include this variable.",4,0.6153846153846154,4,26,1996
"For example, C( ha, bi , hc, di , ) is the number of times ha, bi and hc, di appear in the same sentence at a distance  apart.",4,0.75,4,36,1996
(11) is then maximised instead of (10).,2,0.9583333333333334,2,12,1996
"A simple example of j,hj would be j,hj = hj  j.",1,0.9666666666666667,2,15,1996
"However, other features of a sentence, such as punctuation, are also useful when deciding if two words  are related.",3,0.9347826086956522,3,23,1996
"We have developed a heuristic distance measure which takes several such features into account The current distance measure j,hj is the combination of 6 features, or questions (we motivate the choice of these questions qualitatively  section 4 gives quantitative results showing their merit).",3,0.9693877551020408,3,49,1996
Question 1 Does the hjth word precede or follow the jth word?,2,0.6153846153846154,2,13,1996
"English is a language with strong word order, so the order of the two words in surface text will clearly aect their dependency statistics.",3,0,3,26,1996
Question 2 Are the hjth word and the jth word adjacent?,3,0.625,2,12,1996
"English is largely right-branching and head-initial, which leads to a large proportion of dependencies being between adjacent words 7.",2,0.9761904761904762,3,21,1996
Table 1 shows just how local most dependencies are.,2,0.9,2,10,1996
Table 1.,1,1,1,3,1996
Percentage of dependencies vs. distance between the head words involved.,2,1.1363636363636365,2,11,1996
"These gures count baseNPs as a single word, and are taken from WSJ training data.",3,0.6764705882352942,3,17,1996
Table 2.,1,1,1,3,1996
Percentage of dependencies vs. number of verbs between the head words involved.,2,1.1923076923076923,2,13,1996
Question 3 Is there a verb between the hjth word and the jth word?,2,0.7333333333333333,2,15,1996
"Conditioning on the exact distance between two words by making j,hj = hj  j leads to severe sparse data problems.",4,0.717391304347826,3,23,1996
But Table 1 shows the need to make ner distance distinctions than just whether two words are adjacent.,2,0.8157894736842105,2,19,1996
"Consider the prepositions to, in and of in the following sentence.",2,0.7692307692307693,2,13,1996
"Example 3 Oil stocks escaped the brunt of Friday s selling and several were able to post gains , including Chevron , which rose 5/8 to 66 3/8 in Big Board composite trading of 2.4 million shares .",3,0.8918918918918919,2,37,1996
"The prepositions main candidates for attachment would appear to be the previous verb, rose, and the baseNP heads between each preposition and this verb.",5,0.9074074074074074,3,27,1996
They are less likely to modify a more distant verb such as escaped.,2,1.0714285714285714,2,14,1996
"Question 3 allows the parser to prefer modication of the most recent verb  eectively another, weaker preference for right-branching structures.",3,1.0,2,22,1996
"Table 2 shows that 94% of dependencies do not cross a verb, giving empirical evidence that question 3 is useful.",3,1.0434782608695652,3,23,1996
7For example in (John (likes (to (go (to (University (of Pennsylvania))))))) all dependencies are between adjacent words.,6,0.8125,2,32,1996
" Are there 0, 1, 2, or more than 2 commas between the hjth word and the jth word?",3,0.6590909090909091,3,22,1996
"(All symbols tagged as a , or . are considered to be commas).",2,0.7222222222222222,3,9,1996
 Is there a comma immediately following the  rst of the hjth word and the jth word?,2,0,2,17,1996
 Is there a comma immediately preceding the  second of the hjth word and the jth word?,2,0,2,17,1996
"People nd that punctuation is extremely useful for identifying phrase structure, and the parser described here also relies on it heavily.",3,0.9130434782608695,3,23,1996
Commas are not considered to be words or modiers in the dependency model  but they do give strong indications about the parse structure.,3,0.6875,3,24,1996
"Questions 4, 5 and 6 allow the parser to use this information.",2,0.8571428571428571,2,14,1996
"The maximum likelihood estimator is likely to be plagued by sparse data problems  C( h wj , tji , h whj , thj i , j,hj ) may be too low to give a reliable estimate, or worse still it may be zero leaving the estimate undened.",4,0.9615384615384616,2,52,1996
(Collins 95) describes how a backed-o estimation strategy is used for making prepositional phrase attachment decisions.,2,0.868421052631579,3,19,1996
The idea is to back-o to estimates based on less context.,1,1.0833333333333333,2,12,1996
"In this case, less context means looking at the POS tags rather than the specic words.",2,0.7222222222222222,2,18,1996
"There are four estimates, E1, E2, E3 and E4, based respectively on.",4,0.6176470588235294,3,17,1996
1) both words and both tags; 2) wj and the two POS tags; 3) whj and the two POS tags; 4) the two POS tags alone.,5,0.7205882352941176,5,34,1996
where V is the set of all words seen in training data.,1,1.1153846153846154,1,13,1996
the other denitions of C follow similarly.,2,0.9375,1,8,1996
Estimates 2 and 3 compete  for a given pair of words in test data both estimates may exist and they are equally specic to the test case example.,4,0.7068965517241379,2,29,1996
"(Collins 95) suggests the following way of combining them, which favours the estimate appearing more often in training data.",2,0.9782608695652174,2,23,1996
This gives three estimates.,1,0.9,1,5,1996
"E1, E23 and E4, a similar situation to trigram language modeling for speech recognition (Jelinek 90), where there are trigram, bigram and unigram estimates.",3,0.6290322580645161,3,31,1996
"(Jelinek 90) describes a deleted interpolation method which combines these estimates to give a smooth estimate, and the model uses a variation of this idea.",4,0.7586206896551724,3,29,1996
These  values have the desired property of increasing as the denominator of the more specic estimator increases.,2,0.8333333333333334,2,18,1996
"We think that a proper implementation of deleted interpolation is likely to improve results, although basing estimates on co-occurrence counts alone has the advantage of reduced training times.",2,0.9666666666666667,3,30,1996
The overall model would be simpler if we could do without the baseNP model and frame everything in terms of dependencies.,2,0.9090909090909091,2,22,1996
However the baseNP model is needed for two reasons.,2,0.75,1,10,1996
"First, while adjacency between words is a good indicator of whether there is some relationship between them, this indicator is made substantially stronger if baseNPs are reduced to a single word.",3,0.9705882352941176,3,34,1996
it means that words internal to baseNPs are not included in the co-occurrence counts in training data.,2,1.0555555555555556,3,18,1996
"Otherwise,  Second, in a phrase like The Securities and Exchange Commission closed yesterday, pre-modifying nouns like Securities and Exchange would be included in cooccurrence counts, when in practice there is no way that they can modify words outside their baseNP.",2,0.9111111111111111,4,45,1996
"The baseNP model can be viewed as tagging the gaps between words with S(tart), C(ontinue), E(nd), B(etween) or N (ull) symbols, respectively meaning that the gap is at the start of a BaseN P , continues a BaseN P , is at the end of a BaseN P , is between two adjacent baseN P s, or is between two words which are both not in BaseN.",6,0.8275862068965517,4,87,1996
We call the gap before the ith word Gi.,2,0.65,2,10,1996
"Probability estimates are based on counts of consecutive pairs of words in unreduced training data sentences, where baseNP boundaries dene whether gaps fall into the S, C, E, B or N categories.",3,0.7432432432432432,2,37,1996
The probability of a baseNP sequence in an unreduced sentence S is then.,3,0.75,2,14,1996
The estimation method is analogous to that described in the sparse data section of this paper.,2,0.7941176470588235,2,17,1996
"The method is similar to that described in (Ramshaw and Marcus 95; Church 88), where baseNP detection is also framed as a tagging problem.",3,0.896551724137931,3,29,1996
"The denominator in Equation (9) is not actually constant for dierent baseNP sequences, but we make this approximation for the sake of eciency and simplicity.",4,0,2,29,1996
"In practice this is a good approximation because most baseNP boundaries are very well dened, so parses which have high enough P (B|S) to be among the highest scoring parses for a sentence tend to have identical or very similar baseNPs.",3,0.9893617021276596,3,47,1996
Parses are ranked by the following quantity9.,1,0.8125,1,8,1996
The parser nds the tree which maximises (20) subject to the hard constraint that dependencies cannot cross.,2,1.2619047619047619,2,21,1996
"In fact we also model the set of unary productions, U , in the tree, which are of the form P < C1 >.",2,0.9259259259259259,3,27,1996
"This introduces an additional term, P (U |B, S), into (20).",3,0.725,3,20,1996
This section describes two modications which improve the models performance.,2,1.0,2,11,1996
" In addition to conditioning on whether dependencies cross commas, a single constraint concerning punctuation is introduced.",3,1.1111111111111112,2,18,1996
"If for any constituent Z in the chart Z  <.. X Y ..> two of its children X and Y are separated by a comma, then the last word in Y must be directly followed by a comma, or must be the last word in the sentence.",2,0,2,11,1996
In training data 96% of commas follow this rule.,2,0.8636363636363636,1,11,1996
The rule also has the benet of improving eciency by reducing the number of constituents in the chart.,2,1.131578947368421,2,19,1996
" The model we have described thus far takes the single best sequence of tags from the tagger, and it is clear that there is potential for better integration of the tagger and parser.",3,0.9142857142857143,2,35,1996
We have tried two modications.,1,0.9166666666666666,1,6,1996
"First, the current estimation methods treat occurrences of the same word with dierent POS tags as eectively distinct types.",2,0.6904761904761905,2,21,1996
Tags can be ignored when lexical information is available by dening  where T is the set of all tags.,1,1.25,2,20,1996
"Hence C (a, c) is the number of times that the words a and c occur in the same sentence, ignoring their tags.",3,0.875,4,28,1996
"The other denitions in (13) are similarly redened, with POS tags only being used when backing o from lexical information.",2,1.0,2,24,1996
This makes the parser less sensitive to tagging errors.,1,0.95,2,10,1996
"Second, for each word wi the tagger can provide the distribution of tag probabilities P (ti|S) (given the previous two words are tagged as in the best overall sequence of tags) rather than just the rst best tag.",4,0.717391304347826,3,46,1996
"The score for a parse in equation (20) then has an additional term, Qn i=1 P (ti|S), the product of probabilities of the tags which it contains.",3,1.0789473684210527,3,38,1996
Ideally we would like to integrate POS tagging into the parsing model rather than treating it as a separate stage.,2,0.9761904761904762,2,21,1996
This is an area for future research.,1,0.9375,1,8,1996
The parsing algorithm is a simple bottom-up chart parser.,2,0.45,3,10,1996
"There is no grammar as such, although in practice any dependency with a triple of nonterminals which has not been seen in training data will get zero probability.",2,1.05,4,30,1996
Thus the parser searches through the space of all trees with nonterminal triples seen in training data.,2,0.8611111111111112,2,18,1996
"Probabilities of baseNPs in the chart are calculated using (19), while probabilities for other constituents are derived from the dependencies and baseNPs that they contain.",2,1.0344827586206897,3,29,1996
A dynamic programming algorithm is used.,2,0.6428571428571429,1,7,1996
"if two proposed constituents span the same set of words, have the same label, head, and distance from  Table 3.",3,0.875,2,24,1996
Results on Section 23 of the WSJ Treebank.,2,0,2,9,1996
(1) is the basic model; (2) is the basic model with the punctuation rule described in section 2.7; (3) is model (2) with POS tags ignored when lexical information is present; (4) is model (3) with probability distributions from the POS tagger.,5,0.7931034482758621,3,58,1996
LR/LP = labeled recall/precision.,1,0.9,2,5,1996
CBs is the average number of crossing brackets per sentence.,2,1.1818181818181819,1,11,1996
"0 CBs,  2 CBs are the percentage of sentences with 0 or  2 crossing brackets respectively.",2,0.8611111111111112,2,18,1996
Figure 4.,1,1,1,3,1996
Diagram showing how two constituents join to form a new constituent.,1,1.0833333333333333,2,12,1996
Each operation gives two new probability terms.,2,0.6875,2,8,1996
"one for the baseNP gap tag between the two constituents, and the other for the dependency between the head words of the two constituents.",4,0,3,26,1996
"the head to the left and right end of the constituent, then the lower probability constituent can be safely discarded.",3,0.7045454545454546,3,22,1996
Figure 4 shows how constituents in the chart combine in a bottom-up manner.,2,0.9285714285714286,3,14,1996
4 Results  The parser was trained on sections 02 - 21 of the Wall Street Journal portion of the Penn Treebank (Marcus et al.,3,0.9038461538461539,2,26,1996
"93) (approximately 40,000 sentences), and tested on section 23 (2,416 sentences).",3,0,3,18,1996
For comparison SPATTER (Magerman 95; Jelinek et al.,2,1,2,11,1996
94) was also tested on section 23.,1,0.8333333333333334,1,9,1996
We use the PARSEVAL measures (Black et al.,1,1.05,2,10,1996
91) to compare performance.,1,1.3333333333333333,1,6,1996
"For a constituent to be correct it must span the same set of words (ignoring punctuation, i.e.",1,1.0526315789473684,2,19,1996
"all tokens tagged as commas, colons or quotes) and have the same label10 as a constituent in the treebank 10SPATTER collapses ADVP and PRT to the same label, for comparison we also removed this distinction when  Table 4.",4,0.9047619047619048,3,42,1996
The contribution of various components of the model.,2,0,1,9,1996
The results are for all sentences of  100 words in section 23 using model (3).,2,1.0,2,18,1996
For no lexical information all estimates are based on POS tags alone.,2,0.7307692307692307,2,13,1996
For no distance measure the distance measure is Question 1 alone (i.e.,1,1.0769230769230769,2,13,1996
whether wj precedes or follows whj ).,2,0.5625,2,8,1996
parse.,0,1.25,0,2,1996
Four congurations of the parser were tested.,2,0.9375,1,8,1996
"(1) The basic model; (2) The basic model with the punctuation rule described in section 2.7; (3) Model (2) with tags ignored when lexical information is present, as described in 2.7; and (4) Model (3) also using the full probability distributions for POS tags.",3,0.7377049180327869,4,61,1996
"We should emphasise that test data outside of section 23 was used for all development of the model, avoiding the danger of implicit training on section 23.",2,0.9482758620689655,2,29,1996
Table 3 shows the results of the tests.,1,0.8333333333333334,1,9,1996
Table 4 shows results which indicate how dierent parts of the system contribute to performance.,2,1.15625,2,16,1996
"All tests were made on a Sun SPARCServer 1000E, using 100% of a 60Mhz SuperSPARC processor.",2,0.6842105263157895,2,19,1996
"The parser uses around 180 megabytes of memory, and training on 40,000 sentences (essentially extracting the co-occurrence counts from the corpus) takes under 15 minutes.",3,0.8793103448275862,4,29,1996
Loading the hash table of bigram counts into memory takes approximately 8 minutes.,2,0.9642857142857143,2,14,1996
Two strategies are employed to improve parsing eciency.,1,1.0,1,9,1996
"First, a constant probability threshold is used while building the chart  any constituents with lower probability than this threshold are discarded.",2,0.7608695652173914,3,23,1996
"If a parse is found, it must be the highest ranked parse by the model.",2,1.088235294117647,2,17,1996
"If no parse is found, the threshold is lowered and parsing is attempted again.",4,0.78125,2,16,1996
The process continues until a parse is found.,1,1.0,2,9,1996
"Second, a beam search strategy is used.",2,0.6111111111111112,2,9,1996
"For each span of words in the sentence the probability, Ph, of the highest probability constituent is recorded.",3,0.8333333333333334,3,21,1996
All other constituents spanning the same words must have probability greater than Ph  for some constant beam size   constituents which fall out of this beam are discarded.,3,1.0,3,28,1996
"The method risks introducing search-errors, but in practice eciency can be greatly improved with virtually no loss of accuracy.",3,0,2,21,1996
Table 5 shows the trade-o between speed and accuracy as the beam is narrowed.,2,0.8666666666666667,3,15,1996
Table 5.,1,1,1,3,1996
The trade-o between speed and accuracy as the beam-size is varied.,2,0.875,2,12,1996
Model (3) was used for this test on all sentences  100 words in section 23.,2,0.9166666666666666,2,18,1996
We have shown that a simple statistical model based on dependencies between words can parse Wall Street Journal news text with high accuracy.,2,0.875,3,24,1996
The method is equally applicable to tree or dependency representations of syntactic structures.,2,0.6785714285714286,1,14,1996
"There are many possibilities for improvement, which is encouraging.",1,1.2727272727272727,1,11,1996
More sophisticated estimation techniques such as deleted interpolation should be tried.,2,1.0,1,12,1996
Estimates based on relaxing the distance measure could also be used for smoothing  at present we only back-o on words.,2,1.2857142857142858,2,21,1996
"The distance measure could be extended to capture more context, such as other words or tags in the sentence.",2,0.7619047619047619,2,21,1996
"Finally, the model makes no account of valency.",2,0.85,1,10,1996
"I would like to thank Mitch Marcus, Jason Eisner, Dan Melamed and Adwait Ratnaparkhi for many useful discussions, and for comments on earlier versions of this paper.",3,0.7741935483870968,4,31,1996
I would also like to thank David Magerman for his help with testing SPATTER.,2,1.0,2,15,1996
"Three Generative, Lexicalised Models for Statistical Parsing  In this paper we rst propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar.",3,0.8833333333333333,2,30,1997
We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement.,2,0.9375,2,16,1997
"Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).",3,0.7419354838709677,3,31,1997
Generative models of syntax have been central in linguistics since they were introduced in (Chomsky 57).,2,1.0,2,19,1997
"Each sentence-tree pair (S, T ) in a language has an associated top-down derivation consisting of a sequence of rule applications of a grammar.",3,0.8333333333333334,2,27,1997
"These models can be extended to be statistical by dening probability distributions at points of non-determinism in the derivations, thereby assigning a probability P(S, T ) to each (S, T ) pair.",3,1.0,3,39,1997
Probabilistic context free grammar (Booth and Thompson 73) was an early example of a statistical grammar.,3,0.6578947368421053,2,19,1997
"A PCFG can be lexicalised by associating a headword with each non-terminal in a parse tree; thus far, (Magerman 95; Jelinek et al.",4,0.8571428571428571,3,28,1997
"94) and (Collins 96), which both make heavy use of lexical information, have reported the best statistical parsing performance on Wall Street Journal text.",4,0.7333333333333333,3,30,1997
"Neither of these models is generative, instead they both estimate P(T | S) directly.",3,0.868421052631579,2,19,1997
This paper proposes three new parsing models.,2,0.5625,2,8,1997
Model 1 is essentially a generative version of the model described in (Collins 96).,2,0.7941176470588235,2,17,1997
"In Model 2, we extend the parser to make the complement/adjunct distinction by adding probabilities over subcategorisation frames for head-words.",2,0.9318181818181818,2,22,1997
"In Model 3 we give a probabilistic treatment of wh-movement, which  is derived from the analysis given in Generalized Phrase Structure Grammar (Gazdar et al.",2,0.9642857142857143,2,28,1997
95).,1,0,1,3,1997
The work makes two advances over previous models.,1,0.8333333333333334,1,9,1997
"First, Model 1 performs signicantly better than (Collins 96), and Models 2 and 3 give further improvements  our nal results are 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).",4,0.6590909090909091,4,44,1997
"Second, the parsers in (Collins 96) and (Magerman 95; Jelinek et al.",3,0,2,18,1997
94) produce trees without information about whmovement or subcategorisation.,2,0.8636363636363636,1,11,1997
Most NLP applications will need this information to extract predicateargument structure from parse trees.,2,0.7,2,15,1997
"In the remainder of this paper we describe the 3 models in section 2, discuss practical issues in section 3, give results in section 4, and give conclusions in section 5.",5,0.7857142857142857,3,35,1997
"In general, a statistical parsing model denes the conditional probability, P(T | S), for each candidate parse tree T for a sentence S. The parser itself is an algorithm which searches for the tree, Tbest, that maximises P(T | S).",3,0.55,3,30,1997
"A generative model uses the observation that maximising P(T, S) is equivalent to maximising P(T | S).",2,0.96,3,25,1997
"P(T, S) is then estimated by attaching probabilities to a top-down derivation of the tree.",2,1.0,2,20,1997
"In a PCFG, for a tree derived by n applications of context-free re-write rules LHSi  RHSi, 1  i  n.  Figure 1.",3,0,4,24,1997
"A lexicalised parse tree, and a list of the rules it contains.",4,0.6785714285714286,2,14,1997
For brevity we omit the POS tag associated with each word.,2,0.875,2,12,1997
"of one or more non-terminals; or lexical, where LHS is a part of speech tag and RHS is a word.",4,0.782608695652174,3,23,1997
A PCFG can be lexicalised2 by associating a word w and a part-of-speech (POS) tag t with each nonterminal X in the tree.,2,0.8076923076923077,3,26,1997
"Thus we write a nonterminal as X(x), where x = hw, ti, and X is a constituent label.",3,0.8,3,25,1997
Each rule now has the form3.,2,0.7857142857142857,1,7,1997
"H is the head-child of the phrase, which inherits the head-word h from its parent P .",2,0.8888888888888888,2,18,1997
"L1...Ln and R1...Rm are left and right modiers of H. Either n or m may be zero, and n = m = 0 for unary rules.",3,0.9375,1,32,1997
Figure 1 shows a tree which will be used as an example throughout this paper.,2,1.0625,2,16,1997
"The addition of lexical heads leads to an enormous number of potential rules, making direct estimation of P(RHS | LHS) infeasible because of sparse data problems.",3,0.9032258064516129,3,31,1997
"We decompose the generation of the RHS of a rule such as (3), given the LHS, into three steps  rst generating the head, then making the independence assumptions that the left and right modiers are generated by separate 0th-order markov processes4.",3,0.8297872340425532,5,47,1997
1.,0,1.25,0,2,1997
"Generate the head constituent label of the  phrase, with probability PH (H | P, h).",3,0.675,3,20,1997
2.,0,1.25,0,2,1997
"Generate modiers to the right of the head with probability Qi=1..m+1 PR(Ri(ri) | P, h, H).",2,1.0357142857142858,2,14,1997
"Rm+1(rm+1) is dened as ST OP  the ST OP symbol is added to the vocabulary of nonterminals, and the model stops generating right modiers when it is generated.",4,0.9,3,35,1997
2We nd lexical heads in Penn treebank data using rules which are similar to those used by (Magerman 95; Jelinek et al.,4,1.02,2,25,1997
94).,1,0,1,3,1997
"3With the exception of the top rule in the tree, which  has the form TOP  H(h).",2,0.9047619047619048,2,21,1997
"4An exception is the rst rule in the tree, TOP   H(h), which has probability PT OP (H, h|T OP )  3.",3,0.7741935483870968,3,31,1997
"Generate modiers to the left of the head with probability Qi=1..n+1 PL(Li(li)|P, h, H), where Ln+1(ln+1) = ST OP .",2,1.0357142857142858,2,14,1997
"For example, the probability of the rule S(bought) -> NP(week) NP(Marks) VP(bought) would be estimated as  Ph(VP | S,bought)  Pl(NP(Marks) | S,VP,bought)  Pl(NP(week) | S,VP,bought)  Pl(STOP | S,VP,bought)   Pr(STOP | S,VP,bought)  We have made the 0th order markov assumptions  but in general the probabilities could be conditioned on any of the preceding modiers.",9,0.883177570093458,2,107,1997
"In fact, if the derivation order is xed to be depth-rst  that is, each modier recursively generates the sub-tree below it before the next modier is generated  then the model can also condition on any structure below the preceding modiers.",4,1.0,2,43,1997
For the moment we exploit this by making the approximations  where distancel and distancer are functions of the surface string from the head word to the edge of the constituent (see gure 2).,2,0.9166666666666666,3,36,1997
"The distance measure is the same as in (Collins 96), a vector with the following 3 elements.",3,0.5952380952380952,2,21,1997
(1) is the string of zero length?,2,0.75,1,10,1997
(Allowing the model to learn a preference for rightbranching structures); (2) does the string contain a verb?,3,1.065217391304348,3,23,1997
(Allowing the model to learn a preference for modication of the most recent verb).,4,1.0,3,17,1997
"(3) Does the string contain 0, 1, 2 or > 2 commas?",3,0.6764705882352942,2,17,1997
"(where a comma is anything tagged as , or .).",2,1.1666666666666667,2,12,1997
Figure 2.,1,1,1,3,1997
"The next child, R3(r3), is generated with probability P(R3(r3) | P, H, h, distancer(2)).",4,0.828125,3,32,1997
"The distance is a function of the surface string from the word after h to the last word of R2, inclusive.",4,0.8478260869565217,3,23,1997
"In principle the model could condition on any structure dominated by H, R1 or R2.",2,0.7941176470588235,2,17,1997
The tree in gure 1 is an example of the importance of the complement/adjunct distinction.,2,0.84375,2,16,1997
"It would be useful to identify Marks as a subject, and Last week as an adjunct (temporal modier), but this distinction is not made in the tree, as both NPs are in the same position5 (sisters to a VP under an S node).",3,0.7843137254901961,3,51,1997
From here on we will identify complements by attaching a -C sux to non-terminals  gure 3 gives an example tree.,3,1.2727272727272727,3,22,1997
Figure 3.,1,1,1,3,1997
A tree with the -C sux used to identify complements.,2,1.1666666666666667,2,12,1997
Marks and Brooks are in subject and object position respectively.,2,0.8636363636363636,2,11,1997
Last week is an adjunct.,1,0.75,1,6,1997
"A post-processing stage could add this detail to the parser output, but we give two reasons for making the distinction while parsing.",3,0.6458333333333334,2,24,1997
"First, identifying complements is complex enough to warrant a probabilistic treatment.",2,0.8461538461538461,2,13,1997
"Lexical information is needed  5Except Marks is closer to the VP, but note that Marks is also the subject in Marks last week bought Brooks.",4,1.0555555555555556,3,27,1997
" for example, knowledge that week is likely to be a temporal modier.",2,1.0,2,14,1997
Knowledge about subcategorisation preferences  for example that a verb takes exactly one subject  is also required.,2,1.2058823529411764,2,17,1997
"These problems are not restricted to NPs, compare The spokeswoman said (SBAR that the asbestos was dangerous) vs. Bonds beat short-term investments (SBAR because the market is down), where an SBAR headed by that is a complement, but an SBAR headed by because is an adjunct.",6,0.9814814814814815,2,54,1997
The second reason for making the complement/adjunct distinction while parsing is that it may help parsing accuracy.,2,1.1111111111111112,3,18,1997
The assumption that complements are generated independently of each other often leads to incorrect parses  see gure 4 for further explanation.,3,1.0,2,22,1997
Identifying Complements and Adjuncts in the Penn Treebank.,2,0,2,9,1997
We add the -C sux to all non-terminals in training data which satisfy the following conditions.,2,1.0,3,18,1997
1.,0,1.25,0,2,1997
The non-terminal must be.,1,0.9,1,5,1997
"(1) an NP, SBAR, or S whose parent is an S; (2) an NP, SBAR, S, or VP whose parent is a VP; or (3) an S whose parent is an SBAR.",5,0.6847826086956522,2,46,1997
2.,0,1.25,0,2,1997
The non-terminal must not have one of the following semantic tags.,2,0.7083333333333334,2,12,1997
"ADV, VOC, BNF, DIR, EXT, LOC, MNR, TMP, CLR or PRP.",7,0,2,20,1997
See (Marcus et al.,2,0,2,6,1997
94) for an explanation of what these tags signify.,2,0.9545454545454546,2,11,1997
"For example, the NP Last week in gure 1 would have the TMP (temporal) tag; and the SBAR in (SBAR because the market is down), would have the ADV (adverbial) tag.",4,0.7317073170731707,3,41,1997
"In addition, the rst child following the head of a prepositional phrase is marked as a complement.",2,0.7631578947368421,3,19,1997
"The model could be retrained on training data with the enhanced set of non-terminals, and it might learn the lexical properties which distinguish complements and adjuncts (Marks vs week, or that vs. because).",4,0.8947368421052632,5,38,1997
"However, it would still suer from the bad independence assumptions illustrated in gure 4.",2,1.0,2,16,1997
"To solve these kinds of problems, the generative process is extended to include a probabilistic choice of left and right subcategorisation frames.",3,0.7291666666666666,2,24,1997
Figure 4.,1,1,1,3,1997
Two examples where the assumption that modiers are generated independently of each other leads to errors.,2,1.1176470588235294,2,17,1997
Each subcat frame is a multiset6 specifying the complements which the head requires in its left or right modiers.,2,0.975,2,20,1997
3.,0,1.25,0,2,1997
"Generate the left and right modiers with probabilities Pl(Li, li | H, P, h, distancel(i  1), LC) and Pr(Ri, ri | H, P, h, distancer(i  1), RC) respectively.",7,0.6176470588235294,5,51,1997
Thus the subcat requirements are added to the conditioning context.,2,0.6818181818181818,2,11,1997
As complements are generated they are removed from the appropriate subcat multiset.,2,1.1153846153846154,2,13,1997
"Most importantly, the probability of generating the ST OP symbol will be 0 when the subcat frame is non-empty, and the probability of generating a complement will be 0 when it is not in the subcat frame; thus all and only the required complements will be generated.",5,0.8725490196078431,3,51,1997
Another obstacle to extracting predicate-argument structure from parse trees is wh-movement.,2,1.0,2,12,1997
This section describes a probabilistic treatment of extraction from relative clauses.,2,0.7916666666666666,2,12,1997
"Noun phrases are most often extracted from subject position, object position, or from within PPs.",3,0.75,2,18,1997
"Here the head initially decides to take a single NP-C (subject) to its left, and no complements 6A multiset, or bag, is a set which may contain du plicate non-terminal labels.",4,0.8648648648648649,3,37,1997
It might be possible to write rule-based patterns which identify traces in a parse tree.,1,1.21875,2,16,1997
"However, we argue again that this task is best integrated into the parser.",2,1.0,2,15,1997
"the task is complex enough to warrant a probabilistic treatment, and integration may help parsing accuracy.",3,0.7777777777777778,2,18,1997
"A couple of complexities are that modication by an SBAR does not always involve extraction (e.g., the fact (SBAR that besoboru is  Figure 5.",2,1.1607142857142858,3,28,1997
A +gap feature can be added to non-terminals to describe NP extraction.,2,1.0714285714285714,2,14,1997
"The top-level NP initially generates an SBAR modier, but species that it must contain an NP trace by adding the +gap feature.",2,0.86,3,25,1997
"The gap is then passed down through the tree, until it is discharged as a T RACE complement to the right of bought.",2,0.9,2,25,1997
"played with a ball and a bat)), and it is not uncommon for extraction to occur through several constituents, (e.g., The changes (SBAR that he said the government was prepared to make TRACE)).",3,1.0813953488372092,1,43,1997
The second reason for an integrated treatment of traces is to improve the parameterisation of the model.,2,0.9444444444444444,2,18,1997
"In particular, the subcategorisation probabilities are smeared by extraction.",2,0.7727272727272727,2,11,1997
"In examples 1, 2 and 3 above bought is a transitive verb, but without knowledge of traces example 2 in training data will contribute to the probability of bought being an intransitive verb.",4,0.9861111111111112,2,36,1997
Formalisms similar to GPSG (Gazdar et al.,2,1,2,9,1997
"95) handle NP extraction by adding a gap feature to each non-terminal in the tree, and propagating gaps through the tree until they are nally discharged as a trace complement (see gure 5).",3,0.868421052631579,3,38,1997
"In extraction cases the Penn treebank annotation co-indexes a TRACE with the WHNP head of the SBAR, so it is straightforward to add this information to trees in training data.",4,0.78125,1,32,1997
"Given that the LHS of the rule has a gap, there are 3 ways that the gap can be passed down to the RHS.",3,0.9423076923076923,3,26,1997
"Head The gap is passed to the head of the phrase,  as in rule (3) in gure 5.",2,0.9545454545454546,3,22,1997
"Left, Right The gap is passed on recursively to one of the left or right modiers of the head, or is discharged as a trace argument to the left/right of the head.",3,0,3,35,1997
"In rule (2) it is passed on to a right modier, the S complement.",2,0.8055555555555556,2,18,1997
In rule (4) a trace is generated to the right of the head VB.,2,0.8529411764705882,2,17,1997
"We specify a parameter PG(G | P, h, H) where G is either Head, Left or Right.",3,0.875,3,24,1997
The generative process is extended to choose between these cases after generating the head of the phrase.,2,0.9722222222222222,2,18,1997
The rest of the phrase is then generated in dierent ways depending on how the gap is propagated.,2,1.0,2,19,1997
In the Head case the left and right modiers are generated as normal.,2,0.6785714285714286,2,14,1997
"In the Left, Right cases a gap requirement is added to either the left or right SUBCAT variable.",3,0.625,3,20,1997
This requirement is fullled (and removed from the subcat list) when a trace or a modier non-terminal which has the +gap feature is generated.,3,0.9107142857142857,4,28,1997
Figure 6.,1,1,1,3,1997
The life of a constituent in the chart.,2,0,1,9,1997
(+) means a constituent is complete (i.e.,1,1.1,2,10,1997
"it includes the stop probabilities), () means a constituent is incomplete.",3,0.9,2,15,1997
(a) a new constituent is started by projecting a complete rule upwards; (b) the constituent then takes left and right modiers (or none if it is unary).,3,0.6857142857142857,2,35,1997
"(c) nally, ST OP probabilities are added to complete the constituent.",3,0.8,2,15,1997
Part of speech tags are generated along with the words in this model.,2,0.8928571428571429,1,14,1997
"When parsing, the POS tags allowed for each word are limited to those which have been seen in training data for that word.",2,1.08,2,25,1997
"For unknown words, the output from the tagger described in (Ratnaparkhi 96) is used as the single possible tag for that word.",3,0.75,2,26,1997
A CKY style dynamic programming chart parser is used to nd the maximum probability tree for each sentence (see gure 6).,3,0.6458333333333334,2,24,1997
The parser was trained on sections 02 - 21 of the Wall Street Journal portion of the Penn Treebank (Marcus et al.,3,0.9375,3,24,1997
"93) (approximately 40,000 sentences), and tested on section 23 (2,416 sentences).",3,0,3,18,1997
We use the PARSEVAL measures (Black et al.,1,1.05,2,10,1997
91) to compare performance.,1,1.3333333333333333,1,6,1997
Table 2.,1,1,1,3,1997
Results on Section 23 of the WSJ Treebank.,2,0,2,9,1997
LR/LP = labeled recall/precision.,1,0.9,2,5,1997
CBs is the average number of crossing brackets per sentence.,2,1.1818181818181819,1,11,1997
"0 CBs,  2 CBs are the percentage of sentences with 0 or  2 crossing brackets respectively.",2,0.8611111111111112,2,18,1997
"For a constituent to be correct it must span the same set of words (ignoring punctuation, i.e.",1,1.0526315789473684,2,19,1997
"all tokens tagged as commas, colons or quotes) and have the same label8 as a constituent in the treebank parse.",3,0.7608695652173914,3,23,1997
"Table 2 shows the results for Models 1, 2 and 3.",2,0.6538461538461539,2,13,1997
"The precision/recall of the traces found by Model 3 was 93.3%/90.1% (out of 436 cases in section 23 of the treebank), where three criteria must be met for a trace to be correct.",3,0.9634146341463414,2,41,1997
(1) it must be an argument to the correct head-word; (2) it must be in the correct position in relation to that head word (preceding or following); (3) it must be dominated by the correct non-terminal label.,4,0,2,48,1997
"For example, in gure 5 the trace is an argument to bought, which it follows, and it is dominated by a VP.",4,0.8653846153846154,2,26,1997
"Of the 436 cases, 342 were string-vacuous extraction from subject position, recovered with 97.1%/98.2% precision/recall; and 94 were longer distance cases, recovered with 76%/60.6% precision/recall 9.",3,0.8205128205128205,3,39,1997
"Model 1 is similar in structure to (Collins 96)  the major dierences being that the score for each bigram dependency is Pl(Li, li|H, P, h, distancel)  8(Magerman 95) collapses ADVP and PRT to the same label, for comparison we also removed this distinction when calculating scores.",3,0.8306451612903226,2,62,1997
"9We exclude innitival relative clauses from these gures, for example I called a plumber TRACE to x the sink where plumber is co-indexed with the trace subject of the innitival.",2,1.0625,3,32,1997
"The algorithm scored 41%/18% precision/recall on the 60 cases in section 23  but innitival relatives are extremely dicult even for human annotators to distinguish from purpose clauses (in this case, the innitival could be a purpose clause modifying called) (Ann Taylor, p.c.)",2,0.875,4,52,1997
"rather than Pl(Li, P, H | li, h, distancel), and that there are the additional probabilities of generating the head and the ST OP symbols for each constituent.",4,0.7027027027027027,3,37,1997
"However, Model 1 has some advantages which may account for the improved performance.",2,0.9333333333333333,2,15,1997
"The model in (Collins 96) is decient, that is for most sentences S, PT P(T | S) < 1, because probability mass is lost to dependency structures which violate the hard constraint that no links may cross.",3,0.967391304347826,3,46,1997
"For reasons we do not have space to describe here, Model 1 has advantages in its treatment of unary rules and the distance measure.",3,0.8269230769230769,2,26,1997
The generative model can condition on any structure that has been previously generated  we exploit this in models 2 and 3  whereas (Collins 96) is restricted to conditioning on features of the surface string alone.,2,1.0263157894736843,2,38,1997
"(Charniak 95) also uses a lexicalised generaIn our notation, he decomposes tive model.",4,0.6764705882352942,2,17,1997
"P(RHSi | LHSi) as P(Rn...R1HL1..Lm | P, h)  Qi=1..n P(ri | P, Ri, h)  Qi=1..m P(li | P, Li, h).",3,1,2,13,1997
"The Penn treebank annotation style leads to a very large number of context-free rules, so that directly estimating P(Rn...R1HL1..Lm | P, h) may lead to sparse data problems, or problems with coverage (a rule which has never been seen in training may be required for a test data sentence).",3,0.74,3,25,1997
"The complement/adjunct distinction and traces increase the number of rules, compounding this problem.",3,0.8666666666666667,2,15,1997
"(Eisner 96) proposes 3 dependency models, and gives results that show that a generative model similar to Model 1 performs best of the three.",3,0.9821428571428571,3,28,1997
"However, a pure dependency model omits non-terminal information, which is important.",2,0.8571428571428571,2,14,1997
"(Alshawi 96) extends a generative dependency model to include an additional state variable which is equivalent to having non-terminals  his suggestions may be close to our models 1 and 2, but he does not fully specify the details of his model, and doesnt give results for parsing accuracy.",4,0.8773584905660378,4,53,1997
(Miller et al.,1,0.9,1,5,1997
"96) describe a model where the RHS of a rule is generated by a Markov process, although the process is not head-centered.",3,0.86,4,25,1997
They increase the set of non-terminals by adding semantic labels rather than by adding lexical head-words.,2,1.088235294117647,2,17,1997
(Magerman 95; Jelinek et al.,2,0,2,8,1997
94) describe a history-based approach which uses decision trees to estimate P(T |S).,2,1.0789473684210527,3,19,1997
"Our models use much less sophisticated n-gram estimation methods, and might well benet from methods such as decision-tree estimation which could condition on richer history than just surface distance.",3,0.8709677419354839,3,31,1997
"There has  recently been interest  in using dependency-based parsing models in speech recognition, for example (Stolcke 96).",2,0.9047619047619048,3,21,1997
"It is interesting to note that Models 1, 2 or 3 could be used as language models.",2,1.0263157894736843,3,19,1997
"The probability for any sentence can be estimated as P(S) = PT P(T, S), or (making a Viterbi approximation for eciency reasons) as P(S)  P(Tbest, S).",4,0.8863636363636364,3,44,1997
"We intend to perform experiments to compare the perplexity of the various models, and a structurally similar pure PCFG10.",3,0.8809523809523809,3,21,1997
"This paper has proposed a generative, lexicalised, probabilistic parsing model.",3,0.4230769230769231,3,13,1997
"We have shown that linguistically fundamental ideas, namely subcategorisation and wh-movement, can be given a statistical interpretation.",3,0.75,3,20,1997
"This improves parsing performance, and, more importantly, adds useful information to the parsers output.",2,0,2,18,1997
"I would like to thank Mitch Marcus, Jason Eisner, Dan Melamed and Adwait Ratnaparkhi for many useful discussions, and comments on earlier versions of this paper.",3,0.8,4,30,1997
This work has also beneted greatly from suggestions and advice from Scott Miller.,2,0.75,2,14,1997
Ranking Algorithms for NamedEntity Extraction.,1,0,2,6,2002
"Boosting and the Voted Perceptron  This paper describes algorithms which rerank the top N hypotheses from a maximum-entropy tagger, the application being the recovery of named-entity boundaries in a corpus of web data.",3,0.8714285714285714,3,35,2002
The rst approach uses a boosting algorithm for ranking problems.,2,0.6818181818181818,1,11,2002
The second approach uses the voted perceptron algorithm.,2,0.5,2,9,2002
"Both algorithms give comparable, signicant improvements over the maximum-entropy baseline.",2,0.625,2,12,2002
"The voted perceptron algorithm can be considerably more efcient to train, at some cost in computation on test examples.",2,0.9761904761904762,2,21,2002
Recent work in statistical approaches to parsing and tagging has begun to consider methods which incorporate global features of candidate structures.,2,1.1136363636363635,2,22,2002
Examples of such techniques are Markov Random Fields (Abney 1997; Della Pietra et al.,3,0.9117647058823529,2,17,2002
1997; Johnson et al.,1,0,1,6,2002
"1999), and boosting algorithms (Freund et al.",2,0.7727272727272727,2,11,2002
1998; Collins 2000; Walker et al.,2,0,2,9,2002
2001).,1,0,1,3,2002
One appeal of these methods is their exibility in incorporating features into a model.,2,1.0,1,15,2002
essentially any features which might be useful in discriminating good from bad structures can be included.,2,1.1470588235294117,2,17,2002
"A second appeal of these methods is that their training criterion is often discriminative, attempting to explicitly push the score or probability of the correct structure for each training sentence above the score of competing structures.",2,0.868421052631579,2,38,2002
This discriminative property is shared by the methods of (Johnson et al.,2,0.8928571428571429,2,14,2002
"1999; Collins 2000), and also the Conditional Random Field methods of (Lafferty et al.",3,0,3,19,2002
2001).,1,0,1,3,2002
"In a previous paper (Collins 2000), a boosting algorithm was used to rerank the output from an ex isting statistical parser, giving signicant improvements in parsing accuracy on Wall Street Journal data.",3,0.6891891891891891,3,37,2002
"Similar boosting algorithms have been applied to natural language generation, with good results, in (Walker et al.",2,0.8809523809523809,3,21,2002
2001).,1,0,1,3,2002
In this paper we apply reranking methods to named-entity extraction.,2,0.7727272727272727,2,11,2002
"A state-ofthe-art (maximum-entropy) tagger is used to generate 20 possible segmentations for each input sentence, along with their probabilities.",2,0.8260869565217391,3,23,2002
We describe a number of additional global features of these candidate segmentations.,2,0.8076923076923077,2,13,2002
These additional features are used as evidence in reranking the hypotheses from the max-ent tagger.,2,0.8125,2,16,2002
We describe two learning algorithms.,1,1.3333333333333333,1,6,2002
"the boosting method of (Collins 2000), and a variant of the voted perceptron algorithm, which was initially described in (Freund & Schapire 1999).",4,0.8166666666666667,4,30,2002
We applied the methods to a corpus of over one million words of tagged web data.,2,0.7941176470588235,2,17,2002
"The methods give significant improvements over the maximum-entropy tagger (a 17.7% relative reduction in error-rate for the voted perceptron, and a 15.6% relative improvement for the boosting method).",4,0.6617647058823529,4,34,2002
"One contribution of this paper is to show that existing reranking methods are useful for a new domain, named-entity tagging, and to suggest global features which give improvements on this task.",3,0.9705882352941176,3,34,2002
"We should stress that another contribution is to show that a new algorithm, the voted perceptron, gives very credible results on a natural language task.",2,1.0178571428571428,3,28,2002
"It is an extremely simple algorithm to implement, and is very fast to train (the testing phase is slower, but by no means sluggish).",3,0.8448275862068966,3,29,2002
It should be a viable alternative to methods such as the boosting or Markov Random Field algorithms described in previous work.,3,0.7045454545454546,2,22,2002
Over a period of a year or so we have had over one million words of named-entity data annotated.,2,0.925,2,20,2002
"The  Data is drawn from web pages, the aim being to support a question-answering system over web data.",2,0.85,2,20,2002
A number of categories are annotated.,2,1.0714285714285714,1,7,2002
"the usual people, organization and location categories, as well as less frequent categories such as brand-names, scientic terms, event titles (such as concerts) and so on.",4,0,3,33,2002
"From this data we created a training set of 53,609 sentences (1,047,491 words), and a test set of 14,717 sentences (291,898 words).",3,0.7321428571428571,3,28,2002
The task we consider is to recover named-entity boundaries.,2,1.35,2,10,2002
We leave the recovery of the categories of entities to a separate stage of processing.1 We evaluate different methods on the task through pre cision and recall.,3,0,2,29,2002
"The problem can be framed as a tagging task  to tag each word as being either the start of an entity, a continuation of an entity, or not to be part of an entity at all (we will use the tags S, C and N respectively for these three cases).",4,0.8571428571428571,4,56,2002
"As a baseline model we used a maximum entropy tagger, very similar to the ones described in (Ratnaparkhi 1996; Borthwick et.",3,0.78,3,25,2002
al 1998; McCallum et al.,2,0,1,7,2002
2000).,1,0,1,3,2002
"Max-ent taggers have been shown to be highly competitive on a number of tagging tasks, such as part-of-speech tagging (Ratnaparkhi 1996), named-entity recognition (Borthwick et.",2,0.8709677419354839,3,31,2002
"al 1998), and information extraction tasks (McCallum et al.",2,0,2,13,2002
2000).,1,0,1,3,2002
Thus the maximumentropy tagger we used represents a serious baseline for the task.,2,1.0,2,14,2002
We used the following features (several of the features were inspired by the approach of (Bikel et.,1,1.0,2,20,2002
"al 1999), an HMM model which gives excellent results on named entity extraction).",3,0.8823529411764706,3,17,2002
"1In initial experiments, we found that forcing the tagger to recover categories as well as the segmentation, by exploding the number of tags, reduced performance on the segmentation task, presumably due to sparse data problems.",4,0.8625,3,40,2002
The parameters are trained using Generalized Iterative Scaling.,1,0.8888888888888888,1,9,2002
"Following (Ratnaparkhi 1996), we only include features which occur 5 times or more in training data.",3,0.825,2,20,2002
"In decoding, we use a beam search to recover 20 candidate tag sequences for each sentence (the sentence is decoded from left to right, with the top 20 most probable hypotheses being stored at each point).",3,0.7804878048780488,3,41,2002
"As a baseline we trained a model on the full 53,609 sentences of training data, and decoded the 14,717 sentences of test data.",3,0.7,3,25,2002
"This gave 20 candidates per  test sentence, along with their probabilities.",2,0.8076923076923077,2,13,2002
"The baseline method is to take the most probable candidate for each test data sentence, and then to calculate precision and recall gures.",4,0.68,3,25,2002
"Our aim is to come up with strategies for reranking the test data candidates, in such a way that precision and recall is improved.",3,0.9230769230769231,3,26,2002
"the 53,609 sentences of training data were split into a 41,992 sentence training portion, and a 11,617 sentence development set.",3,0.5227272727272727,2,22,2002
"The training portion was split into 5 sections, and in each case the maximum-entropy tagger was trained on 4/5 of the data, then used to decode the remaining 1/5.",3,0.734375,3,32,2002
"The top 20 hypotheses under a beam search, together with their log probabilities, were recovered for each training sentence.",4,0.6590909090909091,2,22,2002
"In a similar way, a model trained on the 41,992 sentence set was used to produce 20 hypotheses for each sentence in the development set.",2,0.7777777777777778,3,27,2002
The module we describe in this section generates global features for each candidate tagged sequence.,2,1.21875,2,16,2002
"As input it takes a sentence, along with a proposed segmentation (i.e., an assignment of a tag for each word in the sentence).",2,0.8035714285714286,3,28,2002
"As output, it produces a set of feature strings.",2,0.8636363636363636,1,11,2002
We will use the following tagged sentence as a running example in this section.,2,0.9,2,15,2002
An example feature type is simply to list the full strings of entities that appear in the tagged input.,2,0.975,2,20,2002
"In this example, this would give the three features.",2,0.6818181818181818,2,11,2002
Here WE stands for whole entity.,1,0.9285714285714286,2,7,2002
"Throughout this section, we will write the features in this format.",2,0.7307692307692307,2,13,2002
"The start of the feature string indicates the feature type (in this case WE), followed by =.",3,0.7380952380952381,3,21,2002
"Following the type, there are generally 1 or more words or other .",3,0.75,2,14,2002
"symbols, which we will separate with the symbol implementation takes the strings produced by the global-feature  A seperate module in our  generator, and hashes them to integers.",3,0.8666666666666667,4,30,2002
"For example, suppose the three strings WE=Gen Xer, WE=The Day They Shot John Lennon, WE=Dougherty Arts Center were hashed to 100, 250, and 500 respectively.",5,0,2,37,2002
"Conceptually, is represented by a large number is the number of distinct feature strings in training data.",2,0.7631578947368421,2,19,2002
We now introduce some notation with which to describe the full set of global features.,2,1.0,2,16,2002
"First, we assume the following primitives of an input candidate.",2,0.7083333333333334,2,12,2002
sequence.,0,0,0,2,2002
Each character in the word is  character types are not repeated in the mapped string.,2,1.125,2,16,2002
"For example, Animal would be mapped to Aa in this feature, G.M.",1,0.9642857142857143,2,14,2002
would again be mapped to A.A..,1,1.0714285714285714,1,7,2002
The ag indicates whether or not the word appears in a dictionary of words which appeared more often lower-cased than capitalized in a large corpus of text.,2,1.0357142857142858,2,28,2002
Most of the features we describe are anchored on entity boundaries in the candidate segmentation.,2,1.125,2,16,2002
We will use feature templates to describe the features that we used.,2,1.1153846153846154,2,13,2002
"As an example, suppose that an entity.",2,0.7222222222222222,2,9,2002
Figure 1.,1,1,1,3,2002
The full set of entity-anchored feature templates.,2,0,2,8,2002
One of these features is generated for each entity  Applying this template to the three entities in the running example generates the three feature strings described in the previous section.,2,0.8064516129032258,3,31,2002
"For the full set of feature tem plates that are anchored around entities, see gure 1.",5,0.8611111111111112,2,18,2002
A second set of feature templates is anchored around quotation marks.,2,0.7916666666666666,1,12,2002
"In our corpus, entities (typically with long names) are often seen surrounded by quotes.",3,0.9444444444444444,3,18,2002
"For example, The Day They Shot John Lennon, the name of a band, appears in the running to be the index of any double quoto be the index of the next (matching) double quotation marks if they apto be the index of the last word beginning with a lower case letter, upper case letter, or digit within the quotation marks.",3,0.8602941176470589,3,68,2002
The rst set of feature templates tracks  example.,2,0.8333333333333334,1,9,2002
"At this point, we have fully described the representation used as input to the reranking algorithms.",2,0.8055555555555556,2,18,2002
The maximum-entropy tagger gives 20 proposed segmentations for each input sentence.,2,0.7083333333333334,2,12,2002
This section introduces notation for the reranking task.,1,0.8333333333333334,1,9,2002
The framework is derived by the transformation from ranking problems to a margin-based classication problem in (Freund et al.,2,0.8333333333333334,2,21,2002
1998).,1,0,1,3,2002
It is also related to the Markov Random Field methods for parsing suggested in (Johnson et al.,2,1.0526315789473684,2,19,2002
"1999), and the boosting methods for parsing in (Collins 2000).",2,0,2,15,2002
We consider the following set-up.,1,0.75,2,6,2002
correct sequence of tags for that sentence.,2,0,1,8,2002
pairs.,0,0,0,2,2002
In tagging we would have training examples  outputs from a maximum entropy tagger are used as the set of candidates.,2,1.0714285714285714,2,21,2002
The features could be arbitrary functions of the candidates; our hope is to include features which help in discriminating good candidates from bad ones.,2,1.0192307692307692,2,26,2002
"This function assigns a real-valued number to a canIt will be taken to be a measure of the plausibility of a candidate, higher scores meaning higher plausibility.",2,0.9827586206896551,2,29,2002
The rst algorithm we consider is the boosting algorithm for ranking described in (Collins 2000).,2,1.0,2,18,2002
The algorithm is a modication of the method in (Freund et al.,2,0.9642857142857143,2,14,2002
1998).,1,0,1,3,2002
"Intuitively, it is useful to note that this loss function is an upper bound on the number of ranking errors, a ranking error being a case where an incorrect candidate gets a higher value than a correct candidate.",2,0.8902439024390244,2,41,2002
Figure 2 shows an algorithm which implements this greedy procedure.,2,1.0,2,11,2002
"See (Collins 2000) for a full description of the method, including justication that the algorithm does in fact implement the update in Eq.",2,1.037037037037037,3,27,2002
1 at each iteration.,1,0,1,5,2002
"Figure 3 shows the training phase of the perceptron algorithm, originally introduced in (Rosenblatt 1958).",2,0.7105263157894737,3,19,2002
"The algorithm maintains a parameter vector , which is initially set to be all zeros.",2,1.03125,2,16,2002
"In this case the update is very simple, involving adding the difference of the offending examples representations in the gure).",3,0.9782608695652174,3,23,2002
"See (Cristianini and Shawe-Taylor 2000) chapter 2 for discussion of the perceptron algorithm, and theory justifying this method for setting the parameters.",4,0.8846153846153846,4,26,2002
"(Freund & Schapire 1999) describe a renement of the perceptron, the voted perceptron.",4,0,2,17,2002
The training phase is identical to that in gure 3.,2,0.8636363636363636,1,11,2002
"Note, how are stored.",1,1.3333333333333333,1,6,2002
Thus the training phase can be thought different parameter settings.,2,0.6818181818181818,2,11,2002
We applied the voted perceptron and boosting algorithms to the data described in section 2.3.,2,0,2,16,2002
Only features occurring on 5 or more distinct training sentences were included in the model.,3,0.75,2,16,2002
"This resulted 5Note that, for reasons of explication, the decoding algorithm we present is less efcient than necessary.",3,1.0714285714285714,4,21,2002
"The two methods were trained on the training portion (41,992 sentences) of the training set.",2,0.6944444444444444,2,18,2002
We used the development set to pick the best values for tunable parameters in each algorithm.,2,0.9411764705882353,2,17,2002
"For boosting, the main parameter to pick is .",3,1.05,2,10,2002
"We ran the algorithm for a total of 300,000 rounds, and found that the optimal value for F-measure on the development set occurred after 83,233 rounds.",3,0.8571428571428571,4,28,2002
Figure 5 shows the results for the three methods on the test set.,2,0.6785714285714286,2,14,2002
Both of the reranking algorithms show signicant improvements over the baseline.,2,0.875,2,12,2002
"a 15.6% relative reduction in error for boosting, and a 17.7% relative error reduction for the voted perceptron.",4,0.7045454545454546,3,22,2002
"In our experiments we found the voted perceptron algorithm to be considerably more efcient in training, at some cost in computation on test examples.",2,0.8076923076923077,2,26,2002
"Another attractive property of the voted perceptron is that it can be used with kernels, for example the kernels over parse trees described in (Collins and Duffy 2001; Collins and Duffy 2002).",3,0.9459459459459459,3,37,2002
"(Collins and Duffy 2002) describe the voted perceptron applied to the named-entity data in this paper, but using kernel-based features rather than the explicit features described in this paper.",4,0.6818181818181818,3,33,2002
"See (Collins 2002) for additional work using perceptron algorithms to train tagging models, and a more thorough description of the theory underlying the perceptron algorithm applied to ranking problems.",3,0.803030303030303,3,33,2002
"A question regarding the approaches in this paper is whether the features we have described could be incorporated in a maximum-entropy tagger, giving similar improvements in accuracy.",2,1.0344827586206897,3,29,2002
This section discusses why this is unlikely to be the case.,1,1.2083333333333333,2,12,2002
The problem described here is closely related to the label bias problem described in (Lafferty et al.,3,0.9736842105263158,2,19,2002
2001).,1,0,1,3,2002
One straightforward way to incorporate global features into the maximum-entropy model would be  to introduce new features whether the tagging decision which indicated in the history   cre ates a particular global feature.,2,0.8333333333333334,3,33,2002
"The decision which effectively created the entity University for was the decision to tag for as C, and this has already been made.",4,0.92,3,25,2002
The independence assumptions in maximum-entropy taggers of this form often lead points of local ambiguity (in this example the tag for the word for) to create globally implausible structures with unreasonably high scores.,3,1.0,2,36,2002
See (Collins 1999) section 8.4.2 for a discussion of this problem in the context of parsing.,2,1.105263157894737,2,19,2002
Acknowledgements Many thanks to Jack Minisi for annotating the named-entity data used in the exper iments.,2,0.8823529411764706,2,17,2002
"Thanks also to Nigel Duffy, Rob Schapire and Yoram Singer for several useful discussions.",3,0,3,16,2002
Head-Driven Statistical Models for Natural Language Parsing  Michael Collins MIT Computer Science and Articial Intelligence Laboratory  This article describes three statistical models for natural language parsing.,3,0.6481481481481481,1,27,2003
"The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree.",2,0.9078947368421053,3,38,2003
"Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment.",7,1.0147058823529411,2,34,2003
All of these preferences are expressed by probabilities conditioned on lexical heads.,2,1.0384615384615385,1,13,2003
"The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature.",3,0.78,3,25,2003
"To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies.",4,0.803030303030303,2,33,2003
"We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples.",4,0.896551724137931,3,29,2003
"Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.",2,1.09375,2,32,2003
Ambiguity is a central problem in natural language parsing.,2,0.75,2,10,2003
Combinatorial effects mean that even relatively short sentences can receive a considerable number of parses under a wide-coverage grammar.,2,0.75,3,20,2003
"Statistical parsing approaches tackle the ambiguity problem by assigning a probability to each parse tree, thereby ranking competing trees in order of plausibility.",2,0.9,2,25,2003
"In many statistical models the probability for each candidate tree is calculated as a product of terms, each term corresponding to some substructure within the tree.",2,0.8392857142857143,2,28,2003
The choice of parameterization is essentially the choice of how to represent parse trees.,2,1.1333333333333333,2,15,2003
There are two critical questions regarding the parameterization of a parsing approach.,2,0.8076923076923077,2,13,2003
1.,0,1.25,0,2,2003
"Which linguistic objects (e.g., context-free rules, parse moves) should the  models parameters be associated with?",3,1.125,2,20,2003
"In other words, which features should be used to discriminate among alternative parse trees?",2,1.03125,2,16,2003
How can this choice be instantiated in a sound probabilistic model?,2,0,2,12,2003
"In this article we explore these issues within the framework of generative models, more precisely, the history-based models originally introduced to parsing by Black.",5,0,2,27,2003
"In a history-based model, a parse tree is represented as a sequence of decisions, the decisions being made in some derivation of the tree.",2,0.7962962962962963,2,27,2003
"Each decision has an associated probability, and the product of these probabilities denes a probability distribution over possible derivations.",3,0,2,21,2003
We rst describe three parsing models based on this approach.,2,1.0,2,11,2003
The models were originally introduced in Collins (1997); the current article1 gives considerably more detail about the models and discusses them in greater depth.,3,0,2,28,2003
In Model 1 we show one approach that extends methods from probabilistic context-free grammars (PCFGs) to lexicalized grammars.,2,1.0,2,21,2003
"Most importantly, the model has parameters corresponding to dependencies between pairs of headwords.",2,1.0333333333333334,1,15,2003
"We also show how to incorporate a distance measure into these models, by generalizing the model to a history-based approach.",2,0.8863636363636364,3,22,2003
"The distance measure allows the model to learn a preference for close attachment, or right-branching structures.",3,0.7777777777777778,2,18,2003
"In Model 2, we extend the parser to make the complement/adjunct distinction, which will be important for most applications using the output from the parser.",2,0.9285714285714286,2,28,2003
Model 2 is also extended to have parameters corresponding directly to probability distributions over subcategorization frames for headwords.,2,1.105263157894737,2,19,2003
The new parameters lead to an improvement in accuracy.,2,0.85,1,10,2003
In Model 3 we give a probabilistic treatment of wh-movement that is loosely based on the analysis of wh-movement in generalized phrase structure grammar (GPSG) (Gazdar et al.,2,0.984375,3,32,2003
1985).,1,0,1,3,2003
The output of the parser is now enhanced to show trace coindexations in wh-movement cases.,2,0.9375,2,16,2003
"The parameters in this model are interesting in that they correspond directly to the probability of propagating GPSG-style slash features through parse trees, potentially allowing the model to learn island constraints.",2,1.121212121212121,3,33,2003
"In the three models a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree.",2,0.74,3,25,2003
"Independence assumptions then follow naturally, leading to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, lexical dependencies, wh-movement, and preferences for close attachment.",4,1.0,2,36,2003
All of these preferences are expressed by probabilities conditioned on lexical heads.,2,1.0384615384615385,1,13,2003
For this reason we refer to the models as head-driven statistical models.,2,0.8076923076923077,2,13,2003
"We describe evaluation of the three models on the Penn Wall Street Journal Treebank (Marcus, Santorini, and Marcinkiewicz 1993).",3,0.6875,2,24,2003
"Model 1 achieves 87.7% constituent precision and 87.5% consituent recall on sentences of up to 100 words in length in section 23 of the treebank, and Models 2 and 3 give further improvements to 88.3% constituent precision and 88.0% constituent recall.",5,0.6808510638297872,3,47,2003
These results are competitive with those of other models that have been applied to parsing the Penn Treebank.,2,1.0789473684210527,2,19,2003
Models 2 and 3 produce trees with information about wh-movement or subcategorization.,2,0,2,13,2003
Many NLP applications will need this information to extract predicate-argument structure from parse trees.,2,0.7,2,15,2003
The rest of the article is structured as follows.,2,1.1,1,10,2003
Section 2 gives background material on probabilistic context-free grammars and describes how rules can be lexicalized through the addition of headwords to parse trees.,2,1.1,4,25,2003
Section 3 introduces the three probabilistic models.,2,0.5625,2,8,2003
Section 4 describes various renments to these models.,2,0.7222222222222222,2,9,2003
"Section 5 discusses issues of parameter estimation, the treatment of unknown words, and also the parsing algorithm.",3,0.725,3,20,2003
"Section 6 gives results evaluating the performance of the models on the Penn Wall Street Journal Treebank (Marcus, Santorini, and Marcinkiewicz 1993).",3,0.7222222222222222,2,27,2003
Section 7 investigates various aspects of the models in more detail.,2,0.875,1,12,2003
"We give a  Head-Driven Statistical Models for NL Parsing  detailed analysis of the parsers performance on treebank data, including results on different constituent types.",3,0.7115384615384616,3,26,2003
We also give a breakdown of precision and recall results in recovering various types of dependencies.,2,1.0,2,17,2003
The intention is to give a better idea of the strengths and weaknesses of the parsing models.,2,0.7777777777777778,2,18,2003
"Section 7 goes on to discuss the distance features in the models, the implicit assumptions that the models make about the treebank annotation style, and the way that context-free rules in the original treebank are broken down, allowing the models to generalize by producing new rules on test data examples.",3,0.8425925925925926,3,54,2003
"We analyze these phenomena through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples.",4,0.8846153846153846,3,26,2003
"Finally, section 8 gives more discussion, by comparing the models to others that have been applied to parsing the treebank.",2,1.0434782608695652,2,23,2003
We aim to give some explanation of the differences in performance among the various models.,2,1.0,2,16,2003
Probabilistic context-free grammars are the starting point for the models in this article.,2,0.75,2,14,2003
"For this reason we briey recap the theory behind nonlexicalized PCFGs, before moving to the lexicalized case.",2,0.7894736842105263,2,19,2003
"Following Hopcroft and Ullman (1979), we dene a context-free grammar G as a 4-tuple (N, , A, R), where N is a set of nonterminal symbols,  is an alphabet, A is a distinguished start symbol in N, and R is a nite set of rules, in which each rule is of the form X   for some X  N,   (N  ) .",6,0.86,3,75,2003
The grammar denes a set of possible strings in the language and also denes a set of possible leftmost derivations under the grammar.,3,0.8125,2,24,2003
Each derivation corresponds to a tree-sentence pair that is well formed under the grammar.,2,1.0,2,15,2003
A probabilistic context-free grammar is a simple modication of a context-free grammar in which each rule in the grammar has an associated probability P( | X).,2,0.7586206896551724,3,29,2003
"This can be interpreted as the conditional probability of Xs being expanded using the rule X  , as opposed to one of the other possibilities for expanding X listed in the grammar.",2,1.0303030303030303,3,33,2003
"The probability of a derivation is then a product of terms, each term corresponding to a rule application in the derivation.",2,0.8913043478260869,2,23,2003
"The probability of a given tree-sentence pair (T, S) derived by n applications of context-free rules LHSi  RHSi (where LHS stands for left-hand side, RHS for right-hand side), 1  i  n, under the PCFG is  Booth and Thompson (1973) specify the conditions under which the PCFG does in fact dene a distribution over the possible derivations (trees) generated by the underlying grammar.",3,1.0133333333333334,4,75,2003
The rst condition is that the rule probabilities dene conditional distributions over how each nonterminal in the grammar can expand.,2,0.9761904761904762,2,21,2003
The second is a technical condition that guarantees that the stochastic process generating trees terminates in a nite number of steps with probability one.,2,1.02,2,25,2003
A central problem in PCFGs is to dene the conditional probability P( | X) for each rule X   in the grammar.,3,0.7916666666666666,3,24,2003
A simple way to do this is to take counts from a treebank and then to use the maximum-likelihood estimates.,3,0.9285714285714286,2,21,2003
"If the treebank has actually been generated from a probabilistic context-free grammar with the same rules and nonterminals as the model, then in the limit, as the training sample size approaches innity, the probability distribution implied by these estimates will converge to the distribution of the underlying grammar.2  Once the model has been trained, we have a model that denes P(T, S) for any sentence-tree pair in the grammar.",5,0.8607594936708861,3,79,2003
"The output on a new test sentence S is the most likely tree under this model,  The parser itself is an algorithm that searches for the tree, Tbest, that maximizes P(T, S).",2,1.0,3,40,2003
"In the case of PCFGs, this can be accomplished using a variant of the CKY algorithm applied to weighted grammars (providing that the PCFG can be converted to an equivalent PCFG in Chomsky normal form); see, for example, Manning and Sch utze (1999).",4,0.9230769230769231,4,52,2003
"If the model probabilities P(T, S) are the same as the true distribution generating training and test examples, returning the most likely tree under P(T, S) will be optimal in terms of minimizing the expected error rate (number of incorrect trees) on newly drawn test examples.",4,0.8362068965517241,3,58,2003
"Hence if the data are generated by a PCFG, and there are enough training examples for the maximum-likelihood estimates to converge to the true values, then this parsing method will be optimal.",5,0.7285714285714285,4,35,2003
"In practice, these assumptions cannot be veried and are arguably quite strong, but these limitations have not prevented generative models from being successfully applied to many NLP and speech tasks.",3,0.6029411764705882,2,34,2003
(See Collins [2002] for a discussion of other ways of conceptualizing the parsing problem.),2,0.9473684210526315,3,19,2003
"In the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), which is the source of data for our experiments, the rules are either internal to the tree, where LHS is a nonterminal and RHS is a string of one or more nonterminals, or lexical, where LHS is a part-of-speech tag and RHS is a word.",5,0.859375,3,64,2003
(See Figure 1 for an example.),2,0.7222222222222222,2,9,2003
2.2 Lexicalized PCFGs A PCFG can be lexicalized3 by associating a word w and a part-of-speech (POS) tag t with each nonterminal X in the tree.,2,0.9137931034482759,3,29,2003
(See Figure 2 for an example tree.),2,0.65,2,10,2003
The PCFG model can be applied to these lexicalized rules and trees in exactly the same way as before.,2,0.675,2,20,2003
"Whereas before the nonterminals were simple (for example, S or NP), they are now extended to include a word and part-of-speech tag (for example, S(bought,VBD) or NP(IBM,NNP)).",4,0.7666666666666667,4,45,2003
"Thus we write a nonterminal as X(x), where x = (cid.6)w, t(cid.7) and X is a constituent label.",4,0.765625,4,32,2003
"Formally, nothing has changed, we have just vastly increased the number of nonterminals in the grammar (by up to a factor of |V|  |T |,  2 This point is actually more subtle than it rst appears (we thank one of the anonymous reviewers for pointing this out), and we were unable to nd proofs of this property in the literature for PCFGs.",5,1.047945205479452,4,73,2003
"The rule probabilities for any nonterminal that appears with probability greater than zero in parse derivations will converge to their underlying values, by the usual properties of maximum-likelihood estimation for multinomial distributions.",3,0.8823529411764706,2,34,2003
"Assuming that the underlying PCFG generating training examples meet both criteria in Booth and Thompson (1973), it can be shown that convergence of rule probabilities implies that the distribution over trees will converge to that of the underlying PCFG, at least when Kullback-Liebler divergence or the innity norm is taken to be the measure of distance between the two distributions.",3,0.9615384615384616,3,65,2003
Thanks to Tommi Jaakkola and Nathan Srebro for discussions on this topic.,2,0,2,13,2003
3 We nd lexical heads in Penn Treebank data using the rules described in Appendix A of Collins (1999).,3,1.0227272727272727,2,22,2003
The rules are a modied version of a head table provided by David Magerman and used in the parser described in Magerman (1995).,2,0.8653846153846154,3,26,2003
A lexicalized parse tree and a list of the rules it contains.,4,0.7307692307692307,2,13,2003
where |V| is the number of words in the vocabulary and |T | is the number of part-ofspeech tags).,3,0.875,2,24,2003
"Although nothing has changed from a formal point of view, the practical consequences of expanding the number of nonterminals quickly become apparent when one is attempting to dene a method for parameter estimation.",3,1.1,2,35,2003
"The simplest solution would be to use the maximum-likelihood estimate as in equation (1), for example,  But the addition of lexical items makes the statistics for this estimate very sparse.",4,0.7285714285714285,4,35,2003
"The count for the denominator is likely to be relatively low, and the number of outcomes (possible lexicalized RHSs) is huge, meaning that the numerator is very likely to be zero.",4,0.8472222222222222,3,36,2003
Predicting the whole lexicalized rule in one go is too big a step.,3,0,2,14,2003
"One way to overcome these sparse-data problems is to break down the generation of the RHS of each rule into a sequence of smaller steps, and then to make independence assumptions to reduce the number of parameters in the model.",4,1.0,3,42,2003
The decomposition of rules should aim to meet two criteria.,2,1.0909090909090908,1,11,2003
"First, the steps should be small enough for the parameter estimation problem to be feasible (i.e., in terms of having sufcient training data to train the model, providing that smoothing techniques are used to mitigate remaining sparse-data problems).",3,0.9545454545454546,4,44,2003
"Second, the independence assumptions made should be linguistically plausible.",2,0.9090909090909091,2,11,2003
In the next sections we describe three statistical parsing models that have an increasing degree of linguistic sophistication.,2,0.8421052631578947,2,19,2003
Model 1 uses a decomposition of which parameters corresponding to lexical dependencies are a natural result.,2,1.0,3,17,2003
The model also incorporates a preference for right-branching structures through conditioning on distance features.,2,0.9,2,15,2003
Model 2 extends the decomposition to include a step in which subcategorization frames are chosen probabilistically.,2,1.088235294117647,2,17,2003
Model 3 handles wh-movement by adding parameters corresponding to slash categories being passed from the parent of the rule to one of its children or being discharged as a trace.,3,1.096774193548387,3,31,2003
This section describes how the generation of the RHS of a rule is broken down into a sequence of smaller steps in model 1.,2,1.0,3,25,2003
The rst thing to note is that each internal rule in a lexicalized PCFG has the form.,2,0.8333333333333334,3,18,2003
"Note that lexical rules, in contrast to the internal rules, are completely deterministic.",3,0.875,3,16,2003
"They always take the form where P is a part-of-speech tag, h is a word-tag pair (cid.6)w, t(cid.7), and the rule rewrites to just the word w. (See Figure 2 for examples of lexical rules.)",4,0.7346938775510204,3,49,2003
"Formally, we will always take a lexicalized nonterminal P(h) to expand deterministically (with probability one) in this way if P is a part-of-speech symbol.",3,0.8870967741935484,3,31,2003
Thus for the parsing models we require the nonterminal labels to be partitioned into two sets.,2,0.8235294117647058,2,17,2003
part-of-speech symbols and other nonterminals.,2,0,2,6,2003
Internal rules always have an LHS in which P is not a part-of-speech symbol.,2,0.9333333333333333,3,15,2003
"Because lexicalized rules are deterministic, they will not be discussed in the remainder of this article.",2,0.8888888888888888,2,18,2003
All of the modeling choices concern internal rules.,2,0,2,9,2003
The probability of an internal rule can be rewritten (exactly) using the chain rule  of probabilities.,3,0.8157894736842105,2,19,2003
"In summary, the generation of the RHS of a rule such as (2), given the LHS, has  been decomposed into three steps.5  Generate the head constituent label of the phrase, with probability Ph(H | P, h).",3,0.8854166666666666,4,48,2003
"In this example, and in the examples in the rest of the article, for brevity we omit the part-of-speech tags associated with words, writing, for example S(bought) rather than S(bought,VBD).",4,0.8488372093023255,4,43,2003
"We emphasize that throughout the models in this article, each word is always paired with its part of speech, either when the word is generated or when the word is being conditioned upon.",4,1.0555555555555556,4,36,2003
Adding Distance to the Model.,1,0.9166666666666666,1,6,2003
In this section we rst describe how the model can be extended to be history-based. We then show how this extension can be utilized in incorporating distance features into the model.,2,1.3125,2,16,2003
Black et al.,1,0,1,4,2003
(1992) originally introduced history-based models for parsing.,2,0,1,10,2003
"Equations (3) and (4) of the current article made the independence assumption that each modier is generated independently of the others (i.e., that the modiers are generated independently of everything except P, H, and h).",3,0.8555555555555555,3,45,2003
"In general, however, the probability of generating each modier could depend on any function of the previous modiers, head/parent category, and headword.",3,0.7777777777777778,3,27,2003
"Moreover, if the top-down derivation order is fully specied, then the probability of generating a modier can be conditioned on any structure that has been previously generated.",3,0.9666666666666667,4,30,2003
The remainder of this article assumes that the derivation order is depth-rst.,2,0.9230769230769231,2,13,2003
"that is, each modier recursively generates the subtree below it before the next modier is generated.",2,0.7777777777777778,2,18,2003
(Figure 3 gives an example that illustrates this.),2,1.0,2,11,2003
"The models in Collins (1996) showed that the distance between words standing in head-modier relationships was important, in particular, that it is important to capture a preference for right-branching structures (which almost translates into a preference for dependencies between adjacent words) and a preference for dependencies not to cross a verb.",3,1.1293103448275863,3,58,2003
In this section we describe how this information can be incorporated into model 1.,2,1.0,2,15,2003
"In section 7.2, we describe experiments that evaluate the effect of these features on parsing accuracy.",2,1.1388888888888888,2,18,2003
A partially completed tree derived depth-rst.,2,0.7857142857142857,1,7,2003
???,0,1,1,1,2003
"? marks the position of the next modier to be generatedit could be a nonterminal/headword/head-tag triple, or the STOP symbol.",0,1,3,1,2003
"The distribution over possible symbols in this position could be conditioned on any previously generated structure, that is, any structure appearing in the gure.",2,0.9444444444444444,2,27,2003
"The next child, R3(r3), is generated with probability P(R3(r3) | P, H, h, distancer(2)).",4,0.828125,3,32,2003
The distance is a function of the surface string below previous modiers R1 and R2.,2,0.65625,2,16,2003
"In principle the model could condition on any structure dominated by H, R1, or R2 (or, for that matter, on any structure previously generated elsewhere in the tree).",4,0.7285714285714285,3,35,2003
Distance can be incorporated into the model by modifying the independence assumptions so that each modier has a limited dependence on the previous modiers.,2,0.82,2,25,2003
Here distancel and distancer are functions of the surface string below the previous modiers.,2,0.7666666666666667,2,15,2003
(See Figure 4 for illustration.),2,0.8125,2,8,2003
"The distance measure is similar to that in Collins (1996), a vector with the following two elements.",2,0.8333333333333334,3,21,2003
(1) Is the string of zero length?,2,0.75,1,10,2003
(2) Does the string contain a verb?,2,0.8,1,10,2003
The rst feature allows the model to learn a preference for right-branching structures.,2,0.8571428571428571,2,14,2003
The second feature6 allows the model to learn a preference for modication of the most recent verb.7  3.2 Model 2.,2,0.8181818181818182,2,22,2003
The Complement/Adjunct Distinction and Subcategorization The tree depicted in Figure 2 illustrates the importance of the complement/adjunct distinction.,2,0.8157894736842105,2,19,2003
"It would be useful to identify IBM as a subject and Last week as an adjunct (temporal modier), but this distinction is not made in the tree, as both NPs are in the same position8 (sisters to a VP under an S node).",3,0.74,4,50,2003
From here on we will identify complements9 by attaching a -C sufx to nonterminals.,2,1.0625,2,16,2003
Figure 5 shows the tree in Figure 2 with added complement markings.,2,0.7307692307692307,2,13,2003
"A postprocessing stage could add this detail to the parser output, but there are a couple of reasons for making the distinction while parsing.",3,0.75,2,26,2003
"First, identifying complements is complex enough to warrant a probabilistic treatment.",2,0.8461538461538461,2,13,2003
"Lexical information is needed (for example, knowledge that week is likely to be a temporal modier).",2,1.075,3,20,2003
"Knowledge about subcategorization preferences (for example, that a verb takes exactly one subject) is also required.",3,1.125,2,20,2003
"For example, week can sometimes be a subject, as in Last week was a good one, so the model must balance the preference for having a subject against  6 Note that this feature means that dynamic programming parsing algorithms for the model must keep track of whether each constituent does or does not have a verb in the string to the right or left of its head.",2,0.9084507042253521,3,71,2003
See Collins (1999) for a full description of the parsing algorithms.,2,0.8928571428571429,2,14,2003
"7 In the models described in Collins (1997), there was a third question concerning punctuation.",3,1.0,2,19,2003
"(3) Does  the string contain 0, 1, 2 or more than 2 commas?",3,0.5833333333333334,3,18,2003
"(where a comma is anything tagged as , or .).",2,1.1666666666666667,2,12,2003
"The model described in this article has a cleaner incorporation of punctuation into the generative process, as described in section 4.3.",3,0.8260869565217391,2,23,2003
"8 Except that IBM is closer to the VP, but note that IBM is also the subject in IBM last week bought Lotus.",3,1.02,3,25,2003
9 We use the term complement in a broad sense that includes both complements and speciers under the  terminology of government and binding.,2,0.9791666666666666,2,24,2003
A tree with the -C sufx used to identify complements.,2,1.1666666666666667,1,12,2003
"IBM and Lotus are in subject and object position, respectively.",3,0.625,3,12,2003
Last week is an adjunct.,1,0.75,1,6,2003
"These problems are not restricted to NPs; compare The spokeswoman said (SBAR that the asbestos was dangerous) with Bonds beat short-term investments (SBAR because the market is down), in which an SBAR headed by that is a complement, but an SBAR headed by because is an adjunct.",6,0.9818181818181818,5,55,2003
A second reason for incorporating the complement/adjunct distinction into the parsing model is that this may help parsing accuracy.,2,1.0,2,20,2003
The assumption that complements are generated independently of one another often leads to incorrect parses.,2,1.34375,2,16,2003
(See Figure 6 for examples.),2,0.8125,2,8,2003
Identifying Complements and Adjuncts in the Penn Treebank.,2,0,2,9,2003
We add the -C sufx to all nonterminals in training data that satisfy the following conditions.,2,1.0,2,18,2003
The nonterminal must not have one of the following semantic tags.,2,0.7083333333333334,2,12,2003
"ADV, VOC, BNF, DIR, EXT, LOC, MNR, TMP, CLR or PRP.",7,0,2,20,2003
See Marcus et al.,1,1.1,2,5,2003
(1994) for an explanation of what these tags signify.,3,0,2,12,2003
"For example, the NP Last week in gure 2 would have the TMP (temporal) tag, and the SBAR in (SBAR because the market is down) would have the ADV (adverbial) tag.",4,0.75,4,40,2003
The nonterminal must not be on the RHS of a coordinated phrase.,2,0.7307692307692307,2,13,2003
"For example, in the rule S  S CC S, the two child Ss would not be marked as complements.",4,0.5909090909090909,2,22,2003
"In addition, the rst child following the head of a prepositional phrase is marked as a complement.",2,0.7631578947368421,3,19,2003
Probabilities over Subcategorization Frames.,1,0,1,5,2003
"Model 1 could be retrained on training data with the enhanced set of nonterminals, and it might learn the lexical properties that distinguish complements and adjuncts (IBM vs. week, or that vs. because).",4,0.9342105263157895,3,38,2003
"It would still suffer, however, from the bad independence assumptions illustrated in Figure 6.",2,0.9411764705882353,2,17,2003
"To solve these kinds of problems, the generative process is extended to include a probabilistic choice of left and right subcategorization frames.",3,0.7291666666666666,2,24,2003
Thus the subcategorization requirements are added to the conditioning context.,2,0.6818181818181818,2,11,2003
As complements are generated they are removed from the appropriate subcategorization multiset.,2,1.1153846153846154,2,13,2003
"Most importantly, the probability of generating the STOP symbol will be zero when the subcategorization frame is non-empty, and the probability of generating a particular complement will be zero when that complement is not in the subcategorization frame; thus all and only the required complements will be generated.",5,0.8557692307692307,4,52,2003
Here the head initially decides to take a single NP-C (subject) to its left and no complements to its right.,3,0.8260869565217391,2,23,2003
"NP-C(IBM) is immediately generated as the required subject, and NP-C is removed from LC, leaving it empty when the next modier, NP(week), is generated.",3,0.9,3,35,2003
"The incorrect structures in Figure 6 should now have low probability, because Plc({NP-C,NP-C} | S,VP,was) and Prc({NP-C,VP-C} | VP,VB,was) should be small.",5,0.9239130434782609,4,46,2003
Another obstacle to extracting predicate-argument structure from parse trees is whmovement.,2,1.0,2,12,2003
This section describes a probabilistic treatment of extraction from relative clauses.,2,0.7916666666666666,2,12,2003
"Noun phrases are most often extracted from subject position, object position, or from within PPs.",3,0.75,2,18,2003
It might be possible to write rule-based patterns that identify traces in a parse tree.,1,1.21875,2,16,2003
"We argue again, however, that this task is best integrated into the parser.",2,1.0625,3,16,2003
"The task is complex enough to warrant a probabilistic treatment, and integration may help parsing accuracy.",3,0.7777777777777778,2,18,2003
"A couple of complexities are that modication by an SBAR does not always involve extraction (e.g., the fact (SBAR that besoboru is played with a ball and a bat)), and it is not uncommon for extraction to occur through several constituents (e.g., The changes (SBAR that he said the government was prepared to make TRACE)).",4,1.0298507462686568,5,67,2003
One hope is that an integrated treatment of traces will improve the parameterization of the model.,2,0.9411764705882353,2,17,2003
"In particular, the subcategorization probabilities are smeared by extraction.",2,0.7727272727272727,2,11,2003
"In examples (1), (2), and (3), bought is a transitive verb; but without knowledge of traces, example (2) in training data will contribute to the probability of boughts being an intransitive verb.",5,1.065217391304348,4,46,2003
Formalisms similar to GPSG (Gazdar et al.,2,1,2,9,2003
1985) handle wh-movement by adding a gap feature to each nonterminal in the tree and propagating gaps through the tree until they are nally discharged as a trace complement (see Figure 7).,3,0.9166666666666666,3,36,2003
"In extraction cases the Penn Treebank annotation coindexes a TRACE with the WHNP head of the SBAR, so it is straightforward to add this information to trees in training data.",4,0.703125,2,32,2003
A +gap feature can be added to nonterminals to describe wh-movement.,2,1.1538461538461537,2,13,2003
The top-level NP initially generates an SBAR modier but species that it must contain an NP trace by adding the +gap feature.,2,0.8958333333333334,2,24,2003
"The gap is then passed down through the tree, until it is discharged as a TRACE complement to the right of bought.",2,0.9375,2,24,2003
"Given that the LHS of the rule has a gap, there are three ways that the gap can  be passed down to the RHS.",3,0.9423076923076923,3,26,2003
Head.,0,1,0,2,2003
"The gap is passed to the head of the phrase, as in rule (3) in Figure 7.",2,0.8333333333333334,2,21,2003
"Left, Right.",1,1,1,4,2003
The gap is passed on recursively to one of the left or right modiers of the head or is discharged as a TRACE argument to the left or right of the head.,3,0.7727272727272727,3,33,2003
"In rule (2) in Figure 7, it is passed on to a right modier, the S complement.",3,0.7954545454545454,2,22,2003
"In rule (4), a TRACE is generated to the right of the head VB.",2,0.8055555555555556,2,18,2003
"We specify a parameter type Pg(G| P, h, H) where G is either Head, Left, or Right.",3,0.8076923076923077,2,26,2003
The generative process is extended to choose among these cases after generating the head of the phrase.,2,0.9722222222222222,2,18,2003
The rest of the phrase is then generated in different ways depending on how the gap is propagated.,2,1.0,2,19,2003
In the Head case the left and right modiers are generated as normal.,2,0.6785714285714286,2,14,2003
In the Left and Right cases a +gap requirement is added to either the left or right SUBCAT variable.,3,0.7380952380952381,3,21,2003
"This requirement is fullled (and removed from the subcategorization list) when either a trace or a modier nonterminal that has the +gap feature, is generated.",3,0.9166666666666666,4,30,2003
"For example, rule (2) in Figure 7, SBAR(that)(+gap)  WHNP(that) S-C(bought)(+gap), has probability  In rule (2), Right is chosen, so the +gap requirement is added to RC.",5,0.8611111111111112,3,54,2003
Generation of S-C(bought)(+gap) fullls both the S-C and +gap requirements in RC.,3,0,3,21,2003
"In rule (4), Right is chosen again.",3,0.9545454545454546,2,11,2003
Note that generation of TRACE satises both the NP-C and +gap subcategorization requirements.,2,0.8333333333333334,4,15,2003
Linguistically Motivated Renements to the Models  Sections 3.1 to 3.3 described the basic framework for the parsing models in this article.,3,0,2,22,2003
"In this section we describe how some linguistic phenomena (nonrecursive NPs and coordination, for example) clearly violate the independence assumptions of the general models.",3,0.8214285714285714,4,28,2003
"We describe a number of these special cases, in each instance arguing that the phenomenon violates the independence assumptions, then describing how the model can be rened to deal with the problem.",3,0.9428571428571428,3,35,2003
"We dene nonrecursive NPs (from here on referred to as base-NPs and labeled NPB rather than NP) as NPs that do not directly dominate an NP themselves, unless the dominated NP is a possessive NP (i.e., it directly dominates a POS-tag POS).",4,0.9591836734693877,3,49,2003
Figure 8 gives some examples.,1,0.75,1,6,2003
Base-NPs deserve special treatment for three reasons.,1,0.9375,1,8,2003
"Because of this, the probability of generating the STOP symbol should be greatly increased when the previous modier is, for example, a determiner.",2,0.8703703703703703,3,27,2003
"As they stand, the independence assumptions in the three models lose this information.",3,0.8,2,15,2003
"As a result, the model will assign unreasonably high probabilities to NPs such as [NP yesterday the dog] in sentences such as Yesterday the dog barked.",3,0.8666666666666667,2,30,2003
The annotation standard in the treebank leaves the internal structure of base-NPs underspecied.,2,1.0,2,14,2003
"For example, both pet food volume (where pet modies food and food modies volume) and vanilla ice cream (where both vanilla and ice modify cream) would have the structure NPB  NN NN NN.",4,0.7692307692307693,5,39,2003
"Because of this, there is no reason to believe that modiers within NPBs are dependent on the head rather than the previous modier.",2,0.98,2,25,2003
"In fact, if it so happened that a majority of phrases were like pet food volume, then conditioning on the previous modier rather than the head would be preferable.",4,0.921875,5,32,2003
In general it is important (in particular for the distance measure to be effective) to have different nonterminal labels for what are effectively different X-bar levels.,3,1.0344827586206897,3,29,2003
(See section 7.3.2 for further discussion.),2,0.7222222222222222,2,9,2003
The nonterminal label for base-NPs is changed from NP to NPB.,2,0.9583333333333334,2,12,2003
"For consistency, whenever an NP is seen with no pre- or postmodiers, an NPB level is added.",4,0.8095238095238095,3,21,2003
"For example, [S [NP the dog] [VP barks] ] would be transformed into [S [NP [NPB the dog] ] [VP barks ] ].",3,0.8428571428571429,3,35,2003
These extra NPBs are removed before scoring the output of the parser against the treebank.,2,0.9375,2,16,2003
"For simplicity, we give probability terms under model 1 with no distance variables; the probability  terms with distance variables, or for models 2 and 3, will be similar, but with the addition of various pieces of conditioning information.",3,0,4,44,2003
"The modier and previous-modier nonterminals are always adjacent, so the distance variable is constant and is omitted.",3,0,2,19,2003
"For the purposes of this model, L0(l0) and R0(r0) are dened to be H(h).",3,1.0,3,25,2003
Coordination constructions are another example in which the independence assumptions in the basic models fail badly (at least given the current annotation method in the treebank).,3,0.8620689655172413,3,29,2003
The independence assumptions mean that the model fails to learn that there is always exactly one phrase following the coordinator (CC).,2,1.0,2,24,2003
The basic probability models will give much too high probabilities to unlikely phrases such as NP  NP CC or NP  NP CC NP NP.,2,0.54,2,25,2003
"For this reason we alter the generative process to allow generation of both the coordinator and the following phrase in one step; instead of just generating a nonterminal at each step, a nonterminal and a binary-valued coord ag are generated.",4,0.7906976744186046,3,43,2003
coord = 1 if there is a coordination relationship.,1,1.1,2,10,2003
"In the generative process, generation of a coord = 1 ag along with a modier triggers an additional step in the generative  (a) The generic way of annotating coordination in the treebank.",3,0.7638888888888888,3,36,2003
(b) and (c) show specic examples (with base-NPs added as described in section 4.1).,2,0.8809523809523809,2,21,2003
Note that the rst item of the conjunct is taken as the head of the phrase.,2,0.9411764705882353,3,17,2003
"Note the new type of parameter, Pcc, for the generation of the coordinator word and POS tag.",3,0.675,2,20,2003
The generation of coord=1 along with NP(dog) in the example implicitly requires generation of a coordinator tag/word pair through the Pcc parameter.,4,0.875,2,28,2003
"The generation of this tag/word pair is conditioned on the two words in the coordination dependency (man and dog in the example) and the label on their relationship (NP,NP,NP in the example, representing NP coordination).",3,0.7666666666666667,4,45,2003
"This section describes our treatment of punctuation in the model, where punctuation is used to refer to words tagged as a comma or colon.",2,1.0576923076923077,2,26,2003
"Previous workthe generative models described in Collins (1996) and the earlier version of these models described in Collins (1997)conditioned on punctuation as surface features of the string, treating it quite differently from lexical items.",3,0.8536585365853658,2,41,2003
"In particular, the model in Collins (1997) failed to generate punctuation, a deciency of the model.",2,1.0,2,21,2003
This section describes how punctuation is integrated into the generative models.,1,1.0,2,12,2003
Our rst step is to raise punctuation as high in the parse trees as possible.,2,0.9375,2,16,2003
"Punctuation at the beginning or end of sentences is removed from the training/test data altogether.13 All punctuation items apart from those tagged as comma or colon (items such as quotation marks and periods, tagged   or . )",4,0.875,3,40,2003
are removed altogether.,1,1.125,1,4,2003
"These transformations mean that punctuation always appears between two nonterminals, as opposed to appearing at the end of a phrase.",2,1.0454545454545454,2,22,2003
(See Figure 10 for an example.),2,0.7222222222222222,2,9,2003
A parse tree before and after punctuation transformations.,2,0,1,9,2003
"13 As one of the anonymous reviewers of this article pointed out, this choice of discarding the  sentence-nal punctuation may not be optimal, as the nal punctuation mark may well carry useful information about the sentence structure.",3,0.825,4,40,2003
Punctuation is then treated in a very similar way to coordination.,2,0.9583333333333334,2,12,2003
Our intuition is that there is a strong dependency between the punctuation mark and the modier generated after it.,2,0.9,2,20,2003
Punctuation is therefore generated with the following phrase through a punc ag that is similar to the coord ag (a binary-valued feature equal to one if a punctuation mark is generated with the following phrase).,2,0.9078947368421053,3,38,2003
Pp is a new parameter type for generation of punctuation tag/word pairs.,2,0.8076923076923077,2,13,2003
The generation of punc=1 along with ADJP(old) in the example implicitly requires generation of a punctuation tag/word pair through the Pp parameter.,4,0.875,2,28,2003
"The generation of this tag/word pair is conditioned on the two words in the punctuation dependency (Vinken and old in the example) and the label on their relationship (NP,NPB,ADJP in the example.)",4,0.7926829268292683,4,41,2003
Sentences in the treebank occur frequently with PRO subjects that may or may not be controlled.,2,0.9411764705882353,2,17,2003
"As the treebank annotation currently stands, the nonterminal is S whether or not a sentence has an overt subject.",3,0.7380952380952381,2,21,2003
This is a problem for the subcategorization probabilities in models 2 and 3.,2,0.75,2,14,2003
"The probability of having zero subjects, Plc({} | S, VP, verb), will be fairly high because of this.",4,0.7777777777777778,3,27,2003
"In addition, sentences with and without subjects appear in quite different syntactic environments.",2,0.7666666666666667,2,15,2003
For these reasons we modify the nonterminal for sentences without subjects to be SG (see gure 11).,3,1.075,2,20,2003
The resulting model has a cleaner division of subcategorization.,2,0.75,1,10,2003
"Plc({NP-C} | S, VP, verb)  1 and Plc({NP-C} | SG, VP, verb) = 0.",4,1.0,2,29,2003
The model will learn probabilistically the environments in which S and SG are likely to appear.,2,1.088235294117647,2,17,2003
"As a nal step, we use the rule concerning punctuation introduced in Collins (1996) to impose a constraint as follows.",2,1.0208333333333333,2,24,2003
"If for any constituent Z in the chart Z  <..X Y..> two of its children X and Y are separated by a comma, then the last word in Y must be directly followed by a comma, or must be the last word in the sentence.",2,0,2,11,2003
In training data 96% of commas follow this rule.,2,0.8636363636363636,1,11,2003
The rule has the benet of improving efciency by reducing the number of constituents in the chart.,2,1.1388888888888888,2,18,2003
"It would be preferable to develop a probabilistic analog of this rule, but we leave this to future research.",3,0.7857142857142857,3,21,2003
"(a) The treebank annotates sentences with empty subjects with an empty -NONE- element under subject position; (b) in training (and for evaluation), this null element is removed; (c) in models 2 and 3, sentences without subjects are changed to have a nonterminal SG.",4,0.9482758620689655,4,58,2003
5.1 Parameter Estimation Table 1 shows the various levels of back-off for each type of parameter in the model.,2,0.875,2,20,2003
"Note that we decompose PL(Li(lwi, lti), c, p | P, H, w, t, , LC) (where lwi and lti are the word and POS tag generated with nonterminal Li, c and p are the coord and punc ags associated with the nonterminal, and  is the distance measure) into the product  These two probabilities are then smoothed separately.",6,0.72,4,75,2003
Eisner (1996b) originally used POS tags to smooth a generative model in this way.,2,0.8823529411764706,2,17,2003
"In each case the nal estimate is  where e1, e2, and e3 are maximum-likelihood estimates with the context at levels 1, 2, and 3 in the table, and 1, 2 and 3 are smoothing parameters, where 0  i  1.",4,0.7978723404255319,4,47,2003
We use the smoothing method described in Bikel et al.,2,0.8636363636363636,2,11,2003
"(1997), which is derived from a method described in Witten and Bell (1991).",2,0.8947368421052632,2,19,2003
"First, say that the most specic estimate e1 = n1 ; that is, f1 is the value of the denominator count in the relative frequency f1 estimate.",4,0.5833333333333334,3,30,2003
"Second, dene u1 to be the number of distinct outcomes seen in the f1 events in training data.",2,0.875,2,20,2003
The variable u1 can take any value from one to f1 inclusive.,2,0.8461538461538461,2,13,2003
"The coefcient ve was chosen to maximize accuracy on the development set, section 0 of the treebank (in practice it was found that any value in the range 25 gave a very similar level of performance).",3,0.975,4,40,2003
"All words occurring less than six times14 in training data, and words in test data that have never been seen in training, are replaced with the UNKNOWN token.",4,0.9354838709677419,3,31,2003
This allows the model to handle robustly the statistics for rare or new words.,2,0.8666666666666667,2,15,2003
Words in test data that have not been seen in training are deterministically assigned the POS tag that is assigned by the tagger described in Ratnaparkhi (1996).,2,1.15,2,30,2003
"As a preprocessing step, the  tagger is used to decode each test data sentence.",2,0.6875,2,16,2003
"All other words are tagged during parsing, the output from Ratnaparkhis tagger being ignored.",2,0.84375,2,16,2003
The POS tags allowed for each word are limited to those that have been seen in training data for that word (any tag/word pairs not seen in training would give an estimate of zero in the PL2 and PR2 distributions).,3,0.9883720930232558,3,43,2003
"The model is fully integrated, in that part-of-speech tags are statistically generated along with words in the models, so that the parser will make a statistical decision as to the most likely tag for each known word in the sentence.",2,0.7790697674418605,3,43,2003
"The parsing algorithm for the models is a dynamic programming algorithm, which is very similar to standard chart parsing algorithms for probabilistic or weighted grammars.",2,0.7037037037037037,2,27,2003
"The algorithm has complexity O(n5), where n is the number of words in the string.",2,1.0,2,20,2003
"In practice, pruning strategies (methods that discard lower-probability constituents in the chart) can improve efciency a great deal.",3,1.0,4,22,2003
"The appendices of Collins (1999) give a precise description of the parsing algorithms, an analysis of their computational complexity, and also a description of the pruning methods that are employed.",5,0.8571428571428571,3,35,2003
See Eisner and Satta (1999) for an O(n4) algorithm for lexicalized grammars that could be applied to the models in this paper.,2,0.9285714285714286,3,28,2003
Eisner and Satta (1999) also describe an O(n3) algorithm for a restricted class of lexicalized grammars; it is an open question whether this restricted class includes the models in this article.,4,0.6710526315789473,2,38,2003
"The parser was trained on sections 221 of the Wall Street Journal portion of the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) (approximately 40,000 sentences) and tested on section 23 (2,416 sentences).",4,0.7195121951219512,6,41,2003
We use the PARSEVAL measures (Black et al.,1,1.05,2,10,2003
1991) to compare performance.,1,1.3333333333333333,1,6,2003
"Labeled precision = number of correct constituents in proposed parse  number of constituents in proposed parse Labeled recall = number of correct constituents in proposed parse  number of constituents in treebank parse  Crossing brackets = number of constituents that violate constituent boundaries  with a constituent in the treebank parse  For a constituent to be correct, it must span the same set of words (ignoring punctuation, i.e., all tokens tagged as commas, colons, or quotation marks) and have the same label15 as a constituent in the treebank parse.",5,0.8263157894736842,4,95,2003
"Table 2 shows the results for models 1, 2 and 3 and a variety of other models in the literature.",3,0.75,2,22,2003
Two models (Collins 2000; Charniak 2000) outperform models 2 and 3 on section 23 of the treebank.,3,0.7380952380952381,2,21,2003
Collins (2000) uses a technique based on boosting algorithms for machine learning that reranks n-best output from model 2 in this article.,2,1.06,2,25,2003
Charniak (2000) describes a series of enhancements to the earlier model of Charniak (1997).,2,0.9736842105263158,2,19,2003
"The precision and recall of the traces found by Model 3 were 93.8% and 90.1%, respectively (out of 437 cases in section 23 of the treebank), where three criteria must be met for a trace to be correct.",3,0.8555555555555555,4,45,2003
(1) It must be an argument to the correct headword; (2) It must be in the correct position in relation to that headword (preceding or following);  Results on Section 23 of the WSJ Treebank.,3,0,4,43,2003
LR/LP = labeled recall/precision.,1,0.9,2,5,2003
CBs is the average number of crossing brackets per sentence.,2,1.1818181818181819,1,11,2003
"0 CBs,  2 CBs are the percentage of sentences with 0 or  2 crossing brackets respectively.",2,0.8611111111111112,2,18,2003
"All the results in this table are for models trained and tested on the same data, using the same evaluation metric.",3,0.6956521739130435,3,23,2003
(Note that these results show a slight improvement over those in (Collins 97); the main model changes were the improved treatment of punctuation (section 4.3) together with the addition of the Pp and Pcc parameters.),4,0.6744186046511628,3,43,2003
This section discusses some aspects of the models in more detail.,2,0.875,2,12,2003
Section 7.1 gives a much more detailed analysis of the parsers performance.,2,0.6538461538461539,2,13,2003
"In section 7.2 we examine  We exclude innitival relative clauses from these gures (for example, I called a plumber TRACE to x the  sink, where plumber is coindexed with the trace subject of the innitival).",3,1.0125,3,40,2003
"The algorithm scored 41% precision and 18% recall on the 60 cases in section 23but innitival relatives are extremely difcult even for human annotators to distinguish from purpose clauses (in this case, the innitival could be a purpose clause modifying called) (Ann Taylor, personal communication, 1997).",3,0.7946428571428571,3,56,2003
Head-Driven Statistical Models for NL Parsing  the distance features in the model.,2,0,1,13,2003
In section 7.3 we examine how the model interacts with the Penn Treebank style of annotation.,2,0.9411764705882353,2,17,2003
"Finally, in section 7.4 we discuss the need to break down context-free rules in the treebank in such a way that the model will generalize to give nonzero probability to rules not seen in training.",2,0.9459459459459459,2,37,2003
In each case we use three methods of analysis.,2,0.95,1,10,2003
"First, we consider how various aspects of the model affect parsing performance, through accuracy measurements on the treebank.",2,1.0238095238095237,2,21,2003
"Second, we look at the frequency of different constructions in the treebank.",2,0.8928571428571429,2,14,2003
"Third, we consider linguistically motivated examples as a way of justifying various modeling choices.",2,0.9375,2,16,2003
"7.1 A Closer Look at the Results In this section we look more closely at the parser, by evaluating its performance on specic constituents or constructions.",3,0.8214285714285714,2,28,2003
The intention is to get a better idea of the parsers strengths and weaknesses.,2,0.8666666666666667,2,15,2003
"First, Table 3 has a breakdown of precision and recall by constituent type.",2,0.7,2,15,2003
"Although somewhat useful in understanding parser performance, a breakdown of accuracy by constituent type fails to capture the idea of attachment accuracy.",3,1.0208333333333333,2,24,2003
For this reason we also evaluate the parsers precision and recall in recovering dependencies between words.,3,0.7941176470588235,2,17,2003
This gives a better indication of the accuracy on different kinds of attachments.,2,0.9642857142857143,2,14,2003
A dependency is dened as a triple with the following elements (see Figure 12 for an example tree and its associated dependencies).,2,0.8,3,25,2003
1.,0,1.25,0,2,2003
Modier.,0,1,0,2,2003
The index of the modier word in the sentence.,2,0,2,10,2003
"Recall and precision for different constituent types, for section 0 of the treebank with model 2.",2,0.8611111111111112,3,18,2003
Label is the nonterminal label; Proportion is the percentage of constituents in the treebank section 0 that have this label; Count is the number of constituents that have this label.,3,0.9393939393939394,2,33,2003
A tree and its associated dependencies.,2,0,2,7,2003
"Note that in normalizing dependencies, all POS tags are replaced with TAG, and the NP-C parent in the fth relation is replaced with NP.",3,0.6851851851851852,3,27,2003
Head.,0,1,0,2,2003
The index of the headword in the sentence.,2,0,1,9,2003
Relation.,0,0,0,2,2003
"A (cid.6)Parent, Head, Modifier, Direction(cid.7) 4-tuple, where the four elements are the parent, head, and modier nonterminals involved in the dependency and the direction of the dependency (L for left, R for right).",3,0.7745098039215687,2,51,2003
"For example, (cid.6)S, VP, NP-C, L(cid.7) would indicate a subject-verb dependency.",5,0.6041666666666666,4,24,2003
"In coordination cases there is a fth element of the tuple, CC.",2,0.8214285714285714,2,14,2003
"For example, (cid.6)NP, NP, NP, R, CC(cid.7) would be an instance of NP coordination.",4,0.5892857142857143,3,28,2003
"In addition, the relation is normalized to some extent.",2,0.7727272727272727,2,11,2003
"First, all POS tags are replaced with the token TAG, so that POS-tagging errors do not lead to errors in dependencies.17 Second, any complement markings on the parent or head nonterminal are removed.",4,0.8026315789473685,3,38,2003
"For example, (cid.6)NP-C, NPB, PP, R(cid.7) is replaced by (cid.6)NP, NPB, PP, R(cid.7).",4,0.5540540540540541,4,37,2003
"This prevents parsing errors where a complement has been mistaken to be an adjunct (or vice versa), leading to more than one dependency error.",2,1.0178571428571428,3,28,2003
"As an example, in Figure 12, if the NP the man with the telescope was mistakenly identied as an adjunct, then without normalization, this would lead to two dependency errors.",5,0.7714285714285715,5,35,2003
Both the PP dependency and the verb-object relation would be incorrect.,3,0.625,1,12,2003
"With normalization, only the verb-object relation is incorrect.",2,0.65,2,10,2003
Table 4 Dependency accuracy on section 0 of the treebank with Model 2.,2,0,2,14,2003
No labels means that only the dependency needs to be correct; the relation may be wrong; No complements means all complement (-C) markings are stripped before comparing relations; All means complement markings are retained on the modifying nonterminal.,5,0.9,2,45,2003
"Under this denition, gold-standard and parser-output trees can be converted to sets of dependencies, and precision and recall can be calculated on these dependencies.",4,0.6851851851851852,3,27,2003
Dependency accuracies are given for section 0 of the treebank in table 4.,2,0.8928571428571429,1,14,2003
Table 5 gives a breakdown of the accuracies by dependency type.,2,0.7916666666666666,2,12,2003
Table 6 shows the dependency accuracies for eight subtypes of dependency that  together account for 94% of all dependencies.,2,1.0,2,21,2003
"Sentential head (94.99% recall, 94.99% precision).",3,0,3,12,2003
This subtype includes any dependencies involving the headword of the entire sentence.,2,0.8076923076923077,2,13,2003
"Adjunct to a verb (75.11% recall, 78.44% precision).",2,0,2,14,2003
"This subtype includes any dependencies in which the parent is VP, the head is TAG, and the modier is not a PP, or in which the parent is S, the head is VP, and the modier is not a PP.",6,0.6956521739130435,2,46,2003
A conclusion to draw from these accuracies is that the parser is doing very well at recovering the core structure of sentences.,2,1.0,2,23,2003
"complements, sentential heads, and base-NP relationships (NP chunks) are all recovered with over 90% accuracy.",3,0,2,21,2003
The main sources of errors are adjuncts.,2,0.9375,1,8,2003
"Coordination is especially difcult for the parser, most likely   Results on section 0 of the WSJ Treebank.",2,0.8157894736842105,2,19,2003
"A YES in the A column means that the adjacency conditions were used in the distance measure; likewise, a YES in the V column indicates that the verb conditions were used in the distance measure.",3,0.8026315789473685,3,38,2003
LR = labeled recall; LP = labeled precision.,2,0,1,10,2003
CBs is the average number of crossing brackets per sentence.,2,1.1818181818181819,1,11,2003
"0 CBs  2 CBs are the percentages of sentences with 0 and  2 crossing brackets, respectively.",2,0.9444444444444444,3,18,2003
"The distance measure, whose implementation was described in section 3.1.1, deserves more discussion and motivation.",3,0.7777777777777778,2,18,2003
In this section we consider it from three perspectives.,2,0.85,1,10,2003
its inuence on parsing accuracy; an analysis of distributions in training data that are sensitive to the distance variables; and some examples of sentences in which the distance measure is useful in discriminating among competing analyses.,4,1.0256410256410255,3,39,2003
7.2.1 Impact of the Distance Measure on Accuracy.,2,0,2,9,2003
Table 7 shows the results for models 1 and 2 with and without the adjacency and verb distance measures.,3,0.625,2,20,2003
It is clear that the distance measure improves the models accuracy.,2,0.8333333333333334,2,12,2003
What is most striking is just how badly model 1 performs without the distance measure.,2,0.96875,2,16,2003
"Looking at the parsers output, the reason for this poor performance is that the adjacency condition in the distance measure is approximating subcategorization information.",3,0.6730769230769231,3,26,2003
"In particular, in phrases such as PPs and SBARs (and, to a lesser extent, in VPs) that almost always take exactly one complement to the right of their head, the adjacency feature encodes this monovalency through parameters P(STOP|PP/SBAR, adjacent) = 0 and P(STOP|PP/SBAR, not adjacent) = 1.",5,0.7424242424242424,3,66,2003
Figure 13 shows some particularly bad structures returned by model 1 with no distance variables.,2,0.71875,2,16,2003
"Another surprise is that subcategorization can be very useful, but that the distance measure has masked this utility.",2,0.875,3,20,2003
"One interpretation in moving from the least parameterized model (Model 1 [No, No]) to the fully parameterized model (Model 2 [Yes, Yes]) is that the adjacency condition adds around 11% in accuracy; the verb condition adds another 1.5%; and subcategorization nally adds a mere 0.8%.",7,0.819672131147541,3,61,2003
"Under this interpretation subcategorization information isnt all that useful (and this was my original assumption, as this was the order in which features were originally added to the model).",3,0.8939393939393939,3,33,2003
But under another interpretation subcategorization is very useful.,2,0.7222222222222222,2,9,2003
"In moving from Model 1 (No, No) to Model 2 (No, No), we see a 10% improvement as a result of subcategorization parameters; adjacency then adds a 1.5% improvement; and the verb condition adds a nal 1% improvement.",6,0.8333333333333334,3,51,2003
"From an engineering point of view, given a choice of whether to add just distance or subcategorization to the model, distance is preferable.",5,0.8461538461538461,3,26,2003
"But linguistically it is clear that adjacency can only approximate subcategorization and that subcategorization is  Two examples of bad parses produced by model 1 with no distance or subcategorization conditions (Model 1 (No, No) in table 7).",3,0.9186046511627907,3,43,2003
"In (a) one PP has two complements, the other has none; in (b) the SBAR has two complements.",4,0.7,2,25,2003
"In both examples either the adjacency condition or the subcategorization parameters will correct the errors, so these are examples in which the adjacency and subcategorization variables overlap in their utility.",4,0.734375,3,32,2003
"Distribution of nonterminals generated as postmodiers to an NP (see tree to the left), at various distances from the head.",3,0.9583333333333334,2,24,2003
"A = True means the modier is adjacent to the head, V = True means there is a verb between the head and the modier.",2,1.0740740740740742,2,27,2003
Distributions were calculated from the rst 10000 events for each of the three cases in sections 2-21 of the treebank.,2,0.8809523809523809,2,21,2003
"In free-word-order languages, distance may not approximate subcategorization at all well.",3,0.7307692307692307,2,13,2003
"A complement may appear to either the right or left of the head, confusing the adjacency condition.",3,0.6842105263157895,2,19,2003
7.2.2 Frequencies in Training Data.,1,0,1,6,2003
Tables 8 and 9 show the effect of distance on the distribution of modiers in two of the most frequent syntactic environments.,2,0.8913043478260869,2,23,2003
NP and verb modication.,1,0,1,5,2003
The distribution varies a great deal with distance.,2,0.7222222222222222,2,9,2003
Most striking is the way that the probability of STOP increases with increasing distance.,2,1.0666666666666667,2,15,2003
"from 71% to 89% to 98% in the NP case, from 8% to 60% to 96% in the verb case.",5,0.625,3,28,2003
Each modier probability generally decreases with distance.,2,0.8125,1,8,2003
"For example, the probability of seeing a PP modier to an NP decreases from 17.7% to 5.57% to 0.93%.",3,0.75,3,24,2003
"Distribution of nonterminals generated as postmodiers to a verb within a VP (see tree to the left), at various distances from the head.",3,0.9814814814814815,2,27,2003
A = True means the modier is adjacent to the head; V = True means there is a verb between the head and the modier.,2,1.0740740740740742,2,27,2003
The distributions were calculated from the rst 10000 events for each of the distributions in sections 221.,2,1.0277777777777777,2,18,2003
Auxiliary verbs (verbs taking a VP complement to their right) were excluded from these statistics.,3,0.8888888888888888,3,18,2003
7.2.3 Distance Features and Right-Branching Structures.,2,0,2,7,2003
Both the adjacency and verb components of the distance measure allow the model to learn a preference for rightbranching structures.,2,0.9047619047619048,2,21,2003
"First, consider the adjacency condition.",1,0.6428571428571429,1,7,2003
Figure 14 shows some examples in which right-branching structures are more frequent.,2,1.0,2,13,2003
"Using the statistics from Tables 8 and 9, the probability of the alternative structures can be calculated.",3,0,3,19,2003
The results are given below.,1,0.9166666666666666,1,6,2003
"The right-branching structures get higher probability (although this is before the lexical-dependency probabilities are multiplied in, so this prior preference for right-branching structures can be overruled by lexical preferences).",3,0.7575757575757576,3,33,2003
"If the distance variables were not conditioned on, the product of terms for the two alternatives would be identical, and the model would have no preference for one structure over another.",4,0.8235294117647058,2,34,2003
"Probabilities for the two alternative PP structures in Figure 14 (excluding probability terms that are constant across the two structures; A=1 means distance is adjacent, A=0 means not adjacent) are as follows.",4,1.024390243902439,3,41,2003
"Some alternative structures for the same surface sequence of chunks (NPB PP PP in the rst case, NPB PP SBAR in the second case) in which the adjacency condition distinguishes between the two structures.",3,0.6973684210526315,5,38,2003
The percentages are taken from sections 221 of the treebank.,1,0.8636363636363636,1,11,2003
In both cases right-branching structures are more frequent.,2,0.7222222222222222,1,9,2003
Some alternative structures for the same surface sequence of chunks in which the verb condition in the distance measure distinguishes between the two structures.,3,0.92,3,25,2003
"In both cases the low-attachment analyses will get higher probability under the model, because of the low probability of generating a PP modier involving a dependency that crosses a verb.",2,0.90625,2,32,2003
(X stands for any nonterminal.),2,0.6875,2,8,2003
7.2.4 Verb Condition and Right-Branching Structures.,2,0,2,7,2003
Figure 15 shows some examples in which the verb condition is important in differentiating the probability of two structures.,2,1.25,2,20,2003
"In both cases an adjunct can attach either high or low, but high attachment results in a dependencys crossing a verb and has lower probability.",4,0.7407407407407407,2,27,2003
"An alternative to the surface string feature would be a predicate such as were any of the previous modiers in X, where X is a set of nonterminals that are likely to contain a verb, such as VP, SBAR, S, or SG.",3,1.0104166666666667,2,48,2003
This would allow the model to handle cases like the rst example in Figure 15 correctly.,2,0.9411764705882353,2,17,2003
The second example shows why it is preferable to condition on the surface string.,2,0.9333333333333333,2,15,2003
"In this case the verb is invisible to the top level, as it is generated recursively below the NP object.",2,0.8636363636363636,3,22,2003
7.2.5 Structural versus Semantic Preferences.,1,1,1,6,2003
One hypothesis would be that lexical statistics are really what is important in parsing.,2,1.1666666666666667,2,15,2003
"that arriving at a correct interpretation for a sentence is simply a matter of nding the most semantically plausible analysis, and that the statistics related to lexical dependencies approximate this notion of plausibility.",4,0.8857142857142857,3,35,2003
"Implicitly, we would be just as well off (maybe even better off) if statistics were calculated between items at the predicate-argument level, with no reference to structure.",3,0.90625,3,32,2003
The distance preferences under this interpretation are just a way of mitigating sparse-data problems.,2,0.9333333333333333,2,15,2003
"When the lexical statistics are too sparse, then falling back on some structural preference is not ideal, but is at least better than chance.",3,0.8703703703703703,2,27,2003
"This hypothesis is suggested by previous work on specic cases of attachment ambiguity such as PP attachment (see, e.g., Collins and Brooks 1995), which has showed that models will perform better given lexical statistics, and that a straight structural preference is merely a fallback.",4,0.8137254901960784,4,51,2003
But some examples suggest this is not the case.,2,0.9,1,10,2003
"that, in fact, many sentences have several equally semantically plausible analyses, but that structural preferences  John was believed to have been shot by Bill.",3,0.6964285714285714,3,28,2003
"Surprisingly, this sentence has two analyses.",2,0.6875,1,8,2003
Bill can be the deep subject of either believed or shot.,2,0.9166666666666666,2,12,2003
"Yet people have a very strong preference for Bill to be doing the shooting, so much so that they may even miss the second analysis.",3,0.8333333333333334,4,27,2003
"(To see that the dispreferred analysis is semantically quite plausible, consider Bill believed John to have been shot.)",4,0.8863636363636364,3,22,2003
Flip said that Squeaky will do the work yesterday.,1,1.1,2,10,2003
This sentence is a garden path.,1,0.6428571428571429,1,7,2003
"The structural preference for yesterday to modify the most recent verb is so strong that it is easy to miss the (only) semantically plausible interpretation, paraphrased as Flip said yesterday that Squeaky will do the work.",2,1.0625,3,40,2003
The model makes the correct predictions in these cases.,2,0.75,1,10,2003
"In example (4), the statistics in Table 9 show that a PP is nine times as likely to attach low as to attach high when two verbs are candidate attachment points (the chances of seeing a PP modier are 15.8% and 1.73% in columns 1 and 5 of the table, respectively).",3,0.9416666666666667,4,60,2003
"In example (5), the probability of seeing an NP (adjunct) modier to do in a nonadjacent but non-verbcrossing environment is 2.11% in sections 221 of the treebank (8 out of 379 cases); in contrast, the chance of seeing an NP adjunct modifying said across a verb is 0.026% (1 out of 3,778 cases).",4,0.8208955223880597,4,67,2003
The two probabilities differ by a factor of almost 80.,2,0.8636363636363636,1,11,2003
Figures 16 and 17 show some alternative styles of syntactic annotation.,2,0.7916666666666666,2,12,2003
"The Penn Treebank annotation style tends to leave trees quite at, typically with one level of structure for each X-bar level; at the other extreme are completely binary-branching representations.",4,0.671875,2,32,2003
"The two annotation styles are in some sense equivalent, in that it is easy to dene a one-to-one mapping between them.",2,0.8478260869565217,3,23,2003
"But crucially, two different annotation styles may lead to quite different parsing accuracies for a given model, even if the two representations are equivalent under some one-to-one mapping.",3,0.5806451612903226,3,31,2003
A parsing model does not need to be tied to the annotation style of the treebank on which it is trained.,2,1.0227272727272727,2,22,2003
The following procedure can be used to transform trees in both training and test data into a new representation.,2,0.75,2,20,2003
"Alternative annotation styles for a sentence S with a verb head V, left modiers X1, X2, and right modiers Y1, Y2.",4,0.75,2,26,2003
(a) the Penn Treebank style of analysis (one level of structure for each bar level); (b) an alternative but equivalent binary branching representation.,4,0,2,31,2003
"Alternative annotation styles for a noun phrase with a noun head N, left modiers X1, X2, and right modiers Y1, Y2.",3,0.5961538461538461,2,26,2003
"(a) the Penn Treebank style of analysis (one level of structure for each bar level, although note that both the nonrecursive and the recursive noun phrases are (cid.2) labeled NP; (b) an alternative but equivalent binary branching representation; (a modication of the Penn Treebank style to differentiate recursive and nonrecursive NPs (in some sense NPB is a bar 1 structure and NP is a bar 2 structure).",4,0.7439024390243902,4,82,2003
Transform training data trees into the new representation and train the model.,3,0.6538461538461539,2,13,2003
Recover parse trees in the new representation when running the model over test data sentences.,2,0.875,2,16,2003
Convert the test output back into the treebank representation for scoring purposes.,2,0.7307692307692307,2,13,2003
"As long as there is a one-to-one mapping between the treebank and the new representation, nothing is lost in making such a transformation.",4,0.82,2,25,2003
Goodman (1997) and Johnson (1997) both suggest this strategy.,3,0.9642857142857143,3,14,2003
Goodman (1997) converts the treebank into binary-branching trees.,2,0.8636363636363636,2,11,2003
Johnson (1997) considers conversion to a number of different representations and discusses how this inuences accuracy for nonlexicalized PCFGs.,3,0.8409090909090909,3,22,2003
"The models developed in this article have tacitly assumed the Penn Treebank style of annotation and will perform badly given other representations (for example, binary-branching trees).",3,0.8833333333333333,3,30,2003
"This section makes this point more explicit, describing exactly what annotation style is suitable for the models and showing how other annotation styles will cause problems.",3,0.9821428571428571,3,28,2003
This dependence on Penn Treebankstyle annotations does not imply that the models are inappropriate for a treebank annotated in a different style.,2,0.7391304347826086,2,23,2003
"In this case we simply recommend transforming the trees into at, one-levelper-X-bar-level trees before training the model, as in the three-step procedure outlined above.",3,0.8703703703703703,4,27,2003
Other models in the literature are also very likely to be sensitive to annotation style.,2,0.9375,2,16,2003
"Charniaks (1997) models will most likely perform quite differently with binarybranching trees (for example, his current models will learn that rules such as VP  V SG PP are very rare, but with binary-branching structures, this context sensitivity will be lost).",4,0.7083333333333334,4,48,2003
The models of Magerman (1995) and Ratnaparkhi (1997) use contextual predicates that would most likely need to be modied given a different annotation style.,2,1.0344827586206897,3,29,2003
"Goodmans (1997) models are the exception, as he already species that the treebank should be transformed into his chosen representation, binary-branching trees.",2,0.7777777777777778,3,27,2003
"7.3.1 Representation Affects Structural, not Lexical, Preferences.",2,0.85,1,10,2003
"The alternative representations in Figures 16 and 17 have the same lexical dependencies (providing that the binary-branching structures are centered about the head of the phrase, as in the examples).",3,0.7058823529411765,4,34,2003
The difference between the representations involves structural preferences such as the right-branching preferences encoded by the distance measure.,2,0.7631578947368421,2,19,2003
Applying the models in this article to treebank analyses that use this type of head-centered  BB = binary-branching structures; FLAT = Penn treebank style annotations.,2,0.9259259259259259,2,27,2003
In each case the binary-branching annotation style prevents the model from learning that these structures should receive low probability because of the long distance dependency associated with the nal PP (in boldface).,3,0.7285714285714285,2,35,2003
binary-branching tree will result in a distance measure that incorrectly encodes a preference for right-branching structures.,2,1.0,2,17,2003
"To see this, consider the examples in Figure 18.",2,0.8636363636363636,1,11,2003
"In each binary-branching example, the generation of the nal modifying PP is blind to the distance between it and the head that it modies.",3,0.8076923076923077,3,26,2003
"At the top level of the tree, it is apparently adjacent to the head; crucially, the closer modier (SG in (a), the other PP in (b)) is hidden lower in the tree structure.",4,0.7159090909090909,4,44,2003
"So the model will be unable to differentiate generation of the PP in adjacent versus nonadjacent or non-verb-crossing versus verb-crossing environments, and the structures in Figure 18 will be assigned unreasonably high probabilities.",4,0.8142857142857143,4,35,2003
This does not mean that distance preferences cannot be encoded in a binarybranching PCFG.,2,0.8125,3,16,2003
Goodman (1997) achieves this by adding distance features to the nonterminals.,2,1.0,2,14,2003
"The spirit of this implementation is that the top-level rules VP  VP PP and NP  NP PP would be modied to VP  VP(+rverb) PP and NP  NP(+rmod) PP, respectively, where (+rverb) means a phrase in which the head has a verb in its right modiers, and (+rmod) means a phrase that has at least one right modier to the head.",4,0.9358974358974359,5,78,2003
"The model will learn from training data that P(VP  VP(+rverb) PP|VP) (cid.17) P(VP  VP(-rverb) PP|VP), that is, that a prepositional-phrase modication is much more likely when it does not cross a verb.",4,0.8839285714285714,3,56,2003
7.3.2 The Importance of Differentiating Nonrecursive from Recursive NPs.,2,0,1,10,2003
Figure 19 shows the modication to the Penn Treebank annotation to relabel base-NPs as NPB.,2,0.8125,2,16,2003
It also illustrates a problem that arises if a distinction between the two is not made.,2,1.1470588235294117,3,17,2003
Structures such as that in Figure 19(b) are assigned high probabilities even if they  Examples of other phrases in the Penn Treebank in which nonrecursive and recursive phrases are not differentiated.,2,1.0285714285714285,4,35,2003
are never seen in training data.,1,0.9285714285714286,1,7,2003
"(Johnson [1997] notes that this structure has a higher probability than the correct, at structure, given counts taken from the treebank for a standard PCFG.)",3,0.71875,3,32,2003
"The model is fooled by the binary-branching style into modeling both PPs as being adjacent to the head of the noun phrase, so 19(b) will be assigned a very high probability.",4,0.7361111111111112,4,36,2003
This problem does not apply only to NPs.,2,0.8333333333333334,1,9,2003
"Other types of phrases such as adjectival phrases (ADJPs) or adverbial phrases (ADVPs) also have nonrecursive (bar 1) and recursive (bar 2) levels, which are not differentiated in the Penn Treebank.",4,0.8536585365853658,3,41,2003
(See Figure 20 for examples.),2,0.8125,2,8,2003
Ideally these cases should be differentiated too.,1,0.9375,1,8,2003
"We did not implement this change because it is unlikely to make much difference in accuracy, given the relative infrequency of these cases (excluding coordination cases, and looking at the 80,254 instances in sections 221 of the Penn Treebank in which a parent and head nonterminal are the same.",3,0.8207547169811321,3,53,2003
94.5% are the NP case; 2.6% are cases of coordination in which a punctuation mark is the coordinator;18 only 2.9% are similar to those in Figure 20).,3,0.7571428571428571,3,35,2003
Tree representations are at.,1,0.9,1,5,2003
"that is, one level per X-bar level.",1,0.8333333333333334,2,9,2003
"Different X-bar levels have different labels (in particular, nonrecursive and recursive levels are differentiated, at least for the most frequent case of NPs).",3,0.7142857142857143,3,28,2003
The parsing approaches we have described concentrate on breaking down context-free rules in the treebank into smaller components.,2,1.0789473684210527,2,19,2003
"Lexicalized rules were initially broken down to bare-bones Markov processes, then increased dependency on previously generated modiers was built back up through the distance measure and subcategorization.",3,0.8103448275862069,3,29,2003
"Even with this additional context, the models are still able to recover rules in test data that have never been seen in training data.",2,0.9807692307692307,2,26,2003
"An alternative, proposed in Charniak (1997), is to limit parsing to those contextfree rules seen in training data.",2,0.9130434782608695,2,23,2003
A lexicalized rule is predicted in two steps.,2,0.7222222222222222,1,9,2003
"First, the whole context-free rule is generated.",2,0.6111111111111112,3,9,2003
"Second, the lexical items are lled in.",2,0.7222222222222222,2,9,2003
"The estimation technique used in Charniak (1997) for the CF rule probabilities interpolates several estimates, the lowest being P(Ln .",3,0.78,2,25,2003
.,0,1,0,1,2003
.,0,1,0,1,2003
L1HR1 .,0,0,0,2,2003
.,0,1,0,1,2003
.,0,1,0,1,2003
Rm) | P).,1,0,1,6,2003
Any rules not seen in training data will be assigned zero probability with this model.,2,0.84375,2,16,2003
Parse trees in test data will be limited to include rules seen in training.,2,1.0333333333333334,1,15,2003
A problem with this approach is coverage.,2,0.9375,1,8,2003
"As shown in this section, many test data sentences will require rules that have not been seen in training.",2,0.9761904761904762,2,21,2003
This gives motivation for breaking down rules into smaller components.,2,1.0909090909090908,2,11,2003
This section motivates the need to break down rules from four perspectives.,2,0.9230769230769231,2,13,2003
"First, we discuss how the Penn Treebank annotation style leads to a very large number of grammar rules.",2,0.8,2,20,2003
"Second, we assess the extent of the coverage problem by looking at rule frequencies in training data.",2,0.8947368421052632,2,19,2003
"Third, we conduct experiments to assess the impact of the coverage problem on accuracy.",2,0.9375,2,16,2003
"Fourth, we discuss how breaking rules down may improve estimation as well as coverage.",2,1.0,2,16,2003
7.4.1 The Penn Treebank Annotation Style Leads to Many Rules.,2,0.6818181818181818,1,11,2003
"The atness of the Penn Treebank annotation style has already been discussed, in section 7.3.",2,0.6764705882352942,2,17,2003
"The atness of the trees leads to a very large (and constantly growing) number of rules, primarily because the number of adjuncts to a head is potentially unlimited.",3,0.90625,3,32,2003
"For example, there can be any number of PP adjuncts to a head verb.",2,1.0,2,16,2003
A binary-branching (Chomsky adjunction) grammar can generate an unlimited number of adjuncts with very few rules.,2,0.868421052631579,2,19,2003
"For example, the following grammar generates any sequence VP  V NP PP*.",3,0.7,2,15,2003
"Other adverbial adjuncts, such as adverbial phrases or adverbial SBARs, can also modify a verb several times, and all of these different types of adjuncts can be seen together in the same rule.",3,0.7702702702702703,3,37,2003
The result is a combinatorial explosion in the number of rules.,2,0.875,1,12,2003
"To give a avor of this, here is a random sample of rules of the format VP  VB modifier* that occurred only once in sections 221 of the Penn Treebank.",3,0.8333333333333334,3,33,2003
It is not only verb phrases that cause this kind of combinatorial explosion.,2,1.25,2,14,2003
"Other phrases, in particular nonrecursive noun phrases, also contribute a huge number of rules.",3,0.6764705882352942,2,17,2003
The next section considers the distributional properties of the rules in more detail.,2,0.75,2,14,2003
"Note that there is good motivation for the Penn Treebanks decision to represent rules in this way, rather than with rules expressing Chomsky adjunction (i.e., a schema in which complements and adjuncts are separated, through rule types (cid.6)VP  VB {complement}*(cid.7) and (cid.6)VP  VP {adjunct}(cid.7)).",4,0.9225352112676056,3,71,2003
"First, it allows the argument/adjunct distinction for PP modiers to verbs to be left undened.",2,1.0294117647058822,3,17,2003
This distinction was found to be very difcult for annotators.,1,1.0,1,11,2003
"Second, in the surface ordering (as opposed to deep structure), adjuncts are often found closer to the head than complements, thereby yielding structures that fall outside the Chomsky adjunction schema.",3,1.0138888888888888,3,36,2003
"For example, a rule such as (cid.6)VP  VB NP-C PP SBAR-C(cid.7) is found very frequently in the Penn Treebank; SBAR complements nearly always extrapose over adjuncts.",4,0,3,37,2003
7.4.2 Quantifying the Coverage Problem.,1,0,1,6,2003
"To quantify the coverage problem, rules were collected from sections 221 of the Penn Treebank.",2,0,2,17,2003
"Punctuation was raised as high as possible in the tree, and the rules did not have complement markings or the distinction between base-NPs and recursive NPs.",3,0,2,28,2003
"Under these conditions, 939,382 rule tokens were collected; there were 12,409 distinct rule types.",3,0,2,17,2003
We also collected the count for each rule.,2,0.8333333333333334,1,9,2003
Table 10 shows some statistics for these rules.,1,0.8333333333333334,1,9,2003
"A majority of rules in the grammar (6,765, or 54.5%) occur only once.",3,0.8611111111111112,2,18,2003
These rules account for 0.72% of rules by token.,2,0.9545454545454546,2,11,2003
"That is, if one of the 939,382 rule tokens in sections 221 of the treebank were drawn at random, there would be a 0.72% chance of its being the only instance of that rule in the 939,382 tokens.",3,0.8452380952380952,3,42,2003
"On the other hand, if a rule were drawn at random from the 12,409 rules in the grammar induced from those sections, there would be a 54.5% chance of that rules having occurred only once.",4,0.8589743589743589,3,39,2003
The percentage by token of the one-count rules is an indication of the coverage problem.,2,0.84375,2,16,2003
"From this estimate, 0.72% of all rules (or 1 in 139 rules) required in test data would never have been seen in training.",3,0.875,4,28,2003
It was also found that 15.0% (1 in 6.67) of all sentences have at least one rule that occurred just once.,3,1.06,3,25,2003
"This gives an estimate that roughly 1 in 6.67 sentences in test data will not be covered by a grammar induced from 40,000 sentences in the treebank.",2,1.0357142857142858,3,28,2003
"If the complement markings are added to the nonterminals, and the base-NP/nonrecursive NP distinction is made, then the coverage problem is made worse.",5,0.6538461538461539,3,26,2003
Table 11 gives the statistics in this case.,1,0.8333333333333334,2,9,2003
"By our counts, 17.1% of all sentences (1 in 5.8 sentences) contain at least 1 one-count rule.",3,0.75,3,22,2003
"Statistics for rules taken from sections 221 of the treebank, with complement markings not included on nonterminals.",2,0.9210526315789473,2,19,2003
7.4.3 The Impact of Coverage on Accuracy.,1,1,1,8,2003
Parsing experiments were used to assess the impact of the coverage problem on parsing accuracy.,2,1.03125,2,16,2003
"Section 0 of the treebank was parsed with models 1 and 2 as before, but the parse trees were restricted to include rules already seen in training data.",3,0.75,2,30,2003
Table 12 shows the results.,1,0.75,1,6,2003
"Restricting the rules leads to a 0.5% decrease in recall and a 1.6% decrease in precision for model 1, and a 0.9% decrease in recall and a 2.0% decrease in precision for model 2.",3,0,4,40,2003
7.4.4 Breaking Down Rules Improves Estimation.,2,1.0714285714285714,0,7,2003
Coverage problems are not the only motivation for breaking down rules.,2,0.9166666666666666,2,12,2003
The method may also improve estimation.,1,0.9285714285714286,1,7,2003
"To see this, consider the rules headed by told, whose counts are shown in Table 13.",2,1.0789473684210527,2,19,2003
"Estimating the probability P(Rule | VP, told) using Charniaks (1997) method would interpolate two maximum-likelihood estimates.",4,0.9130434782608695,3,23,2003
"Estimation interpolates between the specic, lexically sensitive distribution in Table 13 and the nonlexical estimate based on just the parent nonterminal, VP.",3,0.78,3,25,2003
"There are many different rules in the more specic distribution (26 different rule types, out of 147 tokens in which told was a VP head), and there are several one-count rules (11 cases).",5,0.775,5,40,2003
From these statistics  would have to be relatively low.,2,1.1,2,10,2003
This estimation method is missing a crucial generalization.,2,0.6111111111111112,1,9,2003
"In spite of there being many different rules, the distribution over subcategorization frames is much sharper.",3,0.9444444444444444,2,18,2003
Told is seen with only ve subcategorization frames in training data.,2,0.7083333333333334,2,12,2003
The large number of rules is almost entirely due to adjuncts or punctuation appearing after or between complements.,2,0.9736842105263158,2,19,2003
The estimation method in model 2 effectively estimates the probability of a rule as.,2,0.7666666666666667,2,15,2003
"The left and right subcategorization frames, LC and RC, are chosen rst.",4,0.5666666666666667,2,15,2003
The entire rule is then generated by Markov processes.,2,0.75,1,10,2003
"Once armed with the Plc and Prc parameters, the model has the ability to learn the generalization that told appears with a quite limited, sharp distribution over subcategorization frames.",3,0.921875,2,32,2003
"Say that these parameters are again estimated through interpolation, for example  In this case  can be quite high.",2,0.975,2,20,2003
Only ve subcategorization frames (as opposed to 26 rule types) have been seen in the 147 cases.,2,0.75,2,20,2003
"In summary, from the distributions in Table 13, the model should be quite uncertain about what rules told can appear with.",3,1.0208333333333333,3,24,2003
"It should be relatively certain, however, about the subcategorization frame.",2,0.8076923076923077,3,13,2003
Introducing subcategorization parameters allows the model to generalize in an important way about rules.,2,0.8666666666666667,2,15,2003
We have carefully isolated the core of rulesthe subcategorization framethat the model should be certain about.,2,1.0,2,17,2003
We should note that Charniaks method will certainly have some advantages in estimation.,2,1.0714285714285714,2,14,2003
"It will capture some statistical properties of rules that our independence assumptions will lose (e.g., the distribution over the number of PP adjuncts seen for a particular head).",2,0.96875,4,32,2003
"Unfortunately, because of space limitations, it is not possible to give a complete review of previous work in this article.",3,0.782608695652174,2,23,2003
In the next two sections we give a detailed comparison of the models in this article to the lexicalized PCFG model of Charniak (1997) and the history-based models of Jelinek et al.,3,0.8714285714285714,3,35,2003
"(1994), Magerman (1995), and Ratnaparkhi (1997).",3,0,3,15,2003
"For discussion of additional related work, chapter 4 of Collins (1999) attempts to give a comprehensive review of work on statistical parsing up to around 1998.",3,0.9333333333333333,3,30,2003
Of particular relevance is other work on parsing the Penn WSJ Treebank (Jelinek et al.,2,0.8235294117647058,2,17,2003
"1994; Magerman 1995; Eisner 1996a, 1996b; Collins 1996; Charniak 1997; Goodman 1997; Ratnaparkhi 1997; Chelba and Jelinek 1998; Roark 2001).",8,0,3,31,2003
"Eisner (1996a, 1996b) describes several dependency-based models that are also closely related to the models in this article.",2,1.0,2,22,2003
Collins (1996) also describes a dependency-based model applied to treebank parsing.,2,1.1785714285714286,2,14,2003
Goodman (1997) describes probabilistic feature grammars and their application to parsing the treebank.,2,0.875,2,16,2003
"Chelba and Jelinek (1998) describe an incremental, history-based parsing approach that is applied to language modeling for speech recognition.",2,0.8260869565217391,3,23,2003
History-based approaches were introduced to parsing in Black et al.,2,1.0,1,11,2003
(1992).,1,0,1,4,2003
"Roark (2001) describes a generative probabilistic model of an incremental parser, with good results in terms of both parse accuracy on the treebank and also perplexity scores for language modeling.",3,0.8088235294117647,3,34,2003
Earlier work that is of particular relevance considered the importance of relations between lexical heads for disambiguation in parsing.,3,1.075,1,20,2003
See Hindle and Rooth (1991) for one of the earliest pieces of research on this topic in the context of prepositional-phrase attachment ambiguity.,3,0.8653846153846154,3,26,2003
For work that uses lexical relations for parse disambiguation all with very promising resultssee Sekine et al.,2,0.9166666666666666,2,18,2003
"(1992), Jones and Eisner (1992a, 1992b), and Alshawi and Carter (1994).",4,0,5,21,2003
Statistical models of lexicalized grammatical formalisms also lead to models with parameters corresponding to lexical dependencies.,2,0.9705882352941176,1,17,2003
"See Resnik (1992), Schabes (1992), and Schabes and Waters (1993) for work on stochastic tree-adjoining grammars.",5,0.78,3,25,2003
Joshi and Srinivas (1994) describe an alternative supertagging model for tree-adjoining grammars.,2,0.7,3,15,2003
"See Alshawi (1996) for work on stochastic head-automata, and Lafferty, Sleator, and Temperley (1992) for a stochastic version of link grammar.",4,0.7413793103448276,3,29,2003
"De Marcken (1995) considers stochastic lexicalized PCFGs, with specic reference to EM methods for unsupervised training.",2,0.875,2,20,2003
"Seneff (1992) describes the use of Markov models for rule generation, which is closely related to the Markov-style rules in the models in the current article.",2,0.9,2,30,2003
"Finally, note that not all machine-learning methods for parsing are probabilistic.",3,0.8846153846153846,3,13,2003
See Brill (1993) and Hermjakob and Mooney (1997) for rule-based learning systems.,2,0.7941176470588235,3,17,2003
"In recent work, Chiang (2000) has shown that the models in the current article can be implemented almost unchanged in a stochastic tree-adjoining grammar.",2,0.7857142857142857,3,28,2003
Head-Driven Statistical Models for NL Parsing  (2000) has developed generative statistical models that integrate word sense information into the parsing process.,2,0.8333333333333334,2,24,2003
"Eisner (2002) develops a sophisticated generative model for lexicalized context-free rules, making use of a probabilistic model of lexicalized transformations between rules.",2,0.9423076923076923,3,26,2003
"Blaheta and Charniak (2000) describe methods for the recovery of the semantic tags in the Penn Treebank annotations, a signicant step forward from the complement/adjunct distinction recovered in model 2 of the current article.",4,0,3,38,2003
Charniak (2001) gives measurements of perplexity for a lexicalized PCFG.,2,1.0384615384615385,2,13,2003
Gildea (2001) reports on experiments investigating the utility of different features in bigram lexical-dependency models for parsing.,2,1,2,20,2003
Miller et al.,1,0,1,4,2003
"(2000) develop generative, lexicalized models for information extraction of relations.",2,0.8928571428571429,2,14,2003
The approach enhances nonterminals in the parse trees to carry semantic labels and develops a probabilistic model that takes these labels into account.,3,0.9791666666666666,2,24,2003
Collins et al.,1,0,1,4,2003
(1999) describe how the models in the current article were applied to parsing Czech.,2,1.1470588235294117,3,17,2003
Charniak (2000) describes a parsing model that also uses Markov processes to generate rules.,2,1.1470588235294117,2,17,2003
"The model takes into account much additional context (such as previously generated modiers, or nonterminals higher in the parse trees) through a maximum-entropy-inspired model.",3,0.625,3,28,2003
The use of additional features gives clear improvements in performance.,2,0.9545454545454546,1,11,2003
Collins (2000) shows similar improvements through a quite different model based on boosting approaches to reranking (Freund et al.,2,1.0869565217391304,2,23,2003
1998).,1,0,1,3,2003
An initial modelin fact Model 2 described in the current articleis used to generate N-best output.,2,0.8235294117647058,2,17,2003
The reranking approach attempts to rerank the N-best lists using additional features that are not used in the initial model.,2,0.9523809523809523,2,21,2003
The intention of this approach is to allow greater exibility in the features that can be included in the model.,2,1.0714285714285714,2,21,2003
"Finally, Bod (2001) describes a very different approach (a DOP approach to parsing) that gives excellent results on treebank parsing, comparable to the results of Charniak (2000) and Collins (2000).",3,0.8536585365853658,4,41,2003
We now give a more detailed comparison of the models in this article to the parser of Charniak (1997).,2,0.8863636363636364,2,22,2003
The model described in Charniak (1997) has two types of parameters.,2,0.9642857142857143,1,14,2003
This nonterminal could expand with any of the rules S   in the grammar.,2,0.8928571428571429,2,14,2003
"The rule probability is dened as P(S  |rose, S, VP).",2,0.8823529411764706,2,17,2003
"So the rule probability depends on the nonterminal being expanded, its headword, and also its parent.",4,0.6578947368421053,2,19,2003
The next few sections give further explanation of the differences between Charniaks models and the models in this article.,2,0.775,2,20,2003
8.1.1 Additional Features of Charniaks Model.,2,0,1,7,2003
There are some notable additional features of Charniaks model.,2,0.75,2,10,2003
"First, the rule probabilities are conditioned on the parent of the nonterminal being expanded.",2,0.84375,2,16,2003
"Our models do not include this information, although distinguishing recursive from nonrecursive NPs can be considered a reduced form of this information.",2,0.7291666666666666,3,24,2003
(See section 7.3.2 for a discussion of this distinction; the arguments in that section are also motivation for Charniaks choice of conditioning on the parent.),3,0.9310344827586207,2,29,2003
"Second, Charniak uses word-class information to smooth probabilities and reports a 0.35% improvement from this feature.",3,0.8421052631578947,3,19,2003
"Finally, Charniak uses 30 million words of text for unsupervised training.",2,0.9615384615384616,2,13,2003
A parser is trained from the treebank and used to parse this text; statistics are then collected from this machine-parsed text and merged with the treebank statistics to train a second model.,3,0.7647058823529411,3,34,2003
This gives a 0.5% improvement in performance.,2,0.9444444444444444,2,9,2003
8.1.2 The Dependency Parameters of Charniaks Model.,2,0,1,8,2003
"Though similar to ours, Charniaks dependency parameters are conditioned on less information.",2,0.8214285714285714,2,14,2003
"As noted previously, whereas our parameters are PL2(lwi | Li, lti, c, p, P, H, w, t, , LC), Charniaks parameters in our notation would be PL2(lwi | Li, P, w).",6,0.63,3,50,2003
The additional information included in our models is as follows.,2,1.0454545454545454,1,11,2003
H The head nonterminal label (VP in the previous prots/rose example).,2,0,1,14,2003
At rst glance this might seem redundant.,2,0.9375,1,8,2003
"For example, an S will usually take a VP as its head.",2,0.75,2,14,2003
"In some cases, however, the head label can vary.",3,0.625,3,12,2003
"For example, an S can take another S as its head in coordination cases.",2,0.78125,2,16,2003
"lti, t The POS tags for the head and modier words.",2,0,2,13,2003
Inclusion of these tags allows our models to use POS tags as word class information.,2,0.875,2,16,2003
Charniaks model may be missing an important generalization in this respect.,2,0.7916666666666666,2,12,2003
Charniak (2000) shows that using the POS tags as word class information in the model is important for parsing accuracy.,2,0.9130434782608695,3,23,2003
8.1.3 The Rule Parameters of Charniaks Model.,2,0,1,8,2003
"The rule parameters in Charniaks model are effectively decomposed into our L1 parameters (section 5.1), the head parameters, andin models 2 and 3the subcategorization and gap parameters.",4,0.578125,3,32,2003
This decomposition allows our model to assign probability to rules not seen in training data.,2,0.9375,2,16,2003
See section 7.4 for an extensive discussion.,1,0.8125,2,8,2003
8.1.4 Right-Branching Structures in Charniaks Model.,2,0,2,7,2003
Our models use distance features to encode preferences for right-branching structures.,1,1.0,2,12,2003
Charniaks model does not represent this information explicitly but instead learns it implicitly through rule probabilities.,3,0.7941176470588235,2,17,2003
"For example, for an NP PP PP sequence, the preference for a right-branching structure is encoded through a much higher probability for the rule NP  NP PP than for the rule NP  NP PP PP.",3,0.5394736842105263,4,38,2003
(Note that conditioning on the rules parent is needed to disallow the structure [NP [NP PP] PP]; see Johnson [1997] for further discussion.),4,0.8939393939393939,3,33,2003
This strategy does not encode all of the information in the distance measure.,2,0.8214285714285714,2,14,2003
The distance measure effectively penalizes rules NP  NPB NP PP where the middle NP contains a verb.,2,0.8611111111111112,3,18,2003
In this case the PP modication results in a dependency that crosses a verb.,2,1.1,2,15,2003
"Charniaks model is unable to distinguish cases in which the middle NP contains a verb (i.e., the PP modication crosses a verb) from those in which it does not.",3,1.196969696969697,3,33,2003
8.2 A Comparison to the Models of Jelinek et al.,2,1,2,11,2003
"(1994), Magerman (1995), and Rat naparkhi (1997)  We now make a detailed comparison of our models to the history-based models of Ratnaparkhi (1997), Jelinek et al.",3,0.881578947368421,3,38,2003
"(1994), and Magerman (1995).",2,0,2,10,2003
A strength of these models is undoubtedly the powerful estimation techniques that they use.,2,0.8666666666666667,2,15,2003
maximum-entropy modeling (in Ratnaparkhi 1997) or decision trees (in Jelinek et al.,3,0,2,16,2003
1994 and Magerman 1995).,2,0.9166666666666666,1,6,2003
"A weakness, we will argue in this section, is the method of associating parameters with transitions taken by bottom-up, shift-reduce-style parsers.",2,0.94,3,25,2003
We give examples in which this method leads to the parameters unnecessarily fragmenting the training data in some cases or ignoring important context in other cases.,2,0.9629629629629629,3,27,2003
"Similar observations have been made in the context of tagging problems using maximum-entropy models (Lafferty, McCallum, and Pereira 2001; Klein and Manning 2002).",2,0.8103448275862069,3,29,2003
We rst analyze the model of Magerman (1995) through three common examples of ambiguity.,2,0.9705882352941176,2,17,2003
"PP attachment, coordination, and appositives.",2,0,2,8,2003
"This is justied for the examples in this section, because once the jth decision is made, the following decisions are practically deterministic.",4,0.66,4,25,2003
"Equivalently, we are assuming that P(T1|S) + P(T2|S)  1, that is, that very little probability mass is lost to trees other than T1 or T2.",3,0.9615384615384616,3,39,2003
"Given these two equalities, we have isolated the decision between the two structures to the parameters P(dj|d1 .",2,0.8043478260869565,2,23,2003
.,0,1,0,1,2003
.,0,1,0,1,2003
"dj1, S) and P(ej|e1 .",2,0,2,11,2003
.,0,1,0,1,2003
.,0,1,0,1,2003
"ej1, S).",2,0.7,1,5,2003
Figure 21 shows a case of PP attachment.,1,0.8333333333333334,1,9,2003
The rst thing to note is that the PP attachment decision is made before the PP is even built.,2,0.875,3,20,2003
The decision is linked to the NP preceding the preposition.,1,0.8636363636363636,2,11,2003
whether the arc above the NP should go left or right.,2,0.7083333333333334,2,12,2003
"The next thing to note is that at least one important feature, the verb, falls outside of the conditioning context.",3,0.8913043478260869,3,23,2003
(The model considers only information up to two constituents preceding or following the location of the decision.),3,0.7,2,20,2003
"This could be repaired by considering additional context, but there is no xed bound on how far the verb can be from the decision point.",3,0.9444444444444444,2,27,2003
Note also that in other cases the method fragments the data in unnecessary ways.,2,0.9666666666666667,3,15,2003
"Cases in which the verb directly precedes the NP, or is one place farther to the left, are treated separately.",3,0.9565217391304348,2,23,2003
"Figure 22 shows a similar example, NP coordination ambiguity.",2,0.5909090909090909,2,11,2003
"Again, the pivotal decision is made in a somewhat counterintuitive location.",2,0.6538461538461539,2,13,2003
at the NP preceding the coordinator.,1,0.9285714285714286,2,7,2003
"At this point the NP following the coordinator has not been built, and its head noun is not in the contextual window.",3,0.6041666666666666,2,24,2003
Figure 23 shows an appositive example in which the head noun of the appositive NP is not in the contextual window when the decision is made.,2,0.8703703703703703,3,27,2003
These last two examples can be extended to illustrate another problem.,2,0.8333333333333334,1,12,2003
The NP after the conjunct or comma could be the subject of a following clause.,2,0.71875,2,16,2003
"For example,  (a) and (b) are two candidate structures for the same sequence of words.",3,0.7380952380952381,2,21,2003
(c) shows the rst decision (labeled ?) where the two structures differ.,1,1.0454545454545454,2,11,2003
"The arc above the NP can go either left (for verb attachment of the PP, as in (a)) or right (for noun attachment of the PP, as in (b)).",4,0.925,4,40,2003
(a) and (b) are two candidate structures for the same sequence of words.,2,0.75,2,18,2003
(c) shows the rst decision (labeled ?) where the two structures differ.,1,1.0454545454545454,2,11,2003
The arc above the NP can go either left (for high attachment (a) of the coordinated phrase) or right (for low attachment (b) of the coordinated phrase).,4,0.9027777777777778,3,36,2003
(a) and (b) are two candidate structures for the same sequence of words.,2,0.75,2,18,2003
(c) shows the rst decision (labeled ?) in which the two structures differ.,1,1.0454545454545454,2,11,2003
The arc above the NP can go either left (for high attachment (a) of the appositive phrase) or right (for noun attachment (b) of the appositive phrase).,4,0.875,3,36,2003
"in John likes Mary and Bill loves Jill, the decision not to coordinate Mary and Bill is made just after the NP Mary is built.",2,0.9629629629629629,3,27,2003
"At this point, the verb loves is outside the contextual window, and the model has no way of telling that Bill is the subject of the following clause.",3,0.8225806451612904,2,31,2003
The model is assigning probability mass to globally implausible structures as a result of points of local ambiguity in the parsing process.,3,0.8478260869565217,2,23,2003
Some of these problems can be repaired by changing the derivation order or the conditioning context.,2,0.8823529411764706,2,17,2003
"Ratnaparkhi (1997) has an additional chunking stage, which means that the head noun does fall within the contextual window for the coordination and appositive cases.",2,0.8793103448275862,2,29,2003
The models in this article incorporate parameters that track a number of linguistic phenomena.,2,1.1333333333333333,1,15,2003
"bigram lexical dependencies, subcategorization frames, the propagation of slash categories, and so on.",3,0.5588235294117647,2,17,2003
The models are generative models in which parse trees are decomposed into a number of steps in a top-down derivation of the tree  and the decisions in the derivation are modeled as conditional probabilities.,3,0.9714285714285714,3,35,2003
"With a careful choice of derivation and independence assumptions, the resulting model has parameters corresponding to the desired linguistic phenomena.",3,0.6136363636363636,2,22,2003
"In addition to introducing the three parsing models and evaluating their performance on the Penn Wall Street Journal Treebank, we have aimed in our discussion (in sections 7 and 8) to give more insight into the models.",4,0.7195121951219512,3,41,2003
"their strengths and weaknesses, the effect of various features on parsing accuracy, and the relationship of the models to other work on statistical parsing.",3,0,3,27,2003
"In conclusion, we would like to highlight the following points.",2,0.9166666666666666,2,12,2003
"Section 7.1 showed, through an analysis of accuracy on different types of dependencies, that adjuncts are the main sources of error in the parsing models.",3,0.8928571428571429,3,28,2003
"In contrast, dependencies forming the core structure of sentences (for example, dependencies involving complements, sentential heads, and NP chunks) are all recovered with over 90% precision and recall.",3,0.8194444444444444,5,36,2003
Section 7.2 evaluated the effect of the distance measure on parsing accuracy.,2,0.9230769230769231,2,13,2003
"A model without either the adjacency distance feature or subcategorization parameters performs very poorly (76.5% precision, 75% recall), suggesting that the adjacency feature is capturing some subcategorization information in the model 1 parser.",3,0.6625,3,40,2003
"The results in Table 7 show that the subcategorization, adjacency, and verb-crossing features all contribute signicantly to model 2s (and by implication model 3s) performance.",3,0.8333333333333334,4,30,2003
"Section 7.3 described how the three models are well-suited to the Penn Treebank style of annotation, and how certain phenomena (particularly the distance features) may fail to be modeled correctly given treebanks with different annotation styles.",3,0.925,4,40,2003
This may be an important point to bear in mind when applying the models to other treebanks or other languages.,2,0.9761904761904762,2,21,2003
"In particular, it may be important to perform transformations on some structures in treebanks with different annotation styles.",2,1.0,2,20,2003
"Section 7.4 gave evidence showing the importance of the models ability to break down the context-free rules in the treebank, thereby generalizing to produce new rules on test examples.",3,0.9354838709677419,3,31,2003
Table 12 shows that precision on section 0 of the treebank decreases from 89.0% to 87.0% and recall decreases from 88.8% to 87.9% when the model is restricted to produce only those context-free rules seen in training data.,4,0.8255813953488372,3,43,2003
"Section 8 discussed relationships to the generative model of Charniak (1997) and the history-based (conditional) models of Ratnaparkhi (1997), Jelinek et al.",3,0.8166666666666667,3,30,2003
"(1994), and Magerman (1995).",2,0,2,10,2003
"Although certainly similar to Charniaks model, the three models in this article have some signicant differences, which are identied in section 8.1.",3,0.9,2,25,2003
"(Another important differencethe ability of models 1, 2, and 3 to generalize to produce context-free rules not seen in training datawas described in section 7.4.)",3,0.7333333333333333,3,30,2003
"Section 8.2 showed that the parsing models of Ratnaparkhi (1997), Jelinek et al.",2,0.9117647058823529,3,17,2003
"(1994), and Magerman (1995) can suffer from very similar problems to the label bias or observation bias problem observed in tagging models, as described in Lafferty, McCallum, and Pereira (2001) and Klein and Manning (2002).",5,0.7083333333333334,3,48,2003
"My Ph.D. thesis is the basis of the work in this article; I would like to thank Mitch Marcus for being an excellent Ph.D. thesis adviser, and for contributing in many ways to this research.",3,0.7763157894736842,3,38,2003
"I would like to thank the members of my thesis committeeAravind Joshi, Mark Liberman, Fernando Pereira, and Mark Steedmanfor the remarkable breadth and depth of their feedback.",4,0.7096774193548387,2,31,2003
"The work beneted greatly from discussions with Jason Eisner, Dan Melamed, Adwait Ratnaparkhi, and Paola Merlo.",3,0.675,2,20,2003
Thanks to Dimitrios Samaras for giving feedback on many portions of the work.,2,1.0357142857142858,2,14,2003
"I had discussions with many other people at IRCS, University of Pennsylvnia, which contributed quite directly to this research.",2,0.9090909090909091,2,22,2003
"Breck Baldwin, Srinivas Bangalore, Dan Bikel, James Brooks, Mickey Chandresekhar, David Chiang, Christy Doran, Kyle Hart, Al Kim, Tony Kroch, Robert Macintyre, Max Mintz, Tom Morton, Martha Palmer, Jeff Reynar, Joseph Rosenzweig, Anoop Sarkar, Debbie Steinig, Matthew Stone, Ann Taylor, John Trueswell, Bonnie Webber, Fei Xia, and David Yarowsky.",23,0,2,73,2003
There was also some crucial input from sources outside of Penn.,2,1.0416666666666667,2,12,2003
In the summer of 1996 I worked at BBN Technologies.,2,0.9545454545454546,1,11,2003
"discussions with Scott Miller, Richard Schwartz, and Ralph Weischedel had a deep inuence on the research.",3,0.7105263157894737,2,19,2003
Manny Rayner and David Carter from SRI Cambridge supervised my masters thesis at Cambridge University.,2,0.71875,2,16,2003
Their technical supervision was the beginning of this research.,2,0.75,1,10,2003
"Finally, thanks to the anonymous reviewers for their comments.",2,0,2,11,2003
 PARAMETER ESTIMATION FOR STATISTICAL PARSING MODELS.,1,0,1,7,2005
THEORY AND PRACTICE OF DISTRIBUTION-FREE METHODS  A fundamental problem in statistical parsing is the choice of criteria and algorithms used to estimate the parameters in a model.,2,0.9642857142857143,2,28,2005
The predominant approach in computational linguistics has been to use a parametric model with some variant of maximum-likelihood estimation.,2,0.85,2,20,2005
The assumptions under which maximum-likelihood estimation is justied are arguably quite strong.,2,1.0,2,13,2005
"This paper discusses the statistical theory underlying various parameter-estimation methods, and gives algorithms which depend on alternatives to (smoothed) maximumlikelihood estimation.",3,0.92,3,25,2005
We rst give an overview of results from statistical learning theory.,2,0.875,1,12,2005
"We then show how important concepts from the classication literature  specically, generalization results based on margins on training data  can be derived for parsing models.",2,1.0925925925925926,4,27,2005
"Finally, we describe parameter estimation algorithms which are motivated by these generalization bounds.",2,0.9333333333333333,2,15,2005
A fundamental problem in statistical parsing is the choice of criteria and algorithms used to estimate the parameters in a model.,2,0.8636363636363636,2,22,2005
"The predominant approach in computational linguistics has been to use a parametric model with maximum-likelihood estimation, usually with some method for smoothing parameter estimates to deal with sparse data problems.",2,0.875,3,32,2005
"Methods falling into this category include Probabilistic Context-Free Grammars and Hidden Markov Models, Maximum Entropy models for tagging and parsing, and recent work on Markov Random Fields.",4,0.6166666666666667,3,30,2005
"This paper discusses the statistical theory underlying various parameterestimation methods, and gives algorithms which depend on alternatives to  (smoothed) maximum-likelihood estimation.",3,0.92,3,25,2005
"The assumptions under which maximum-likelihood estimation is justied are arguably quite strong  in particular, an assumption is made that the structure of the statistical process generating the data is known (for example, maximumlikelihood estimation for PCFGs is justied providing that the data was actually generated by a PCFG).",3,0.9716981132075472,3,53,2005
"In contrast, work in computational learning theory has concentrated on models with the weaker assumption that training and test examples are generated from the same distribution, but that the form of the distribution is unknown.",3,0.8289473684210527,3,38,2005
in this sense the results hold across all distributions and are called distribution-free.,2,0.9285714285714286,2,14,2005
"The result of this work  which goes back to results in statistical learning theory by Vapnik (1998) and colleagues, and to work within Valiants PAC model of learning (Valiant, 1984)  has been the development of algorithms and theory which provide radical alternatives to parametric maximum-likelihood methods.",4,0.9622641509433962,4,53,2005
"These algorithms are appealing in both theoretical terms, and in their impressive results in many experimental studies.",3,0.6052631578947368,3,19,2005
"In the rst part of this paper (sections 2 and 3) we describe linear models for parsing, and give an example of how the usual maximum-likelihood estimates for PCFGs can be sub-optimal.",4,0.8055555555555556,3,36,2005
"Sections 4, 5 and 6 describe the basic framework under which we will analyse parameter estimation methods.",2,0.8947368421052632,3,19,2005
"This is essentially the framework advocated by several books on learning theory (see Devroye et al., 1996; Vapnik, 1998; Cristianini and Shawe-Taylor, 2000).",3,0.9838709677419355,4,31,2005
As a warm-up section 5 describes statistical theory for the simple case of nite hypothesis classes.,2,0.6764705882352942,2,17,2005
Section 6 then goes on to the important case of hyperplane classiers.,2,0.8076923076923077,2,13,2005
"Section 7 describes how concepts from the classication literature  specically, generalization results based on margins on training data  can be derived for linear models for parsing.",2,1.125,3,28,2005
Section 8 describes parameter estimation algorithms motivated by these results.,2,0.7727272727272727,1,11,2005
"Section 9 gives pointers to results in the literature using the algorithms, and also discusses relationships to Markov Random Fields or maximum-entropy models (Ratnaparkhi et al., 1994; Johnson et al., 1999; Lafferty et al., 2001).",4,0.875,6,44,2005
In this section we introduce the framework for the learning problem that is studied in this paper.,2,0.9444444444444444,2,18,2005
The task is to learn a function F .,1,0.8888888888888888,1,9,2005
"X  Y where X is some set of possible inputs (for example a set of possible sentences), and Y is a domain of possible outputs (for example a set of parse trees).",3,0.9736842105263158,4,38,2005
this can be achieved by xing some arbitrary ordering on the set Y.),1,1.1785714285714286,2,14,2005
"Several natural language problems can be seen to be special cases of this framework, through different denitions of GEN and .",2,0.9090909090909091,1,22,2005
In the next section we show how weighted context-free grammars are one special case.,2,0.8,2,15,2005
"Tagging problems can also be framed in this way (e.g., Collins, 2002b).",2,0.8529411764705882,2,17,2005
in this case GEN(x) is all possible tag sequences for an input sentence x.,2,0.7941176470588235,2,17,2005
"In (Johnson et al., 1999), GEN(x) is the set of parses for a sentence x under an LFG grammar, and the representation  can track arbitrary features of these parses.",4,0.8026315789473685,3,38,2005
"In (Ratnaparkhi et al., 1994; Collins, 2000; Collins and Duffy, 2002) GEN(x) is the top N parses from a rst pass statistical model, and the representation  tracks the log-probability assigned by the rst pass model together with arbitrary additional features of the parse trees.",5,0.7946428571428571,3,56,2005
Walker et al.,1,0,1,4,2005
(2001) show how the approach can be applied to NLP generation.,2,1.0714285714285714,2,14,2005
"in this case x is a semantic representation, y is a surface string, and GEN is a deterministic system that maps x to a number of candidate surface realizations.",4,0.71875,2,32,2005
"The framework can also be considered to be a generalization of multi-class classication problems, where for all inputs x, GEN(x) is a xed set of k labels {1, 2, .",3,0.8947368421052632,2,38,2005
.,0,1,0,1,2005
.,0,1,0,1,2005
", k} (e.g., see Crammer and Singer, 2001; Elisseeff et al., 1999).",3,0.725,4,20,2005
"Say we have a context-free grammar (see (Hopcroft and Ullman, 1979) for a formal denition) G = (N, , R, S) where N is a set of non-terminal symbols,  is an alphabet, R is a set of rules of the form X  Y1Y2    Yn for n  0, X  N, Yi  (N  ), and S is a distinguished start symbol in N. The grammar denes a set of possible strings, and possible string/tree pairs, in a language.",8,0.8051948051948052,3,77,2005
"We use GEN(x) for all x   to denote the set of possible trees  For convenience we will take the rules in R to be placed in some arbitrary ordering r1, .",2,1.1,2,35,2005
.,0,1,0,1,2005
.,0,1,0,1,2005
", rn.",1,0,0,3,2005
"A weighted grammar G = (N, , R, S, ) also includes a parameter vector   n which assigns a weight to each rule in R. the i-th component of  is the weight of rule ri.",4,0.825,3,40,2005
"Given a sentence x and a tree y spanning the sentence, we assume a function (x, y) which tracks the counts of the rules in (x, y).",3,0.8571428571428571,3,35,2005
"Specically, the i-th component of (x, y) is the number of times rule ri is seen in (x, y).",2,1.0,3,27,2005
"Under these denitions, the weighted contextfree grammar denes a function h from sentences to trees.",2,0.6764705882352942,2,17,2005
"Finding h(x), the parse with the largest weight, can be achieved in polynomial time using the CKY parsing algorithm (in spite of a possibly exponential number of members of GEN(x)), assuming that the weighted CFG can be converted to an equivalent weighted CFG in Chomsky Normal Form.",3,0.7966101694915254,4,59,2005
"In this paper we consider the structure of the grammar to be xed, the learning problem being reduced to setting the values of the parameters .",2,0.9444444444444444,2,27,2005
A basic question is as follows.,2,1.0,1,7,2005
"given a training sample of sentence/tree pairs {(x1, y1), .",3,0.6333333333333333,3,15,2005
.,0,1,0,1,2005
.,0,1,0,1,2005
", (xm, ym)}, what criterion should be used to set the weights in the grammar?",3,1.0476190476190477,3,21,2005
"A very common method  that of Probabilistic Context-Free Grammars (PCFGs)  uses the parameters to dene a distribution P (x, y|) over possible sentence/tree pairs in the grammar.",3,0.7941176470588235,2,34,2005
Maximum likelihood estimation is used to set the weights.,2,0.9,1,10,2005
"We will consider the assumptions under which this method is justied, and argue that these assumptions are likely to be too strong.",2,1.0833333333333333,3,24,2005
We will also give an example to show how PCFGs can be badly mislead when the assumptions are violated.,2,1.25,2,20,2005
"As an alternative we will propose distribution-free methods for estimating the weights, which are justied under much weaker assumptions, and can give quite different estimates of the parameter values in some situations.",3,0.8714285714285714,3,35,2005
"We would like to generalize weighted context-free grammars by allowing the representation (x, y) to be essentially any feature-vector representation of the tree.",2,0.9629629629629629,3,27,2005
"There is still a grammar G, dening a set of candidates GEN(x) for each sentence.",2,0.75,3,20,2005
The parameters of the parser are a vector .,2,0.8333333333333334,1,9,2005
The parsers output is dened in the same way as equation (1.1).,2,0.8333333333333334,2,15,2005
The important thing in this generalization is that the representation  is now not necessarily directly tied to the productions in the grammar.,2,0.8260869565217391,3,23,2005
"This is essentially the approach advocated by (Ratnaparkhi et al., 1994; Abney, 1997; Johnson et al., 1999), although the criteria that we will propose for setting the parameters  are quite different.",4,1.0,5,40,2005
"While supercially this might appear to be a minor change, it introduces two major challenges.",3,0.9117647058823529,2,17,2005
The rst problem is how to set the parameter values under these general representations.,2,0.8,2,15,2005
"The PCFG method described in the next  section, which results in simple relative frequency estimators of rule weights, is not applicable to more general representations.",4,0.75,2,28,2005
"A generalization of PCFGs, Markov Random Fields (MRFs), has been proposed by several authors (Ratnaparkhi et al., 1994; Abney, 1997; Johnson et al., 1999; Della Pietra et al., 1997).",4,0.8488372093023255,3,43,2005
"In this paper we give several alternatives to MRFs, and we describe the theory and assumptions which underly various models.",3,0,2,22,2005
The second challenge is that now that the parameters are not tied to rules in the grammar the CKY algorithm is not applicable  in the worst case we may have to enumerate all members of GEN(x) explicitly to nd the highestscoring tree.,3,0.9565217391304348,4,46,2005
One practical solution is to dene the grammar G as a rst pass statistical parser which allows dynamic programming to enumerate its top N candidates.,2,0.8076923076923077,3,26,2005
A second pass uses the more complex representation  to choose the best of these parses.,2,0.8125,2,16,2005
"This is the approach used in several papers (e.g., Ratnaparkhi et al., 1994; Collins, 2000; Collins and Duffy, 2002).",4,0.9107142857142857,5,28,2005
This section reviews the basic theory underlying Probabilistic Context-Free Grammars (PCFGs).,2,0.75,2,14,2005
"Say we have a context-free grammar G = (N, , R, S) as dened in section 2.1.",5,1.0227272727272727,2,22,2005
"We will use T to denote the set of all trees generated by G. Now say we assign a weight p(r) in the range 0 to 1 to each rule r in R. Assuming some arbitrary ordering r1, .",2,1.1333333333333333,3,15,2005
.,0,1,0,1,2005
.,0,1,0,1,2005
", rn of the n rules in R, we use  to denote a vector of parameters,  = hlog p(r1), log p(r2), .",3,0.765625,4,32,2005
.,0,1,0,1,2005
.,0,1,0,1,2005
", log p(rn)i.",2,0,1,7,2005
"If c(T, r) is the number of times rule r is seen in a tree T , then the probability of a tree T can be written as.",3,0.9848484848484849,3,33,2005
We can now study how to train the grammar from a training sample of trees.,2,1.0625,2,16,2005
"Say there is a training set of trees {T1, T2, .",2,1.0,3,14,2005
.,0,1,0,1,2005
.,0,1,0,1,2005
", Tm}.",1,0,0,4,2005
The log-likelihood  of the training set given parameters  is L() = Pj log P (Tj|).,2,0.7619047619047619,2,21,2005
"The  maximum-likelihood estimates are to take  = arg max L(), where  is the set of allowable parameter settings (i.e., the parameter settings which obey the constraints in Booth and Thompson, 1973).",3,0.8205128205128205,3,39,2005
So under what circumstances is maximum-likelihood estimation justied?,1,1.2777777777777777,2,9,2005
"Say there is a true set of weights , which dene an underlying distribution P (T |), and that the training set is a sample of size m from this distribution.",3,0.9428571428571428,4,35,2005
"Then it can be shown that as m increases to innity, then with probability 1 the parameter estimates  converge to values which give the same distribution over trees as the true parameter values .",3,1.1,3,35,2005
"To illustrate the deciencies of PCFGs, we give a simple example.",3,0,2,13,2005
"Say we have a random process which generates just 3 trees, with probabilities {p1, p2, p3}, as shown in gure 1.1a.",2,1.0,3,28,2005
The training sample will consist of a set of trees drawn from this distribution.,2,0.9,1,15,2005
"A test sample will be generated from the same distribution, but in this case the trees will be hidden, and only the surface strings will be seen (i.e., haaaai, haaai and hai with probabilities p1, p2, p3 respectively).",4,0,3,47,2005
We would like to learn a weighted CFG with as small error as possible on a randomly drawn test sample.,2,0.8571428571428571,3,21,2005
"As the size of the training sample goes to innity, the relative frequencies of trees {T1, T2, T3} in the training sample will converge to {p1, p2, p3}.",4,0.7105263157894737,3,38,2005
This makes it easy to calculate the rule weights that maximum-likelihood estimation converges to  see gure 1.1b.,1,1.25,3,18,2005
We will call the PCFG with these asymptotic weights the asymptotic PCFG.,2,0.7307692307692307,2,13,2005
"Notice that the grammar generates trees never seen in training data, shown in gure 1.1c.",1,0.9705882352941176,2,17,2005
The grammar is ambiguous for strings haaaai (both T1 and T4 are possible) and haaai (T2 and T5 are possible).,3,0.98,3,25,2005
"In fact, under certain conditions T4 and T5 will get higher probabilities under the asymptotic PCFG than T1 and T2, and both strings haaaai and haaai will be mis-parsed.",4,0.578125,3,32,2005
"Figure 1.1d shows the distribution of the asymptotic PCFG over the 8 trees when p1 = 0.2, p2 = 0.1 and p3 = 0.7.",4,1.037037037037037,3,27,2005
"In this case both ambiguous strings are mis-parsed by the asymptotic PCFG, resulting in an expected error rate of (p1 + p2) = 30% on newly drawn test examples.",3,0.7352941176470589,4,34,2005
Figure 1.1d.,0,1,1,3,2005
"The probabilities assigned to the trees as the training size goes to innity, for p1 = 0.2, p2 = 0.1, p3 = 0.7.",3,0.9814814814814815,3,27,2005
"Notice that P (T4) > P (T1), and P (T5) > P (T2), so the induced PCFG will incorrectly map haaaai to T4 and haaai to T2.",4,0.9210526315789473,2,38,2005
"This is a striking failure of the PCFG when we consider that it is easy to derive weights on the grammar rules which parse both training and test examples with no errors.2 On this example there exist weighted grammars which make no errors, but the maximum likelihood estimation method will fail to nd these weights, even with unlimited amounts of training data.",4,0.9318181818181818,2,66,2005
The next 4 sections of this chapter describe theoretical results underlying the parameter estimation algorithms in section 8.,2,0.6842105263157895,2,19,2005
In sections 4.1 to 4.3 we describe the basic framework under which we will analyse the various learning approaches.,2,0.975,2,20,2005
"In section 5 we describe analysis for a simple case, nite hypothesis classes, which will be useful for illustrating ideas and intuition underlying the methods.",2,0.9285714285714286,2,28,2005
In section 6 we describe analysis of hyperplane classiers.,2,0.95,1,10,2005
In section 7 we describe how the results for hyperplane classiers can be generalized to apply to the linear models introduced in section 2.,2,1.02,2,25,2005
This section introduces a general framework for supervised learning problems.,2,0.6818181818181818,2,11,2005
"There are several books (Devroye et al., 1996; Vapnik, 1998; Cristianini and Shawe-Taylor, 2000) which cover the material in detail.",4,0.9642857142857143,4,28,2005
"We will use this framework to analyze both parametric methods (PCFGs, for example), and the distributionfree methods proposed in this paper.",3,0.8846153846153846,4,26,2005
We assume the following.,1,0.9,1,5,2005
An input domain X and an output domain Y.,2,0,2,9,2005
The task will be to learn a function mapping each element of X to an element of Y.,1,1.0833333333333333,1,18,2005
"In parsing, X is a set of possible sentences and Y is a set of possible trees.",3,0.8157894736842105,2,19,2005
"There is some underlying probability distribution D(x, y) over X  Y.",2,0.8333333333333334,2,15,2005
The distribution is used to generate both training and test examples.,2,0.8333333333333334,1,12,2005
"It is an unknown distribution, but it is constant across training and test examples  both training and test examples are drawn independently, identically distributed from D(x, y).",3,0.7794117647058824,3,34,2005
"There is a loss function L(y, y) which measures the cost of proposing an output y when the true output is y.",1,1.0384615384615385,2,26,2005
"A commonly used cost is the 0-1 loss L(y, y) = 0 if y = y, and L(y, y) = 1 otherwise.",4,0.796875,2,32,2005
We will concentrate on this loss function in this paper.,2,0.8636363636363636,2,11,2005
Under 0-1 loss this is the expected proportion of errors that the hypothesis makes on examples drawn from the distribution D. We would like to learn a function whose expected loss is as low as possible.,2,0.9090909090909091,2,22,2005
Er(h) is a measure of how successful a function h is.,2,1.0,2,15,2005
"Unfortunately, because we do not have direct access to the distribution D, we cannot explicitly calculate the expected loss of a hypothesis.",3,0.7307692307692307,3,26,2005
"The training set is a sample of m pairs {(x1, y1), .",3,0.6176470588235294,2,17,2005
.,0,1,0,1,2005
.,0,1,0,1,2005
", (xm, ym)} drawn from the distribution D. This is the only information we have about D.  Finally, a useful concept is the Bayes Optimal hypothesis, which we will denote as hB.",3,0,2,13,2005
"It is dened as hB(x) = arg maxyY D(x, y).",2,0.9166666666666666,1,18,2005
The Bayes optimal hypothesis simply outputs the most likely y under the distribution D for each input x.,1,0.75,2,18,2005
It is easy to prove that this function minimizes the expected loss Er(h) over the space of all possible functions  the Bayes optimal hypothesis cannot be improved upon.,2,0.8787878787878788,4,33,2005
"Unfortunately, in general we do not know D(x, y), so the Bayes optimal hypothesis, while useful as a theoretical construct, cannot be obtained directly in practice.",4,0.7361111111111112,4,36,2005
"Given that the only access to the distribution D(x, y) is indirect, through a training sample of nite size m, the learning problem is to nd a hypothesis whose expected risk is low, using only the training sample as evidence.",4,0.8229166666666666,3,48,2005
"Parametric models attempt to solve the supervised learning problem by explicitly modeling either the joint distribution D(x, y) or the conditional distributions D(y|x) for all x.",3,0.7428571428571429,2,35,2005
"In the joint distribution case, there is a parameterized probability distribution P (x, y|).",3,0.525,2,20,2005
As the parameter values  are varied the distribution will also vary.,3,0.9166666666666666,1,12,2005
"The parameter space  is a set of possible parameter values for which  P (x, y|) is a well-dened distribution (i.e., for which Px,y P (x, y|) = 1).",3,0.8214285714285714,4,42,2005
"A crucial assumption in parametric approaches is that there is some    such that D(x, y) = P (x, y|).",2,0.9107142857142857,2,28,2005
"Because of this, if we consider the function h(x) = arg maxyY P (x, y| ), then in the limit h(x) will converge to the Bayes optimal function hB(x).",4,0.7272727272727273,5,44,2005
"So under the assumption that D(x, y) = P (x, y|) for some   , and with innite amounts of training data, the maximumlikelihood method is provably optimal.",6,0.7837837837837838,4,37,2005
Methods which model the conditional distribution D(y|x) are similar.,3,1.0666666666666667,2,15,2005
"The parameters now dene a conditional distribution P (y|x, ).",3,0.7,2,15,2005
"The assumption is that there is some  such that x, D(y|x) = P (y|x, ).",3,0.8461538461538461,1,26,2005
"Maximumlikelihood estimates can be dened in a similar way, and in this case the function h(x) = arg maxyY P (y|x, ) will converge to the Bayes optimal function hB(x) as the sample size goes to innity.",4,0.7346938775510204,3,49,2005
"From the arguments in the previous section, parametric methods are optimal  provided that two assumptions hold.",3,0.8333333333333334,2,18,2005
1 The distribution generating the data is in the class of distributions being  considered.,2,1.1,2,15,2005
"2 The training set is large enough for the distribution dened by the maximum likelihood estimates to converge to the true distribution D(x, y) (in general the guarantees of ML estimation are asymptotic, holding only in the limit as the training data size goes to innity).",3,0.9259259259259259,3,54,2005
This paper proposes alternatives to maximum-likelihood methods which give theoretical guarantees without making either of these assumptions.,2,1.1388888888888888,2,18,2005
"There is no assumption that the distribution generating the data comes from some predened class  the only assumption is that the same, unknown distribution generates both training and test examples.",2,0.8125,4,32,2005
"The methods also provide bounds suggesting how many training samples are required for learning, dealing with the case where there is only a nite amount of training data.",3,1.0166666666666666,2,30,2005
A crucial idea in distribution-free learning is that of a hypothesis space.,2,0.7307692307692307,2,13,2005
"This is a set of functions under consideration, each member of the set being a function h .",2,1.0,3,19,2005
X  Y.,0,1,1,2,2005
"For example, in weighted context-free grammars the hypothesis space is.",3,0.625,2,12,2005
"So each possible parameter setting denes a different function from sentences to trees, and H is the innite set of all such functions as  ranges over the parameter space n.",2,0,3,31,2005
Learning is then usually framed as the task of choosing a good function in H on the basis of a training sample as evidence.,2,0.96,2,25,2005
This strategy is called Empirical Risk Minimization (ERM) by Vapnik (1998).,2,0.84375,2,16,2005
Two questions which arise are.,2,1.5,1,6,2005
"In the limit, as the training size goes to innity, does the error of the ERM method Er(h) approach the error of the best function in the set, Er(h), regardless of the underlying distribution D(x, y)?",3,0.7941176470588235,4,51,2005
"In other words, is this method of choosing a hypothesis always consistent?",2,0.9285714285714286,2,14,2005
The answer to this depends on the nature of the hypothesis space H. For nite hypothesis spaces the ERM method is always consistent.,2,0.875,1,24,2005
"For many innite hypothesis spaces, such as the hyperplane classiers described in section 6 of this paper, the method is also consistent.",4,0.7,2,25,2005
"However, some innite hypothesis spaces can lead to the method being inconsistent  specically, if a measure called the Vapnik-Chervonenkis (VC) dimension (Vapnik and Chervonenkis, 1971) of H is innite, the ERM method may be inconsistent.",5,0.9318181818181818,5,44,2005
"Intuitively, the VC dimension can be thought of as a measure of the complexity of an innite set of hypotheses.",2,0.7954545454545454,2,22,2005
"If the method is consistent, how quickly does Er(h) converge to Er(h)?",2,1.15,2,20,2005
"In other words, how much training data is needed to have a good chance of getting close to the best function in H?",2,1.0,2,25,2005
We will see in the next section that the convergence rate depends on various measures of the size of the hypothesis space.,2,0.8260869565217391,3,23,2005
"For nite sets, the rate of convergence depends directly upon the size of H. For innite sets, several measures have been proposed  we will concentrate on rates of convergence based on a concept called the margin of a hypothesis on training examples.",2,1.0666666666666667,2,45,2005
This section gives results and analysis for situations where the hypothesis space H is a nite set.,2,0.7777777777777778,2,18,2005
This is in some ways an unrealistically simple situation  many hypothesis spaces used in practice are innite sets  but we give the results and proofs because they can be useful in developing intuition for the nature of convergence bounds.,3,0.8625,3,40,2005
In the following sections we consider innite hypothesis spaces such as weighted context-free grammars.,2,0.6333333333333333,2,15,2005
A couple of basic results from probability theory will be very useful.,2,0.8846153846153846,1,13,2005
The rst results are the Chernoff bounds.,2,0.5625,1,8,2005
"Consider a binary random variable X (such as the result of a coin toss) which has probability p of being 1, and (1  p) of being 0.",3,1.0151515151515151,4,33,2005
"Now consider a sample of size m, {x1, x2, .",2,0.8214285714285714,2,14,2005
.,0,1,0,1,2005
.,0,1,0,1,2005
", xm} drawn from this process.",2,0,1,8,2005
Dene the relative frequency of xi = 1 (the coin coming  up heads) in this sample to be p = Pi xi/m.,2,1.02,3,25,2005
"The relative frequency p is a  very natural estimate of the underlying probability p, and by the law of large numbers p will converge to p as the sample size m goes to innity.",4,0.6527777777777778,3,36,2005
"Chernoff bounds give results concerning how quickly p converges to p. Thus Chernoff bounds go a step further than the law of large numbers, which is an asymptotic result (a result concerning what happens as the sample size goes to innity).",3,1.011111111111111,2,45,2005
The bounds are.,1,0.875,0,4,2005
Theorem 1 (Chernoff Bounds).,2,0,1,7,2005
"For all p  [0, 1],  > 0, with the probability P being taken over the distribution of training samples of size m generated with underlying parameter p.  The rst bound states that for all values of p, and for all values of , if we repeatedly draw training samples of size m of a binary variable with underlying probability p, the relative proportion of training samples for which the value (p p) exceeds  is at most3 e2m2.",3,0.8636363636363636,3,33,2005
"It is always possible for p to diverge substantially from p  it is possible to draw an extremely unrepresentative training sample, such as a sample of all heads when p = 0.7, for example  but as the sample size is increased the chances of us being this unlucky become increasingly unlikely.",4,1.0833333333333333,3,54,2005
A second useful result is the Union Bound.,2,0.7222222222222222,1,9,2005
Here we use the notation P [AB] to mean the probability of A or B occurring.,2,0.9473684210526315,2,19,2005
The Union Bound follows directly from the axioms of probability theory.,2,0.9583333333333334,1,12,2005
"For example, if n = 2, then P [A1  A2] = P [A1] + P [A2]  P [A1A2]  P [A1] + P [A2], where P [A1A2] means the probability of both A1 and A2 occurring.",5,0.5754716981132075,3,53,2005
The more general result for all n follows by induction on n.  We are now in a position to apply these results to learning problems.,2,0.8846153846153846,1,13,2005
"First, consider just a single member of H, a function h. Say we draw a training sample {(x1, y1), .",3,0.7407407407407407,2,27,2005
.,0,1,0,1,2005
.,0,1,0,1,2005
", (xm, ym)} from some unknown distribution D(x, y).",3,0,3,18,2005
"We can calculate the relative frequency of errors of h on this sample,  So for any single member of H, the Chernoff bound describes how its observed error on the training set is related to its true probability of error.",3,0.8604651162790697,3,43,2005
"Now consider the entire set of hypotheses H. Say we assign an arbitrary ordering to the n = |H| hypotheses, so that H = {h1, h2, .",3,0.9848484848484849,5,33,2005
.,0,1,0,1,2005
.,0,1,0,1,2005
", hn}.",1,0,0,4,2005
Consider the probability of any one of the hypotheses hi having its estimated loss Er(hi) diverge by more than  from its expected loss Er(hi).,3,0.9838709677419355,4,31,2005
"Thus for all hypotheses h in the set H, Er(h) converges to Er(h) as the sample size m goes to innity.",3,0.8275862068965517,3,29,2005
"This result is known as a Uniform Convergence Result, in that it describes how a whole set of empirical error rates converge to their respective expected errors.",2,0.7758620689655172,3,29,2005
Note that this result holds for the hypothesis with minimum error on the training sample.,2,0.875,2,16,2005
"It can be shown that this implies that the ERM method for nite hypothesis spaces  choosing the hypothesis h which has minimum error on the training sample  is consistent, in that in the limit as m  , the error of h converges to the error of the minimum error hypothesis.",4,0.9134615384615384,3,52,2005
Another important result is how the rate of convergence depends on the size of the hypothesis space.,2,0.9444444444444444,2,18,2005
"Qualitatively, the bound implies that to avoid overtraining the number of training samples should scale with log |H|.",2,1.0,3,22,2005
"Ideally, we would like a learning method to have expected error that is close Now consider the ERM  to the loss of the bayes-optimal hypothesis hB.",2,1.0,3,28,2005
method.,0,0,0,2,2005
Breaking the error down in this way suggests that there are two components to the difference from the optimal loss Er(hB).,2,0.82,3,25,2005
"The rst term captures the errors due to a nite sample size  if the hypothesis space is too large, then theorem 3 states that there is a good chance that the ERM method will pick a hypothesis that is far from the best in the hypothesis space, and the rst term will be large.",5,0.7631578947368421,3,57,2005
"Thus the rst term indicates a pressure to keep H small, so that there is a good   chance of nding the best hypothesis in the set.",2,0.8571428571428571,2,28,2005
"In contrast, the second term reects a pressure to make H large, so that there is a good chance that at least one of the hypotheses is close to the Bayes optimal hypothesis.",2,0.8333333333333334,3,36,2005
"The two terms can be thought of as being analogues to the familiar biasvariance trade-off, the rst term being a variance term, the second being the bias.",4,0.6166666666666667,3,30,2005
In this section we describe a method which explicitly attempts to model the trade-off between these two types of errors.,2,1.119047619047619,2,21,2005
"Rather than picking a single hypothesis class, Structural Risk Minimization (Vapnik, 1998) advocates picking a set of hypothesis classes H1, H2, .",3,0.6428571428571429,3,28,2005
.,0,1,0,1,2005
.,0,1,0,1,2005
", Hs of increasing size (i.e., such that |H1| < |H2| <    < |Hs|).",3,0.9166666666666666,3,24,2005
"The following theorem then applies (it is an extension of theorem 3, and is derived in a similar way through application of the Chernoff and Union bounds).",3,0.6935483870967742,3,31,2005
"Theorem 4 Assume a set of nite hypothesis classes {H1, H2, .",3,0.7,2,15,2005
.,0,1,0,1,2005
.,0,1,0,1,2005
", Hs}, and some distribution D(x, y).",3,0,2,14,2005
"For all i = 1, .",1,0,1,7,2005
.,0,1,0,1,2005
.,0,1,0,1,2005
", s, for all hypotheses h  Hi, with probability at least 1   over the choice of training set of size m drawn from D.  This theorem is very similar to theorem 3, except that the second term in the bound now varies depending on which Hi a function h is drawn from.",3,0,3,27,2005
Note also that we pay an extra price of log(s) for our hedging over which of the hypothesis spaces the function is drawn from.,2,1.0178571428571428,3,28,2005
The SRM principle is then as follows.,2,1.0,1,8,2005
"1 Pick a set of hypothesis classes, Hi for i = 1, .",2,1.0333333333333334,3,15,2005
.,0,1,0,1,2005
.,0,1,0,1,2005
", s, of increasing size.",2,1,1,7,2005
This must be done independently of the training data for the above bound to apply.,2,1.0,2,16,2005
2 Choose the hypothesis h which minimizes the bound in theorem 4.,2,1.1538461538461537,2,13,2005
"Thus rather than simply choosing the hypothesis with the lowest error on the training sample, there is now a trade-off between training error and the size of the hypothesis space of which h is a member.",3,0.8289473684210527,3,38,2005
The SRM method advocates picking a compromise between keeping the number of training errors small versus keeping the size of the hypothesis class small.,3,1.0,2,25,2005
Note that this approach has a somewhat similar avour to Bayesian approaches.,2,0.9230769230769231,2,13,2005
The Maximum A-Posteriori (MAP) estimates in a Bayesian approach involve choosing the parameters which maximize a combination of the data likelihood and a prior over the parameter values.,2,0.9354838709677419,2,31,2005
The rst term is a measure of how well the parameters  t the data.,2,0.8666666666666667,2,15,2005
The second term is a prior which can be interpreted as a term which penalizes more complex parameter settings.,2,1.075,2,20,2005
The SRM approach in our example implies choosing the hypothesis that minimizes the bound in theorem 4.,2,1.1388888888888888,2,18,2005
"The function indicating the goodness of a hypothesis h again has two terms, one measuring how well the hypothesis ts the data, the second penalizing hypotheses which are too complex.",2,0.9242424242424242,3,33,2005
Here complexity has a very specic meaning.,2,0.8125,2,8,2005
it is a direct measure of how quickly the training data error Er(h) converges to its true value Er(h).,3,0.7692307692307693,3,26,2005
"This section describes analysis applied for binary classiers, where the set Y = {1, +1}.",3,0.95,3,20,2005
"We consider hyperplane classiers, where a linear separator in some feature space is used to separate examples into the two classes.",2,0.9347826086956522,3,23,2005
This section describes uniform convergence bounds for hyperplane classiers.,2,0.75,1,10,2005
Algorithms which explicitly minimize these bounds  namely the Support Vector Machine and Boosting algorithms  are described in section 8.,3,0.8,3,20,2005
There has been a large amount of research devoted to the analysis of hyperplane classiers.,2,0.96875,2,16,2005
"They go back to one of the earliest learning algorithms, the Perceptron algorithm (Rosenblatt, 1958).",2,0.825,3,20,2005
They are similar to the linear models for parsing we proposed in section 2 (in fact the framework of section 2 can be viewed as a generalization of hyperplane classiers).,2,1.0606060606060606,2,33,2005
"We will initially review some results applying to linear classiers, and then discuss how various results may be applied to linear models for parsing.",3,1.0,3,26,2005
"We will discuss a hypothesis space of n-dimensional hyperplane classiers,  dened as follows.",2,0.9333333333333333,3,15,2005
Each instance x is represented as a vector (x) in n.,1,0.9615384615384616,2,13,2005
There is a clear geometric interpretation of this classier.,2,0.85,2,10,2005
The points (x) are in n-dimensional Euclidean space.,2,0.7727272727272727,2,11,2005
"The parameters , b dene a hyperplane through the   space, the hyperplane being the set of points z such that (z   + b) = 0.",2,0.9655172413793104,3,29,2005
"This is a hyperplane with normal , at distance b/|||| from the origin, j .",3,0.9761904761904762,4,21,2005
This hyperplane is used to classify points.,1,1.125,1,8,2005
"all points falling on one side of the hyperplane are classied as +1, points on the other side are classied as 1.",4,0,2,24,2005
"It can be shown that the ERM method is consistent for hyperplanes, through a method called VC analysis (Vapnik and Chervonenkis, 1971).",2,0.9629629629629629,3,27,2005
"We will not go into details here, but roughly speaking, the VC-dimension of a hypothesis space is a measure of its size or complexity.",3,0,3,27,2005
A set of hyperplanes in n has VC dimension of (n + 1).,2,0.96875,2,16,2005
For any hypothesis space with nite VC dimension the ERM method is consistent.,3,0.6785714285714286,2,14,2005
An alternative to VC-analysis is to analyse hyperplanes through properties of margins on training examples.,2,1.1875,2,16,2005
"For any hyperplane dened by parameters (, b), for a training sample {(x1, y1), .",3,0,3,23,2005
.,0,1,0,1,2005
.,0,1,0,1,2005
", (xm, ym)}, the margin on the i-th training example is dened as  The margin ,b has a simple geometric interpretation.",5,0,3,28,2005
"it is the minimum distance of any training point to the hyperplane dened by , b.",1,0.90625,2,16,2005
The following theorem then holds.,2,0.75,1,6,2005
Theorem 5 (Shawe-Taylor et al.,1,1,2,7,2005
1998).,1,0,1,3,2005
"Assume the hypothesis class H is a set of hyperplanes, and that there is some distribution D(x, y) generating examples.",3,0.9807692307692307,3,26,2005
"Dene R to be a constant such that x, ||(x)||  R. For all  The bound is minimized for the hyperplane with maximum margin (i.e., maximum value for ,b) on the training sample.",3,0.8863636363636364,2,44,2005
"This bound suggests that if the training data is separable, the hyperplane with maximum margin should be chosen as the hypothesis with the best bound on its expected error.",2,0.967741935483871,3,31,2005
"It can be shown that the maximum margin hyperplane is unique, and can be found efciently using algorithms described in section 8.1.",2,0.9791666666666666,2,24,2005
"Search for the maximummargin hyperplane is the basis of Support Vector Machines (hard-margin version; Vapnik, 1998).",2,0.8095238095238095,2,21,2005
The previous theorem does not apply when the training data cannot be classied with 0 errors by a hyperplane.,2,0.7619047619047619,3,21,2005
"There is, however, a similar theorem that can be applied in the non-separable case.",2,0.9705882352941176,3,17,2005
"First, dene L(, b, ) to be the proportion of examples on training data with margin less than  for the hyperplane h,b.",2,1.0178571428571428,2,28,2005
The following theorem can now be stated.,2,0.8125,1,8,2005
"Theorem 6 Cristianini and Shawe-Taylor, 2000, Theorem 4.19.",3,0,4,11,2005
"Assume the hypothesis class H is a set of hyperplanes, and that there is some distribution D(x, y) generating examples.",3,0.9807692307692307,3,26,2005
"Let R be a constant such that x, ||(x)||  R. For all h,b  H, for all  > 0, with probability at least 1   over the choice of training set of size m drawn from D.  (The rst result of the form of Theorem 6 was given in (Bartlett 1998).",4,0.8203125,5,64,2005
This was a general result for large margin classiers; the immediate corollary that implies the above theorem was given in (Anthony and Bartlett 1999).,3,0.7321428571428571,3,28,2005
Note that Zhang (2002) proves a related theorem where the log2 m factor is replaced by log m. Note also that the square-root in the second term of theorem 6 means that this bound is in general a looser bound than the bound in theorem 5.,2,1.0612244897959184,3,49,2005
"This is one cost of moving to the case where some training samples are misclassied, or where some training samples are classied with a small margin.)",3,0.9310344827586207,3,29,2005
"This result is important in cases where a large proportion of training samples can be classied with relatively large margin, but a relatively small number of outliers make the problem inseparable, or force a small margin.",4,0.7948717948717948,3,39,2005
"The result suggests that in some cases a few examples are worth giving up on, resulting in the rst term in the bound being larger than 0, but the second term being much smaller due to a larger value for .",4,0.813953488372093,3,43,2005
"The soft margin version of Support Vector Machines (Cortes and Vapnik, 1995), described in section 8.1, attempts to explicitly manage the trade-off between the two terms in the bound.",4,0.7428571428571429,3,35,2005
"Theorem 7 (Schapire et al., 1998).",2,0,2,10,2005
"Assume the hypothesis class H is a set of hyperplanes in n, and that there is some distribution D(x, y) generating examples.",3,1.0178571428571428,3,28,2005
"This bound suggests a strategy that keeps the 1-norm of the parameters low, while trying to classify as many of the training examples as possible with large margin.",3,1.1833333333333333,3,30,2005
"It can be shown that the AdaBoost algorithm (Freund and Schapire, 1997) is an effective way of achieving this goal; its application to parsing is described in section 8.2.",4,0.8235294117647058,3,34,2005
We now consider how the theory for hyperplane classiers might apply to the linear models for parsing described in section 2.,2,1.0227272727272727,2,22,2005
"The method for converting parsing to a margin-based problem is similar to the method for ranking problems described in (Freund et al., 1998), and to the approach to multi-class classication problems in (Schapire et al., 1998; Crammer and Singer, 2001; Elisseeff et al., 1999).",4,0.8035714285714286,6,56,2005
"As a rst step, we give a denition of the margins on training examples.",2,0.78125,2,16,2005
"Assume we have a training sample {(x1, y1), .",2,0.8571428571428571,3,14,2005
.,0,1,0,1,2005
.,0,1,0,1,2005
", (xm, ym)}.",2,1.0625,1,8,2005
"Theorem 8 Assume the hypothesis class H is a set of linear models as dened in equation (1.11) and equation (1.12), and that there is some distribution D(x, y) generating examples.",3,0.926829268292683,3,41,2005
"For all h  H, for all  > 0, with probability at least 1   over the choice of training set of size m drawn from D,  where R is a constant such that x  X , y  GEN(x), z  GEN(x), ||(x, y)  (x, z)||  R. The variable N is the smallest positive integer such that x  X , |GEN(x)|  1  N.  Proof.",7,0.8939393939393939,4,66,2005
"The proof follows from results in (Zhang, 2002).",2,0.7916666666666666,2,12,2005
See the appendix of this chapter for the proof.,1,0.95,2,10,2005
Note that this is similar to the bound in theorem 6.,2,1.0833333333333333,2,12,2005
"A difference, however, is the dependence on N, a bound on the number of candidates for any example.",2,0.8863636363636364,2,22,2005
"Even though this term is logarithmic, the dependence is problematic because the number of candidate parses for a sentence will usually have an exponential dependence on the length of the sentence, leading to log N having linear  dependence on the maximum sentence length.",3,0.9239130434782609,3,46,2005
"(For example, the number of labeled binary-branching trees for a sentence of length n, with G non-terminals, is Gn(2n)!",4,0.7592592592592593,5,27,2005
(n+1)!n!,1,0,0,5,2005
", the log of this number is O(n log G + n log n).)",3,0.6578947368421053,3,19,2005
"It is an open problem whether tighter bounds  in particular, bounds which do not depend on N  can be proved.",2,1.1136363636363635,5,22,2005
"Curiously, we show in section 8.3 that the perceptron algorithm leads to a margin-based learning bound that is independent of the value for N. This suggests that it may be possible to prove tighter bounds than those in theorem 8.",2,1.0769230769230769,2,26,2005
"The bounds in theorems 8 and 9 suggested a trade-off between keeping the values for L(, ) and L1(, ) low and keeping the value of  high.",3,0.875,3,32,2005
The algorithms described in section 8 attempt to nd a hypothesis  which can achieve low values for these quantities with a high value for .,2,0.94,1,25,2005
The algorithms are direct modications of algorithms for learning hyperplane classiers for binary classication.,2,1.0,2,15,2005
these classication algorithms are motivated by the bounds in theorems 6 and 7.,2,0.6785714285714286,1,14,2005
In this section we describe parameter estimation algorithms which are motivated by the generalization bounds for linear models in section 7 of this paper.,2,0.92,2,25,2005
"The rst set of algorithms, support vector machines, use constrained optimization problems that are related to the bounds in theorems 8 and 9.",2,0.8461538461538461,2,26,2005
"The second algorithm we describe is a modication of AdaBoost (Freund and Schapire, 1997), which is motivated by the bound in theorem 9.",3,1.0178571428571428,3,28,2005
"Finally, we describe a variant of the perceptron algorithm applied to parsing.",2,1.0,2,14,2005
"The perceptron algorithm does not explicitly attempt to optimize the generalization bounds in section 7, but its convergence and generalization properties can be shown to be dependent on the existence of parameter values which separate the training data with large margin under the 2-norm.",3,0.75,3,46,2005
In this sense they are a close relative to support vector machines.,2,0.8461538461538461,2,13,2005
We now describe an algorithm which is motivated by the bound in theorem 8.,2,1.1333333333333333,2,15,2005
"First, recall the denition of the margin for the parameter values  on the i-th training example.",2,0.75,2,18,2005
"Vapnik (1998) shows that the hyperplane  is unique4, and gives a method for nding .",3,0.9444444444444444,2,18,2005
The method involves solving the following constrained optimization problem.,1,1.0,2,10,2005
Any hyperplane  satisfying these constraints separates the data with margin  = 1/||||.,2,0.9166666666666666,1,18,2005
"By minimizing ||||2 (or equivalently ||||) subject to the constraints, the method nds the parameters  with maximal value for .",3,0.8666666666666667,2,30,2005
Simply nding the maximum-margin hyperplane may not be optimal or even possible.,2,0,2,13,2005
"the data may not be separable, or the data may be noisy.",3,0,1,14,2005
The bound in theorem 8 suggests giving up on some training examples which may be difcult or impossible to separate.,2,1.0952380952380953,2,21,2005
"(Cortes and Vapnik, 1995) suggest a rened optimization task for the classication case which addresses this problem; we suggest the following modied optimization problem as a natural analogue of this approach (our approach is similar to the method for multi-class classication problems in Crammer and Singer, 2001).",4,0.6545454545454545,3,55,2005
Here we have introduced a slack variable i for each training example.,2,0.7307692307692307,3,13,2005
"At the solution of the optimization problem, the margin on the i-th training example is at least (1i)/||||.",3,0.7962962962962963,3,27,2005
"On many examples the slack variable i will be zero, and the margin i  will be at least 1/||||.",3,0,2,26,2005
"On some examples the slack variable i will be positive, implying that the algorithm has given up on separating the example with margin 1/||||.",2,1.032258064516129,2,31,2005
The constant C controls the cost for having non-zero values of i.,1,1.0,2,12,2005
"As C  , the problem becomes the same as the hardmargin SVM problem, and the method attempts to nd a hyperplane which correctly separates all examples with margin at least 1/|||| (i.e., all slack variables are 0).",3,0.851063829787234,2,47,2005
"For smaller C, the training algorithm may give up on some examples (i.e., set i > 0) in order to keep ||||2 low.",3,0.875,3,32,2005
"Thus by varying C, the method effectively modies the trade-off between the two terms in the bound in theorem 8.",3,0.7954545454545454,3,22,2005
"In practice, a common approach is to train the model for several values of C, and then to pick the classier which has best performance on some held-out set of development data.",3,0.9285714285714286,3,35,2005
"Both kinds of SVM optimization problem outlined above have been studied extensively (e.g., see Joachims, 1998; Platt, 1998) and can be solved relatively efciently.",3,1.0161290322580645,3,31,2005
"(A package for SVMs, written by Thorsten Joachims, is available from http.//ais.gmd.de/thorsten/svm light/.)",3,0.84375,2,16,2005
This can be framed as a linear programming problem.,2,0.75,2,10,2005
"See (Demiriz et al., 2001) for details, and the relationships between linear programming approaches and the boosting algorithms described in the next section.",3,0.75,4,28,2005
"The AdaBoost algorithm (Freund and Schapire, 1997) is one method for optimizing the bound for hyperplane classiers in theorem 7 (Schapire et al., 1998).",3,0.8387096774193549,4,31,2005
"This section describes a modied version of AdaBoost, applied to the parsing problem.",2,0.7666666666666667,2,15,2005
Figure 1.2 shows the modied algorithm.,1,0.6428571428571429,1,7,2005
The algorithm converts the training set into a set of triples.,2,0.9583333333333334,1,12,2005
"Each member (x, y1, y2) of T is a triple such that x is a sentence, y1 is the correct tree for that sentence, and y2 is an incorrect tree also proposed by GEN(x).",4,0.7727272727272727,3,44,2005
"AdaBoost maintains a distribution Dt over the training examples such that Dt(x, y1, y2) is proportional to exp{  ((x, y1)  (x, y2))}.",4,0.8947368421052632,4,38,2005
"Members of T which are well discriminated by the current parameter values  are given low weight by the distribution, whereas examples which are poorly discriminated are weighted more highly.",2,1.1451612903225807,4,31,2005
"The magnitude of rs can be taken as a measure of how correlated (s(x, y1)  s(x, y2)) is with the distribution Dt.",3,1.0,3,33,2005
"If it is highly correlated, |rs| will be large, and the s-th parameter will be useful in driving down the margins on the more highly weighted members of T .",3,0.8676470588235294,2,34,2005
"In the classication case, Schapire et al.",2,0,2,9,2005
"(1998) show that the AdaBoost algorithm has direct properties in terms of optimizing the value of L1(, b, ) dened in equation (1.10).",2,1.064516129032258,3,31,2005
"Unfortunately it is not possible to show that the algorithm in gure 1.2 has a similar effect on the parsing quantity L1(, ) in equation (1.15).",2,0.9516129032258065,3,31,2005
"Instead, we show its effect on a similar quantity5 RL1.",2,0.625,2,12,2005
"There is a strong relation between the values of |rs|, and the effect on the values of RL1(, ).",3,0.94,4,25,2005
If we dene t = (1  |rst|)/2 then the following theorem holds.,3,1.0263157894736843,2,19,2005
"It can be shown that f (, ) is less than one providing that  < . the implication is that for all  < , RL1(, ) decreases exponentially in the number of iterations, T .",2,1.1764705882352942,3,17,2005
"So if the AdaBoost algorithm can successfully maintain high values of |rst| for several iterations, it will be successful at minimizing RL1(, ) for a relatively large range of .",3,0.9285714285714286,3,35,2005
"Given that RL1 is related to L1, we can view this as an approximate method for optimizing the bound in theorem 9.",3,1.0625,2,24,2005
"In practice, a set of held-out data is usually used to optimize T , the number of rounds of boosting.",2,1.1136363636363635,2,22,2005
The algorithm states a restriction on the representation .,1,0.8333333333333334,1,9,2005
"For all members (x, y1, y2) of T , for s = 1, .",3,0,3,19,2005
.,0,1,0,1,2005
.,0,1,0,1,2005
", n, (s(x, y1)  s(x, y2)) must be in the range 1 to +1.",4,0.8846153846153846,3,26,2005
This is not as restrictive as it might seem.,2,1.0,2,10,2005
"If  is always strictly positive, it can be rescaled so that its components are always between 0 and +1.",2,0.8809523809523809,2,21,2005
"If some components may be negative, it sufces to rescale the components so that they are always between 0.5 and +0.5.",2,0.9565217391304348,2,23,2005
"A common use of the algorithm, as applied in (Collins, 2000), is to have the n components of  to be the values of n indicator functions, in which case all values of  are either 0 or 1, and the condition is satised.",6,0.82,3,50,2005
"The nal parameter estimation algorithm which we will describe is a variant of the perceptron algorithm, as introduced by (Rosenblatt, 1958).",3,0.8653846153846154,2,26,2005
Figure 1.3 shows the algorithm.,1,0.75,1,6,2005
Note that the main computational expense is in calculating y = h(xi) for each example in turn.,2,0.9761904761904762,3,21,2005
For weighted context-free grammars  this step can be achieved in polynomial time using the CKY parsing algorithm.,2,0.7222222222222222,2,18,2005
"Other representations may have to rely on explicitly calculating (xi, z)   for all z  GEN(xi), and hence depend computationally on the number of candidates |GEN(xi)| for i = 1, .",4,1.2209302325581395,4,43,2005
.,0,1,0,1,2005
.,0,1,0,1,2005
", m.  Proof.",1,0,1,4,2005
"See (Collins, 2002b) for a proof.",2,1.0,2,10,2005
"The proof is a simple modication of the proof for hyperplane classiers (Block, 1962; Novikoff, 1962, see also Freund and Schapire, 1999).",3,0.8666666666666667,3,30,2005
"This theorem implies that if the training sample in gure 1.3 is separable, and we iterate the algorithm repeatedly over the training sample, then the algorithm converges to a parameter setting that classies the training set with zero errors.",3,0.7380952380952381,4,42,2005
"(In particular, we need at most (R/)2 passes over the training sample before convergence.)",3,0.9090909090909091,2,22,2005
Thus we now have an algorithm for training weighted context-free grammars which will nd a zero error hypothesis if it exists.,2,1.0227272727272727,2,22,2005
"For example, the algorithm would nd a weighted grammar with zero expected error on the example problem in section 3.",2,0.7045454545454546,3,22,2005
Of course convergence to a zero-error hypothesis on training data says little about how well the method generalizes to new test examples.,2,0.9130434782608695,2,23,2005
Fortunately a second theorem gives a bound on the generalization error of the perceptron method.,2,0.71875,2,16,2005
"Theorem 12 (Direct consequence of the sample compression bound in (Littlestone and Warmuth, 1986); see also theorem 4.25, page 70, Cristianini and Shawe-Taylor, 2000).",4,0.75,3,34,2005
"Say the perceptron algorithm makes d mistakes when run to convergence over a training set of size m.   Given that d  (R/)2, this bound states that if the problem is separable with large margin  i.e., the ratio R/ is relatively small  then the perceptron will converge to a hypothesis with good expected error with a reasonable number of training examples.",3,0.9117647058823529,3,68,2005
The perceptron algorithm is remarkable in a few respects.,2,0.65,1,10,2005
"First, the algorithm in gure 1.3 can be efcient even in cases where GEN(x) is of exponential size in terms of the input x, providing that the highest scoring structure can be found efciently for each training example.",2,0.9545454545454546,4,44,2005
"For example, nding the arg max can be achieved in polynomial time for context-free grammars, so they can be trained efciently using the algorithm.",4,0.8148148148148148,3,27,2005
"This is in contrast to the support vector machine and boosting algorithms, where we are not aware of algorithms whose computational complexity does not depend on the size of GEN(xi) for i = 1, .",4,1.05,5,40,2005
.,0,1,0,1,2005
.,0,1,0,1,2005
", n. Second, the convergence properties (number of updates) of the algorithm are also independent of the size of GEN(xi) for i = 1, .",3,0.921875,3,32,2005
.,0,1,0,1,2005
.,0,1,0,1,2005
", n, depending on the maximum achievable margin  on the training set.",2,1.0714285714285714,2,14,2005
"Third, the generalization theorem (theorem 12) shows that the generalization properties are again independent of the size of each GEN(xi), depending only on .",3,0.8387096774193549,3,31,2005
"This is in contrast to the bounds in theorems 8 and 9, which depended on N, a bound on the number of candidates for any input.",2,1.0,2,29,2005
"The theorems quoted here do not treat the case where the data is not separable, but results for the perceptron algorithm can also be derived in this case.",2,0.8833333333333333,3,30,2005
"See (Freund and Schapire, 1999) for analysis of the classication case, and see (Collins, 2002b) for how these results can be carried over to problems such as parsing.",4,0.8055555555555556,4,36,2005
"Collins (2002b) shows how the perceptron algorithm can be applied to tagging problems, with improvements in accuracy over a maximum-entropy tagger on part-of-speech tagging and NP chunking; see this paper for more analysis of the perceptron algorithm, and some modications to the basic algorithm.",3,0.84,4,50,2005
In this section we give further discussion of the algorithms in this chapter.,2,0.8928571428571429,2,14,2005
Section 9.1 describes experimental results using some of the algorithms.,1,0.9545454545454546,1,11,2005
Section 9.2 describes relationships to Markov Random Field approaches.,2,0.65,2,10,2005
There are several papers describing experiments on NLP tasks using the algorithms described in this paper.,2,0.9705882352941176,2,17,2005
Collins (2000) describes a boosting method  which is related to the algorithm in gure 1.2.,2,1.0555555555555556,2,18,2005
"In this case GEN(x) is the top N most likely parses from the parser of (Collins, 1999).",3,0.7708333333333334,2,24,2005
"The representation (x, y) combines the log probability under the initial model, together with a large number of additional indicator functions which are various features of trees.",3,0.8125,3,32,2005
"The paper describes a boosting algorithm which is particularly efcient when the features are indicator (binary-valued) functions, and the features are relatively sparse.",4,0.8888888888888888,4,27,2005
"The method gives a 13% relative reduction in error over the original parser of (Collins, 1999).",3,0.7857142857142857,2,21,2005
"(See (Ratnaparkhi et al., 1994) for an approach which also uses a N-best output from a baseline model combined with global features, but a different algorithm for training the parameters of the model.)",3,0.85,4,40,2005
Collins (2002a) describes a similar approach applied to named entity extraction.,2,0.9285714285714286,2,14,2005
GEN(x) is the top 20 most likely hypotheses from a maximum-entropy tagger.,2,0.71875,3,16,2005
"The representation again includes the log probability under the original model, together with a large number of indicator functions.",3,0.6428571428571429,2,21,2005
The boosting and perceptron algorithms give relative error reductions of 15.6% and 17.7% respectively.,3,0.6176470588235294,2,17,2005
Collins and Duffy (2002) and Collins and Duffy (2001) describe the perceptron algorithm applied to parsing and tagging problems.,3,0.9166666666666666,3,24,2005
GEN(x) is again the top N most likely parses from a baseline model.,2,0.7352941176470589,2,17,2005
"The particular twist in these papers is that the representation (x, y) for both the tagging and parsing problems is an extremely high-dimensional representation, which tracks all subtrees in the parsing case (in the same way as the DOP approach to parsing, see Bod, 1998), or all sub-fragments of a tagged sequence.",4,0.8387096774193549,4,62,2005
"The key to making the method computationally efcient (in spite of the high dimensionality of ) is that for any pair of structures (x1, y1) and (x2, y2) it can be shown that the inner product (x1, y1)  (x2, y2) can be calculated efciently using dynamic programming.",4,0.9590163934426229,4,61,2005
"The perceptron algorithm has an efcient dual implementation which makes use of inner products between examples  see (Cristianini and Shawe-Taylor, 2000; Collins and Duffy, 2002).",3,0.9838709677419355,3,31,2005
"Collins and Duffy (2002) show a 5% relative error improvement for parsing, and a more signicant 15% relative error improvement on the tagging task.",4,0.6833333333333333,3,30,2005
Collins (2002b) describes perceptron algorithms applied to the tagging task.,2,1.0,2,13,2005
GEN(x) for a sentence x of length n is the set of all possible tag sequences of length n (there are T n such sequences if T is the number of tags).,3,0.881578947368421,2,38,2005
"The representation used is similar to the feature-vector representations used in maximumentropy taggers, as in (Ratnaparkhi, 1996).",3,0.7954545454545454,3,22,2005
"The highest scoring tagged sequence under this representation can be found efciently using the perceptron algorithm, so the weights can be trained using the algorithm in gure 1.3 without having to exhaustively enumerate all tagged sequences.",3,0.9736842105263158,2,38,2005
The method gives improvements over the maximum-entropy approach.,1,0.8333333333333334,2,9,2005
"a 12% relative error   reduction for part-of-speech tagging, a 5% relative error reduction for nounphrase chunking.",4,0,3,20,2005
"Another method for training the parameters  can be derived from loglinear models, or Markov Random Fields (otherwise known as maximumentropy models).",2,0.84,2,25,2005
"Several approaches (Ratnaparkhi et al., 1994; Johnson et al., 1999; Lafferty et al., 2001) use the parameters  to dene a conditional probability distribution over the candidates y  GEN(x).",4,0.9615384615384616,3,39,2005
"Once the model is trained, the output on a new sentence x is the highest probability parse, arg maxyGEN(x) P (y | x, ) = arg maxyGEN(x) (x, y)  .",4,0.6976744186046512,3,43,2005
So the output under parameters  is identical to the method used throughout this paper.,2,0.9,2,15,2005
The differences between this method and the approaches advocated in this paper are twofold.,2,0.9,1,15,2005
"First, the statistical justication differs.",2,0.6428571428571429,2,7,2005
"the log-linear approach is a parametric approach (see section 4.2), explicitly attempting to model the conditional distribution D(y | x), and potentially suffering from the problems described in section 4.3.",3,0.8421052631578947,3,38,2005
The second difference concerns the algorithms for training the parameters.,2,0.8181818181818182,1,11,2005
"In training log-linear models, a rst crucial concept is the log-likelihood of the training data,   Parameter estimation methods in the MRF framework generally involve maximizing the log-likelihood while controlling for overtting the training data.",4,0.6891891891891891,3,37,2005
"A rst method for controlling the degree of overtting, as used in (Ratnaparkhi et al., 1994), is to use feature selection.",4,0.9629629629629629,4,27,2005
In this case a greedy method is used to minimize the log likelihood using only a small number of features.,2,0.8095238095238095,2,21,2005
It can be shown that the boosting algorithms can be considered to be a feature selection method for minimizing the exponential loss.,2,0.9565217391304348,2,23,2005
"A second method for controlling overtting, used in (Johnson et al., 1999; Lafferty et al., 2001), is to use a gaussian prior over the parameters.",4,0.8181818181818182,4,33,2005
The method then selects the MAP parameters  the parameters which maximize the objective function  for some constant C which is determined by the variance term in the gaussian prior.,2,0.9166666666666666,2,30,2005
"This method has at least a supercial similarity to the SVM algorithm in section 8.1 (2-norm case), which also attempts to balance the norm of the parameters versus a function measuring how well the parameters t the data (i.e., the sum of the slack variable values).",3,0.9150943396226415,3,53,2005
"We should stress again, however, that in spite of some similarities between the algorithms for MRFs and the boosting and SVM methods, the statistical justication for the methods differs considerably.",4,0.8235294117647058,4,34,2005
This paper has described a number of methods for learning statistical grammars.,2,1.0,1,13,2005
All of these methods have several components in common.,2,1.05,1,10,2005
"the choice of a grammar which denes the set of candidates for a given sentence, and the choice of representation of parse trees.",3,0.98,3,25,2005
"A score indicating the plausibility of competing parse trees is taken to be a linear model, the result of the inner product between a trees feature vector and the vector of model parameters.",4,0.7857142857142857,2,35,2005
The only respect in which the methods differ is in how the parameter values (the weights on different features) are calculated using a training sample as evidence.,3,1.0,4,30,2005
Section 4 introduced a framework under which various parameter estimation methods could be studied.,2,0.9333333333333333,3,15,2005
This framework included two main components.,1,0.6428571428571429,1,7,2005
"First, we assume some xed but unknown distribution over sentence/parsetree pairs.",2,1.0,2,13,2005
Both training and test examples are drawn from this distribution.,2,0.5909090909090909,1,11,2005
"Second, we assume some loss function, which dictates the penalty on test examples for proposing a parse which is incorrect.",2,1.1304347826086956,2,23,2005
"We focused on a simple loss function, where the loss is 0 if the proposed parse is identical to the correct parse, 1 otherwise.",2,0.8703703703703703,5,27,2005
"Under these assumptions, the quality of a parser is its expected loss (expected error rate) on newly drawn test examples.",3,0.6875,3,24,2005
The goal of   learning is to use the training data as evidence for choosing a function which has small expected loss.,2,1.0909090909090908,2,22,2005
A central idea in the analysis of learning algorithms is that of the margins on examples in training data.,2,1.1,2,20,2005
We described theoretical bounds which motivate approaches which attempt classify a large proportion of examples in training with a large margin.,2,1.2045454545454546,2,22,2005
"Finally, we described several algorithms which can be used to achieve this goal on the parsing problem.",2,1.0263157894736843,2,19,2005
There are several open problems highlighted in this paper.,2,0.85,2,10,2005
"The margin bounds for parsing (theorems 8 and 9) both depend on N, a bound on the number of candidates for any input sentence.",3,0.8392857142857143,2,28,2005
It is an open question whether bounds which are independent of N can be proved.,2,1.21875,3,16,2005
"The perceptron algorithm in section 8.3 has generalization bounds which are independent of N, suggesting that this might also be possible for the margin bounds.",2,1.0185185185185186,2,27,2005
The Boosting and Support Vector Machine methods both require enumerating all members of GEN(xi) for each training example xi.,2,0.9130434782608695,3,23,2005
"The perceptron algorithm avoided this in the case where the highest scoring hypothesis could be calculated efciently, for example using the CKY algorithm.",2,0.88,3,25,2005
It would be very useful to derive SVM and boosting algorithms whose computational complexity can be shown to depend on the separation  rather than the size of GEN(xi) for each training example xi.,3,0.9864864864864865,2,37,2005
"The boosting algorithm in section 8.2 optimized the quantity RL1, rather than the desired quantity L1.",2,0.5277777777777778,2,18,2005
It would be useful to derive a boosting algorithm which provably optimized L1.,2,1.25,2,14,2005
"I would like to thank Sanjoy Dasgupta, Yoav Freund, John Langford, David McAllester, Rob Schapire and Yoram Singer for answering many of the questions I have had about the learning theory and algorithms in this paper.",5,0.7560975609756098,2,41,2005
Fernando Pereira pointed out several issues concerning analysis of the perceptron algorithm.,2,0.8846153846153846,2,13,2005
"Thanks also to Nigel Duffy, for many useful discussions while we were collaborating on the use of kernels for parsing problems.",3,0.8913043478260869,3,23,2005
I would like to thank Tong Zhang for several useful insights concerning margin-based generalization bounds for multi-class problems.,2,0.9473684210526315,2,19,2005
"Thanks to Brian Roark for helpful comments on an initial draft of this paper, and to Patrick Haffner for many useful suggestions.",3,0,4,24,2005
"Thanks also to Peter Bartlett, for feedback on the paper, and some useful pointers to references.",2,0,3,19,2005
"The proofs in this section closely follow the framework and results of (Zhang, 2002).",3,0.6944444444444444,2,18,2005
"The basic idea is to show that the covering number results of (Zhang, 2002) apply to the parsing problem, with the modication that any dependence on m (the sample size) is replaced by a dependence on mN (where N is the smallest integer such that .",2,0.8584905660377359,3,53,2005
"In the problems in this paper we again assume that sample points are (x, y) pairs, where x  X is an input, and y  Y is the correct structure for that input.",5,0.7894736842105263,3,38,2005
There is some function GEN(x) which maps any x  X to a set of candidates.,2,0.9473684210526315,2,19,2005
There is also a function  .,1,0.9166666666666666,2,6,2005
"X  Y  n that maps each (x, y) pair to a feature vector.",3,0.8529411764705882,3,17,2005
"We will transform any sample point (x, y) to a matrix Z  N n in the following way.",3,0.6590909090909091,2,22,2005
Take N to be a positive integer such that |GEN(x)| = (N + 1).,2,0.8181818181818182,3,22,2005
"Zhang (2002) shows how bounds on the covering numbers of L lead to the theorems 6 and 8 of (Zhang, 2002), which are similar but tighter bounds than the bounds given in theorems 6 and 7 in section 6 of the current paper.",3,0.96,3,50,2005
"Theorem A1 below states a relationship between the covering numbers for L and M. Under this result, theorems 8 and 9 in the current paper follow from the covering bounds on M in exactly the same way that theorems 6 and 8 of (Zhang, 2002) are  derived from the covering numbers of L, and theorem 2 of (Zhang, 2002).",4,0.8676470588235294,2,68,2005
So theorem A1 leads almost directly to theorems 8 and 9 in the current paper.,2,0.65625,2,16,2005
